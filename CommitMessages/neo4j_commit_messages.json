[
{
    "commit_hash": "7daed39424bbd9ec1c98bd3332517fb2abc7e1c7",
    "commit_message": "update description to include backup file as well as dump as option ()"
},
{
    "commit_hash": "d9d4a2ccc8fcae6f9e4180a92b13f8ea736d800d",
    "commit_message": "Remove experimental annotation from getGqlStatusObjects()"
},
{
    "commit_hash": "b73b41cea3702cb3bb1cf5bd3aef141fa76f7ed1",
    "commit_message": "QueryAPI Flaky test assertion ()"
},
{
    "commit_hash": "39d8a46528e3f6e2437d5a4c411212f6b3a95842",
    "commit_message": "Remove CommandStream interface from StorageEngineTransaction ()"
},
{
    "commit_hash": "01c770d4493a692252ba1de7a910a82a793f47f5",
    "commit_message": "Rename committed command batches to match new names of command batches ()"
},
{
    "commit_hash": "64e20b6ee72d4edd1a6aac6135550fab4fe5748c",
    "commit_message": "Renaming command batches ()"
},
{
    "commit_hash": "80ce8ada8f1c17c432d90ffb1fdab7d67925842b",
    "commit_message": "Updates index usage stats on point (bounding box) queries too"
},
{
    "commit_hash": "e088eff3a306456c00c58733c30ffbb4fe037136",
    "commit_message": "Correct obfuscation of positions back ported ()\n\nCo-authored-by: Love Kristofer Leifland <love.leifland@neotechnology.com>"
},
{
    "commit_hash": "b177047f6814336874b86d7f286d14afe314c7ca",
    "commit_message": "Fix paths for ConfigDiagnosticsTest on Windows ()\n\nOn windows `Path.of(\"/\").toAbsolutePath()` equals `C:\\`, not `/`."
},
{
    "commit_hash": "55a73b3b12192e8f35c3c6be283fed716a737a0d",
    "commit_message": "Spd basic privileges ()"
},
{
    "commit_hash": "d9ae53a35b7b26e0abd3ef22ab233f60adb87790",
    "commit_message": "Remove pluggable DatabaseAllocator\n\nThere were two significant DatabaseAllocator implementations in the codebase, from which one was turned off. We don't see in the future that the DatabaseAllocator needs to be customized on this level, therefor the service provider pattern is unnecessary and also kind of messy. Removing the pattern means, that we can also:\n* Remove the property form the topology config node\n* Deprecate the `setDatabaseAllocator` procedure and make it into a no-op.\n* Deprecate the `initial.dbms.database_allocator` setting.\n* Remove the `OptimalDiskUtilizationAllocator`"
},
{
    "commit_hash": "ce4bf2b536412f76c66deb2109c652b726dda52d",
    "commit_message": "Obfuscating position correctly ()\n\nCo-authored-by: Love Kristofer Leifland <love.leifland@neotechnology.com>\r\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "65f2ecf7f2beaf307f0d2ce5b825dea4b3723d6c",
    "commit_message": "Adds variable scope clause to call subquery ()"
},
{
    "commit_hash": "0f3739296946e06e1e3c551ba379d394a0912f81",
    "commit_message": "Backup metadata script extraction take 2 ()"
},
{
    "commit_hash": "a17cc3183341e5045f53c54a988c628b80a04647",
    "commit_message": "add annotation for unsupported spd procedures ()"
},
{
    "commit_hash": "9c42de4aa0e28d2599c216688bfb9d323392aaf0",
    "commit_message": "Remove Periodic Commit from the parser ()"
},
{
    "commit_hash": "bfada6509d60c228bd3c118f0101ee0aec65b747",
    "commit_message": "Handle small idle timeouts in Cypher Shell"
},
{
    "commit_hash": "4894e126b74e76477e412f77e6dceed5e1e579b1",
    "commit_message": "Codegen gqlcode ()"
},
{
    "commit_hash": "d2bc0c2159821066811c8fb2c05ab6c758f174e0",
    "commit_message": "Add GQL TIME ZONE ()"
},
{
    "commit_hash": "0bca8c55010115eb10f85dd61ac9edbfb19a38dd",
    "commit_message": "rawTxPull rotates tx log on version change\n\nThe append of byte arrays directly to the transaction log wouldn't\ndo rotation on version change because it didn't know the contents\nit was writing. Now each log file sent over knows its kernel version\nand we can rotate appropriately when a change is seen.\n\nWith this change all cases of tx log getting the version change\nwill trigger a rotation of the log.\nThis means that from now on each transaction log file should only\ncontain transactions of a single kernel version.\n\nThis is not true for checkpoint log files yet."
},
{
    "commit_hash": "5106df79b4bd34f5c58dd37a89f244561877d86a",
    "commit_message": "New util for tx log validation in tests"
},
{
    "commit_hash": "95a88b9b8fe7142fd16a157325fdda5bdecb3b48",
    "commit_message": "Fix NPE for root directory setting ()"
},
{
    "commit_hash": "2ad47a61c6b1ccce7805988510245e86958659a8",
    "commit_message": "Avoid doing rollback for transactions that throw exception on commit attempt ()"
},
{
    "commit_hash": "f8f2c9c19977389af082322008af50359c79e019",
    "commit_message": "Introducing Bolt 5.6 with GqlStatusObject ()\n\nTo comply with GQL, the notification and error objects must be updated to include a `GQLSTATUS` code and status description.\r\n\r\nTherefore, the current structure of these objects will need to be updated.\r\n\r\nWhile we need to update the format of the errors and notifications to adhere to the GQL standard, we must also maintain the old style for backward compatibility.\r\n\r\nThe protocol 5.5 was skipped since their specification missed the `description` field and this field is needed for backwards compatibility. Since drivers already implemented Bolt 5.5, Bolt Server should skip the bugged version.\r\n\r\n## Bolt 5.6\r\n\r\nThis new version of Bolt will drop the `notifications` field in favor to the GQL complaint `statuses`field. This new field is a list of dictionary which is define like:\r\n\r\n```\r\nstatuses::List<Dictionary>( // note this is minified from the API object of GqlStatusObject, this is to save some bytes.\r\n\tgql_status::String,\r\n\tstatus_description::String,\r\n\tdescription::String // for backward compatibility\r\n\tneo4j_code::String // for backward compatibility\r\n\ttitle::String // for backward compatibility\r\n\tdiagnostic_record::Dictionary(\r\n\t\tOPERATION::String\r\n\t\tOPERATION_CODE::String\r\n\t\tCURRENT_SCHEMA::String\r\n\t\t_status_parameters::Dictionary\r\n\t\t_severity::String // this is notificationSeverity renamed\r\n\t\t_classification::String // this is notificationCategory renamed\r\n\t\t_position::Dictionary(\r\n\t\t\toffset::int\r\n\t\t\tline::int\r\n\t\t\tcolumn::int\r\n\t\t)\r\n\t)\r\n)\r\n```\r\n\r\n Fields on the `diagnostic_record` or the entire object should be omitted if filled with default values.This defaults are versioned along with the protocol version, so newer protocol version can define different defaults.\r\n\r\n* Bolt 5.6 Default Diagnostic Record:\r\n\r\n```json\r\n{\r\n\t\"OPERATION\": \"\",\r\n\t\"OPERATION_CODE\": \"0\",\r\n\t\"CURRENT_SCHEMA\": \"/\"\r\n}\r\n```\r\n\r\nEmpty objects are also omitted for saving bandwidth. \r\n\r\nThe notification configuration will also be adjusted in this version, the property `notifications_disabled_categories` is renamed to `notifications_disabled_classifications` on `HELLO`, `BEGIN` and `RUN`.\r\n\r\n## Design and Implementation notes\r\n\r\n### Adding GQL statuses to metadata\r\n\r\nFor handling the new metadata fields, the `MetadataHandler` supports receiving in `statuses` and the `DefaultMetadataHandler` handles notifications as `statuses` using the Bolt 5.6 specs, including default values. Protocols version between `4.4`  and `5.4` are using the `MetadataHandlerV44` and older protocols (< 4.4) use `MetadataHandlerV40`. `AbstractLegacyMetadataHandler`provides a base implementation for the legacies handlers with support for legacy notifications.  \r\n\r\nExtension methods were added to enable protocols that don't support `db` on routing tables or `contains-updates` and `contains-system-updates`to extends with the desirable behaviour, to do not add theses fields.\r\n\r\nThe `MetadataHandler::onNotifications` was changed to include `Iterable<GqlStatusObject> statuses` as parameter. Older implementations ignore this parameter. The default implementation use it fill the `statuses` field on the metadata. `title` and `neo4j_code`are only field when status are instance of `Notification`and values are not null. \r\n\r\nEmpty positions are removed from the diagnostic record since this value means no value.\r\n\r\n### Renaming configuration fields\r\n\r\nFor renaming the configuration field on `HELLO`, `BEGIN` and `RUN`, the `NotificationsConfigMetadataReader`methods were segmented with a new read legacy methods for supporting the `notifications_disabled_categories` while the default methods are using the new configuration property, `notifications_disabled_classifications`.\r\n\r\nLegacy implementations were added, `HelloMessageDecoderV53`, `BeginMessageDecoderV52` and `RunMessageDecoderV52`.\r\n\r\n### Fix `OPERATION_CODE` in `DiagnosticRecord`\r\n\r\nThe value should be a `string` instead of a `integer` as specified in the technical document and drivers ADR. \r\n\r\n### Fabric Support\r\n\r\nThe fabric support to `GqlStatusObject`was done by extending the `ResultSummaryWrapper`class. `GqlStatusObjectWrapper`was added for status which aren't notifications and `GqlStatusObjectNotificationWrapper`for status which are notifications. \r\n### Testing\r\n\r\nThe test for the legacy implementation of Notifications are moved from `NotificationsConfigIT` to `LegacyNotificationsConfigIT` and they applied to Bolt Protocol versions from `5.4` to `5.2`. `NotificationConfigIT` is responsible to tests Bolt 5.5 or newer.\r\n\r\nUnit tests were added to `DefaultMetadataHandler` to make sure all the data types present in the `status_parameters` as supported, at the moment `int`, `boolean`, `String` and `List<String>`. See, `CommonGqlStatusObjectImplementation::insertMessageParameters`.\r\n\r\nCo-authored-by: grant lodge <6323995+thelonelyvulpes@users.noreply.github.com>\r\nCo-authored-by: dotStart <dotstart@pm.me>"
},
{
    "commit_hash": "6f9eea18a8772ab256560533de76e1da68b285e3",
    "commit_message": "Small refactoring in TokenScanValueIndexProgressor"
},
{
    "commit_hash": "3f15e4cac6adde951db86e70f07c5af71fe1a738",
    "commit_message": "disallow explicitly set null index setting values for vector index"
},
{
    "commit_hash": "71e0fde88f6e9aff12d1890fe87254fcd44b7fa8",
    "commit_message": "Update checkpoint validation ()"
},
{
    "commit_hash": "0e0a1834fd39230076d5f0e4c240c874acf56768",
    "commit_message": "Added support for configuring partial legacy logging behavior within the error accountant ()\n\nCo-authored-by: grant lodge <6323995+thelonelyvulpes@users.noreply.github.com>"
},
{
    "commit_hash": "6968b7279e26f185c4ebeb882cce75889997dbbb",
    "commit_message": "Remove resetUponFailure functionality\n\nThis is currently unused in the codebase and makes RandomAdversary more\ncomplex than what it needs to be. Replace it with a simpler API that\nallows tests to enable/disable the adversary as needed."
},
{
    "commit_hash": "7578142708dcb82da62c83f1f273a0e327e831f4",
    "commit_message": "remove query router setting ()"
},
{
    "commit_hash": "03ec7d4d7840c910f2c6a5689a25da31d445746f",
    "commit_message": "Add azure cloud storage backend ()"
},
{
    "commit_hash": "6133b52c4cdcd08e41fd02557490f0a664371029",
    "commit_message": "Ensure SPD entity graph contains the property key tokens ()"
},
{
    "commit_hash": "8cf0814578fa1f5a471ef5881b5b78bae852cfa4",
    "commit_message": "change quantization index setting to a boolean"
},
{
    "commit_hash": "0048f6aabd913d973966971c4fb867955149d291",
    "commit_message": "fix VectorIndexProviderTests with updated invalid prototypes"
},
{
    "commit_hash": "d05f2f974bac152b3662f73be88e7bc8384ee5d1",
    "commit_message": "normalize index setting validation parameter orders"
},
{
    "commit_hash": "51edacc96271d473ec8f1c3f116c09355739479a",
    "commit_message": "use OptionalInt for vector dimensions"
},
{
    "commit_hash": "f6aae57c51f66ff06944061654b57e27e412576d",
    "commit_message": "remove unnessesary unmodifiable wrapper in IndexConfig\n\nImmutableMaps are immutable, and already employ the same kind of runtime\nprotections as the Collections wrapper add in.\n\nThis also allows us to expose that it is now a SortedSet and we know we\ncan depend on that interface."
},
{
    "commit_hash": "2888376f5bc16aa32f6592251d5f07a34431544c",
    "commit_message": "EnterpriseTopologyGraphDbmsModelSnapshot supports DatabaseReferences fully"
},
{
    "commit_hash": "3173750a17a2663390ff31a37b240b732bb1c1a6",
    "commit_message": "set default for vector similarity function"
},
{
    "commit_hash": "75deaded6b8bb9f7d9cd35819ed3e2aba2b71d99",
    "commit_message": "Update point deserialiser for typed format to use WKT format. ()"
},
{
    "commit_hash": "5fd0c4bcd135359e95e82308d80adaaee3fe7bd9",
    "commit_message": "VectorIndex - expose HNSW params in vector config ()\n\nSigned-off-by: Ionut Anghelcovici <ionut.anghelcovici@neo4j.com>"
},
{
    "commit_hash": "ebc7966aeebbbdc499470825f08a271eab3f356d",
    "commit_message": "Allow ID maintenance interval to be configured. ()"
},
{
    "commit_hash": "2724656eaf93d5d9f5d51cc737642bde41ed2b4d",
    "commit_message": "Remove unused PageCursor.raiseOutOfBounds()\n\nIn production code, this was only being used in a delegation pattern.\nThere was no real callers.\n\nIn test code, this was used to simulate a out-of-bounds get, but we can\ndo the same very easily passing a negative offset."
},
{
    "commit_hash": "89eb86e380ae62978dc3d065ec81a04930c85353",
    "commit_message": "Refactor scoutNextSibling()\n\nThis is a pure refactor commit, where we perform the checks and bailout\nfrom the method earlier instead of keep accumulating variables and\nre-checking everything in the end.\n\nAlso add some extra comments to make it more clear why we're doing each\ncheck."
},
{
    "commit_hash": "2169cbec2306a796f14de653fb57f6ba9c0c4f43",
    "commit_message": "Publish last commit batch only after log sync. To prevent publication of index and position that will potentially point to not visible data ()"
},
{
    "commit_hash": "af6304d4b228ceb347aa854dcead9d1f6b6aaa73",
    "commit_message": "test: variable digits for file regex"
},
{
    "commit_hash": "1da7ea2c418030a85c3687e72944b6d24817c667",
    "commit_message": "add 5.11 validation specifically with int max dimension limit\n\nMore for semantics, and to ensure clarity of what _was_ valid, intended\nor not, at points in time; separate the 5.11 and 5.12 validation."
},
{
    "commit_hash": "fc4bc59c80aaa45873653fac3ca34eac36a59235",
    "commit_message": "add rolling upgrade protections for settings"
},
{
    "commit_hash": "be8acea928845367c3cb5b741cf06dc86aa740ca",
    "commit_message": "test stored index config contains defaulted values"
},
{
    "commit_hash": "89709350cc6a1cb46413cfcc00259536bccf82d9",
    "commit_message": "replace the IndexConfig in the IndexPrototype with the validated one\n\nThis ensures the correct, validated, normalized, and defaulted\nIndexConfig is persisted.\n\nIndexPrototype's pattern creates new instances rather than mutating in\nplace; and due to Java passing the _reference_ by _value_, the\nvalidation of the prototype would have to return the prototype for it to\nbe used.\n\nCo-authored-by: Ionut Anghelcovici <ionut.anghelcovici@neo4j.com>"
},
{
    "commit_hash": "051ca1ec9cae9022b65f46d4b49de5fb6b8d9f3a",
    "commit_message": "enable quantization in new kernel version"
},
{
    "commit_hash": "0408694b932128e1c38afb03739d9df84c4b7492",
    "commit_message": "add kernel version 5.23"
},
{
    "commit_hash": "f3296a29a1061fa00118fa8550fd44439ba32076",
    "commit_message": "add quantized vector codecs"
},
{
    "commit_hash": "c0985812249c5f4b313928e8420ca47efd905917",
    "commit_message": "test quantisation validation"
},
{
    "commit_hash": "f429a83a709548aa8aef1467645ae63edfd428a9",
    "commit_message": "add quantization setting validation\n\n  * legacy indexes should not include quantization in IndexConfig\n    the lack of quantization in IndexConfig should therefore be\n    considered the default: OFF\n\n  * newer indexes should always include quantization in IndexConfig\n    and have a default: ON if not specified.\n    always writing ensures that a default quantized index is not\n    interpreted later as a legacy index above, thus incorrectly OFF\n\nCo-authored-by: Ionut Anghelcovici <ionut.anghelcovici@neo4j.com>"
},
{
    "commit_hash": "fca153a1230dfe12d3545f714e3c5738858cec43",
    "commit_message": "ensure IndexConfig settings order"
},
{
    "commit_hash": "c0795fda9ca0047efa441a3c71a5d796567cce1a",
    "commit_message": "refactor VectorIndexCreationTest\n\nBefore Node and Relationships were test instances, all tests were in\nthat specific instance, and each test needed some complex logic to know\nwhat was valid.\n\nAs we are introducing new settings which would be valid within different\nversions; more control over those valid versions is needed.\n\nThis refactor moves the provider/schema/config tests into their own test\ninstances for each. The Node and Relationship equivlents are now suites\nof tests providing a factory of how to do specific entity things.\n\nEach of the test instances knows their own valid version ranges. This\nsimplifies the test parameterisations, and becomes a little easier to\nfollow now that each setting is being tested in its 'own universe'.\n\nWhilst here, switch to using the VectorIndexSetting test helper, and add\nan unset method."
},
{
    "commit_hash": "08ed62ac6f534448f51b8c07ecd88ec5272b4849",
    "commit_message": "Allow visibility boundary to progress even without active transactions ()"
},
{
    "commit_hash": "81baf16277b887dddcf87a72e392e459b471bb32",
    "commit_message": "Update checkpointer to return append index ()"
},
{
    "commit_hash": "c8e4f7523971561d0e5ff99de50dad32313c0209",
    "commit_message": "LOAD CSV: explicitly set Host header to prevent it from being set to a pinned IP ()"
},
{
    "commit_hash": "5774a3c85d3f3cb8b752d91b4bafdea08e00e147",
    "commit_message": "User invalidation ()\n\nFix so that a suspended user is not allowed to start a transaction against any database.\r\nLinked users fail authentication/authorization if they are suspended."
},
{
    "commit_hash": "41b7fed16c46f07660b0efd04330adaca34ae75b",
    "commit_message": "userDescription for ConstraintType"
},
{
    "commit_hash": "c9c67ceb820a77a112a18a97203ab10a0c3a3724",
    "commit_message": "Testing endpoint constraint creation"
},
{
    "commit_hash": "36a65a5cbec1936722cb93c7f633c0748bceb0c9",
    "commit_message": "Creation of endpoint constraints\n\nEndpoint constraints use new SchemaPatternMatchingType\n\nTo allow us to use different logic than other descriptors a\nnew SchemaPatternMatchingType is introduced\n\nNew validation method for SINGLE_ENTITY_TOKEN"
},
{
    "commit_hash": "8da82bb57e35c4b35fa7c3f01599213911bea8a3",
    "commit_message": "Spd full text index ()"
},
{
    "commit_hash": "c3b3fc7dc652be0cc3d75ce8b4e82fff25233dee",
    "commit_message": "UpgradeTestUtil retries failed dbms.upgrade() ()"
},
{
    "commit_hash": "94000851c234cd883d861f95e729a382a45a7df8",
    "commit_message": "Don't mess up index stats on interrupted sampling on shutdown\n\nIndex stats were being updated to empty sample for any index\nthat had its sampling interrupted on shutdown. Start-up blocks\non sampling indexes with empty stats so this would be sad on next\nstart. Allowing long-running index sampling to be interrupted\non shutdown was introduced in 4.4.29"
},
{
    "commit_hash": "b1bbcd7f812472a0efb54fab0eabe877eb647196",
    "commit_message": "Highest ever seen closed transaction based checkpointing ()"
},
{
    "commit_hash": "7b8e6e41fe2b0013a1b012a0ae1be6b8c5b266ab",
    "commit_message": "Query API Test Refactor ()"
},
{
    "commit_hash": "b2821e25600f0a7643c19d93c16d6809f6955fed",
    "commit_message": "Don't mess up index stats on interrupted sampling on shutdown\n\nIndex stats were being updated to empty sample for any index\nthat had its sampling interrupted on shutdown. Start-up blocks\non sampling indexes with empty stats so this would be sad on next\nstart. Allowing long-running index sampling to be interrupted\non shutdown was introduced in 5.15."
},
{
    "commit_hash": "4fd041efe976cc81f09a4e3b6430e806655d27fb",
    "commit_message": "Accumulate previously uncaught network errors ()"
},
{
    "commit_hash": "3f4ac7ebde42d74f2b811d36594fc9c53b0ca0ed",
    "commit_message": "refactor: Make VectorEncodingCall metrics a database local metric. ()"
},
{
    "commit_hash": "3bf198b03e0bb7876118abe57c17d5eebf44ab14",
    "commit_message": "Refactor recovery position and batch tracking ()"
},
{
    "commit_hash": "61fbf2429ed2bf6193f68dedb39f6b5e4ae58f59",
    "commit_message": "Remove old parsing for nicer errors from cypher 6 parser - index and constraint commands ()\n\nMoving the semantic check errors to the cypher 5 parsers to keep the cypher 5 behaviour.\r\n\r\nCovers:\r\nDrop by schema:\r\n+ `DROP INDEX ON :Label(prop)`\r\n+ `DROP CONSTRAINT ON ... ASSERT ...`\r\n+ `DROP CONSTRAINT ON ... ASSERT EXISTS ..`\r\nOld create variations:\r\n+ `CREATE INDEX ON :Label(prop)`\r\n+ `CREATE CONSTRAINT ON ... ASSERT ...`\r\n+ `CREATE CONSTRAINT ON ... ASSERT EXISTS ...`\r\nOld show variations:\r\n+ `SHOW ... BRIEF`\r\n+ `SHOW ... VERBOSE`\r\n+ `SHOW ... EXISTS CONSTRAINTS`"
},
{
    "commit_hash": "0ed8522549b64588bb1886421c67d29457086f22",
    "commit_message": "else-if in detectArchetype"
},
{
    "commit_hash": "6e2a3498d8a2a852cd49a78b33eb86a36489a91b",
    "commit_message": "Rename PropertySchemaType -> SchemaPatternMatchingType\n\nMore closely reflects the meaning of the enum\nadds more thoroughly explained Javadoc\nallowes more flexible use of the enum for new constraints"
},
{
    "commit_hash": "1525b18f6f14b7e37e7fb22c4937c927c524a71d",
    "commit_message": "Remove IndexUsageTracker\n\nIndexUsageTracker existed so we could locally accumulate queried\nsignals, before passing them to their main IndexUsageTracking. This was\ndone so we would avoid high contention on the AtomicLongs used\ninternally by DefaultIndexUsageTracking.\n\nSince PR , we changed DefaultIndexUsageTracking to use LongAdder\nand LongAccumulator to perform the same operations. These\nimplementations can trade a bit more of memory usage for much better\naccess contention, since they will only be fully processed when we call\n`getAndReset()`.\n\nSo now we can ditch the whole IndexUsageTracker because it is error\nprone, since it requires properly handling to invoke `close()` at some\npoint, otherwise we'll have missed query metrics. By bringing back the\nfunctionality into the IndexUsageTracking and having only it now, we can\navoid those missed query counts."
},
{
    "commit_hash": "1d74f98d53a86db8c03825e26bb83379ea74c958",
    "commit_message": "Revert \"Backup artifact metadata script extraction ()\" ()"
},
{
    "commit_hash": "3e4813da5b489e75d1936a6d089f7cfdc1cf7f9a",
    "commit_message": "CommunityTopologyGraphDbmsModel#getDatabaseRefByAlias now returns SPD shard references\n\n\r\n+ Query router uses the new SPD shard reference to determine if a transaction is an in-cluster RPC call."
},
{
    "commit_hash": "eae4ecebbfacb35d72e888fc6a9d4a23731dee0d",
    "commit_message": "expose valid settings from validator\n\nswitch to sorted collections to ensure ordering is consistant"
},
{
    "commit_hash": "03191ef9b842902761a84807d94ab9907a9abef4",
    "commit_message": "Neo4j Import API module ()"
},
{
    "commit_hash": "ed07e789786a00a2c6a74680cf3852efb4125ff3",
    "commit_message": "Bind to public Log so that unmanaged extensions can use it. ()"
},
{
    "commit_hash": "04f191bd7ce6586ab3881d6175dbd42447b1bc9f",
    "commit_message": "Added support for specifying additional listeners ()"
},
{
    "commit_hash": "62d374690e87f2664d8b1b43e40c4d6d9c9424f8",
    "commit_message": "Revert \"Backup artifact metadata script extraction ()\" ()"
},
{
    "commit_hash": "d6791d309102a9ef6af3e3f626008edd3e9f7549",
    "commit_message": "Assume directories always exist in GCP  ()"
},
{
    "commit_hash": "84f25b96646c897b79aa5c7eead671f73a991841",
    "commit_message": "Add SPDShard database reference"
},
{
    "commit_hash": "5c0681146b893769dc3e1b782cd7b4e080c841b8",
    "commit_message": "replace usages of VectorUtil with VectorIndexConfig"
},
{
    "commit_hash": "bfa54d6463c0f0250d79874ebd06fd7b4233b893",
    "commit_message": "Batch-based checkpoint and recovery ()"
},
{
    "commit_hash": "263d6ba7b0dc416b82d13cb1d5a3b24dc042ebe6",
    "commit_message": "add validator instances to vector index version\n\nHere we separate the different validations by both vector index version\nand the kernel version.\n\nAs you need to know the vector index version, and the validators require\ninformation _from_ it, it makes sense to include them as part of the\nversion.\n\nThe abstraction left is the map of kernel version to the validation.\nAs we add more settings, or modify the accepted range of values (even in\nan addative way), it's better than they are explicitly versioned.\n\nThere is an implied range of validity _between_ map entries; thus having\nthem in a sorted map gives us access to this without having to\nexplicitly have a range that we need to manually update and ensure no\noverlaps. They are reverse ordered in the map such that the newest\nvalidation is first, thus we only need to check if the provided kernel\nversion is at least that version to aquire the validator.\n\nIf not specified, we use the latest."
},
{
    "commit_hash": "1df224f203a970e8f98c4b23cd5c99e8b742fb4c",
    "commit_message": "add index setting validators\n\nThe main interface that will be passed around is the\nVectorIndexSettingsValidator.\n\nThis interface has methods to produce VectorIndexConfig objects.\n\ntrustIsValidToVectorIndexConfig is intendend for reading preexisting\nIndexConfigs, they are treated as truth as they should have been\nalready validated by some method in its creation.\n\nvalidateToVectorIndexConfig is the creation path that does the\nvalidation.\n\nvalidate is the primary checker creating the records, exposed for\nCypher's usage later.\n\nThe methods provided that take the validation records are to avoid\nhaving to regenerate the validation records if already done for other\nexplicitly checking, such as potentially in cypher's validation."
},
{
    "commit_hash": "4e3a6380d82fc6f92087582e5773e299f06cbb5c",
    "commit_message": "Introduce GqlStatusObject API on the server side ()\n\nCo-authored-by: Pontus Melke <pontus.melke@neo4j.com>\r\nCo-authored-by: Alfred Grip <alfred.grip@neo4j.com>"
},
{
    "commit_hash": "d53d73fd0f8051387a3bed8632f4f86f11a7fae2",
    "commit_message": "Backup artifact metadata script extraction ()\n\nCo-authored-by: Tony Butterfield <1846725+tonbut@users.noreply.github.com>"
},
{
    "commit_hash": "b1a8c4d79803de6f81ef4d059089822e6d96d3c6",
    "commit_message": "Fix parser bug in INSERT"
},
{
    "commit_hash": "3e2f253751bd09685fb5f90007940d5933055fe1",
    "commit_message": "Add --idle-timeout argument to cypher shell"
},
{
    "commit_hash": "8e4559266d7aa70a5a0abcd2a3c0e3362de12a3d",
    "commit_message": "Migrate [is|as]AnyTokenSchemaDescriptor()"
},
{
    "commit_hash": "09e760bd798233079eebbf83f1a6062958f4f9da",
    "commit_message": "Introduce [is|as]SchemaDescriptorType(type) to SchemaDescriptor\n\nSo we can avoid growing this interface with a pair of isSchema, asSchema\nfor every new constraint type we want to introduce."
},
{
    "commit_hash": "9749e19c83d13730e479a833e35cb47b098e3568",
    "commit_message": "Splitting location into routing and direct\n\nA dummy URI does not need to be provided in the routing case anymore, which was very confusing."
},
{
    "commit_hash": "36a84aa4375b9b8384d563de824033840dc58235",
    "commit_message": "The last closed batch sequence ()"
},
{
    "commit_hash": "ce2387d1519e0dc8abda910bf1abc21d1b768c89",
    "commit_message": "Stop using SchemaProcessor on SchemaChecker\n\nCurrently there is not a strong reason to use SchemaProcessor here,\nsince our processors are build and called manually in the same place.\n\nSo we can re-work our checkers methods and simplify everything by\nditching the SchemaProcessor model :)"
},
{
    "commit_hash": "e53ea1c018c7ef0cddd6d64ac53c8b04d127b864",
    "commit_message": "Enhance reverse channels to be log position-aware ()"
},
{
    "commit_hash": "cee73e914a86bce20d6881e4ac8b9a45107027d0",
    "commit_message": "Fix PRIVILEGES AS COMMANDS for EXECUTE privileges ()\n\nBy escaping the glob when needed for parsing. It is fine to escape the whole glob as the privilege itself doesn't distinguish between globbing characters when escaped or unescaped, they'll always be treated as special characters."
},
{
    "commit_hash": "808c9a6ffb58f87a4545526428646e8aef5bc87e",
    "commit_message": "Correct checksum in header if we have to switch file directly on startup\n\nThis case should only happen if we saw upgrade tx but nothing more and\nthen restarted."
},
{
    "commit_hash": "34683c44a3d757de545b5aaa5729fab10ebd10dd",
    "commit_message": "Fix PRIVILEGES AS COMMANDS for EXECUTE and SHOW SETTING privileges ()\n\nBy escaping the glob when needed for parsing. It is fine to escape the whole glob as the privilege itself doesn't distinguish between globbing characters when escaped or unescaped, they'll always be treated as special characters."
},
{
    "commit_hash": "496c641a6d8a99a64d33cbcc30e3a4795bba95da",
    "commit_message": "More KernelRead refactoring ()\n\nThis PR makes txStateHolder field and getAccessMode() methods private to KernelRead\r\nPlaces where they were referenced get those things another way, via init methods, usually from the same KernelRead.\r\n\r\nAfter that most usages of KernelRead are replaced with Read."
},
{
    "commit_hash": "92106652f4e26f73684ca261a8d237bd76112fbb",
    "commit_message": "UpgradeChecker returns, does not throw\n\nIt also tells if the upgrade is not necessary"
},
{
    "commit_hash": "9479a89e54916b7bc26ca310d8e1f5aec40b867a",
    "commit_message": "Make block format the default for new databases on Enterprise Edition\n\nCo-authored-by: MishaDemianenko <12656787+mishademianenko@users.noreply.github.com>"
},
{
    "commit_hash": "5d28a9e6afc844d56deaf9a4b91b151f0f1890a6",
    "commit_message": "Fix split package syndrome of parser common"
},
{
    "commit_hash": "e4277230e3b0f90a6b7b161436ef7dfdfe2738c9",
    "commit_message": "More rollback of transaction out of RecoveryService  ()"
},
{
    "commit_hash": "ccc2b8588f7e8954cb07ff60903f1e0a89f9199b",
    "commit_message": "Code review: Clarify setting"
},
{
    "commit_hash": "e4cb71fb72f514f1b3a91cf0171985989631a0dc",
    "commit_message": "simplifying user to just be a record ()"
},
{
    "commit_hash": "e51817fd57d0980ce494ddf51153e42a76139c16",
    "commit_message": "Recovery monitors and transfer objects update ()"
},
{
    "commit_hash": "14befd6df21b9f6d643ebeb98768c42c0ef3b1db",
    "commit_message": "Update neo4j-java-driver to 5.22.0 ()"
},
{
    "commit_hash": "da946fb900b85d206fb56e1aee7dd06ea0b8abd4",
    "commit_message": "SPD relationship create delete simple property read direct sharding ()\n\nImplement direct sharding schema for relationship creation, deletion and simple unbatched property read."
},
{
    "commit_hash": "5da8b0f17e8c33140effa9a4c179a9c896146f1c",
    "commit_message": "New checkpoint log entry format with additional position for the oldest non-completed transaction entry ()"
},
{
    "commit_hash": "6ffd31bd0c62cc26fd6745baff4d84684e206238",
    "commit_message": "Introduce GQL Codes ()"
},
{
    "commit_hash": "222649d22049f06d827accea89b488e26c0a3b1d",
    "commit_message": "Benchmark Cypher 6 in parser benchmarks"
},
{
    "commit_hash": "26c79dec2ec068df7c0a243ea9836ec101733b81",
    "commit_message": "Switch checkpoint thresholds to append indexes ()"
},
{
    "commit_hash": "2baf6d95343fd590f954f9510751c88e5de7db93",
    "commit_message": "Fix incorrect chunk node ranging in NodeRelationshipCache () ()"
},
{
    "commit_hash": "e19947d8cc1a0c86c83742ac077621dd6ab43d0c",
    "commit_message": "Set/Remove Dynamic Labels ()"
},
{
    "commit_hash": "54fe04cd50f7e7b71a10b7f43fe7df10db933491",
    "commit_message": "Remove not used LastCommittedTransactionIdProvider from TransactionLogFiles ()"
},
{
    "commit_hash": "5c1efc6fb7edd79dcb314730d92428cbbf2e7c4f",
    "commit_message": "Skip rotation checks in batch tx appender for envelope channels ()"
},
{
    "commit_hash": "70d9b4705700d1ef445880272c2490204b4b5822",
    "commit_message": "Read is not Read ()\n\nThis PR merges AllStoreHolder into Read, and renames Read to KernelRead.\r\nForTransactionScope and ForExecutionThreadScope subclasses are removed and remaining difference in functionality abstracted via AssertOpen and AccessModeProvider interfaces."
},
{
    "commit_hash": "95e9542ad3a1ac6d0407ec38d08325748954d48f",
    "commit_message": "Remove exposure of currently committing transaction id ()"
},
{
    "commit_hash": "3a1a9c833f8161da9912590ca2a49412732a1efc",
    "commit_message": "Revert \"Fix incorrect chunk node ranging in NodeRelationshipCache () ()"
},
{
    "commit_hash": "809a5b1d19c8363ac8192cb8a5d4dfdfd91018ad",
    "commit_message": "Fix incorrect chunk node ranging in NodeRelationshipCache ()"
},
{
    "commit_hash": "36254104affb825d4767e79f017e952fe6c74e1a",
    "commit_message": "Support scoped procedures and functions in Cypher\n\nCo-authored-by: Joel Bergstrand <joel.bergstrand@neo4j.com>"
},
{
    "commit_hash": "76605e50f3dc71cec15a6c1c40a0387f557388ed",
    "commit_message": "Prevent alias names containing many components ()\n\nCo-authored-by: Lasse Heemann <lasse.heemann@neo4j.com>"
},
{
    "commit_hash": "5391cc64a0f2cccdc5c0af185fe51707306ef32f",
    "commit_message": "Remove transaction id from log file header and from rotation signatures ()"
},
{
    "commit_hash": "9fb2a0b1157752a299c15bb7f4441c1e1d47a7f5",
    "commit_message": "Move PropertyDataShardingFunction to spd api module. Remove import type. Cleanup."
},
{
    "commit_hash": "e3b7109118419e8d936507c68e130c08b302a314",
    "commit_message": "Expose SPD importer in tooling (hidden for now)"
},
{
    "commit_hash": "1c916a0d1bf174721f33c944c8ae4830ab29ddb2",
    "commit_message": "ReadBehaviour has more methods to control and filter content"
},
{
    "commit_hash": "fa9b8c7b47135ed583a2b9e67d8d8ff5b0a88a92",
    "commit_message": "query router should respect awaitActiveTransactionDeadlineMillis ()"
},
{
    "commit_hash": "0049f9d66a245b4af6c1c92fc88bd707f978ab8f",
    "commit_message": "Append index on file rotation ()"
},
{
    "commit_hash": "043bfed16eec97252503341449274dc9b94175ee",
    "commit_message": "Read is not SchemaRead ()"
},
{
    "commit_hash": "23be8691865efa77a2c579f612d4c8f51baf7b4d",
    "commit_message": "Add `WITH AUTH` flag to `SHOW USERS` ()\n\nAlso includes:\r\nMove tests for disallowed clauses/expressions to separate section \r\ninstead of in the middle of the other show users command\r\nand added those tests for show current user as well\r\n\r\nAdd output column tests for SHOW CURRENT USER"
},
{
    "commit_hash": "f77b5feabcf81af6a87d2f9b4a8d43a70efa880f",
    "commit_message": "Remove transactionId argument from LogFile::append ()"
},
{
    "commit_hash": "aa8682dc01d69b53548075a67adf261965cba3df",
    "commit_message": "Read aggregates TxStateHolder rather then implementing it"
},
{
    "commit_hash": "d49728cb325b64d63d156a6171f9ad523d4d7a8f",
    "commit_message": "Improved parameter handling for GenAI providers\n\nVector encoding provider parameters can now be declared as classes to reduce boilerplate when introducing new parameters / providers.\n\nAn example:\n\n```java\n// Extending WithDynamic will expose any unmatched parameters as a map returned\n// by MyParameters::dynamic(), but the only requirement is having a 0-argument constructor.\npublic class MyParameters extends Parameters.WithDynamic {\n    // A required non-nullable field (STRING NOT NULL):\n    public String model;\n\n    // Another required non-nullable (INTEGER NOT NULL); without the annotation it would be defaulted to 0:\n    @Required\n    public long dimensions;\n\n    // A defaulted non-nullable field field (STRING NOT NULL):\n    public String version = \"v2\";\n\n    // Nullable types are represented by optionals and implicitly default to NULL, i.e. Optional.empty() (STRING):\n    public Optional<String> taskType;\n\n    // Nullable types can also have other, explicit defaults (FLOAT):\n    public OptionalDouble temperature = OptionalDouble.of(0.67);\n\n    // Non-public fields will be ignored.\n    private SomeType someValue;\n}\n```\n\nA populated instance of this class can then be created from a `MapValue` like so:\n\n```java\nMyParameters params = Parameters.parse(MyParameters.class, someMapValue);\n```\n\nIn summary:\n* If a field has no default value, it must exist in the map.\n* If a field is nullable (i.e. an `Optional<?>`), it automatically has a default value of `NULL` if nothing else is specified.\n* If a field is non-nullable (i.e. not an `Optional<?>`), its value mustn't be `NULL`."
},
{
    "commit_hash": "b5641adb32ed6dee859dc00c1b4b2196d6cfb2f9",
    "commit_message": "use builder pattern for lucene index writer configs\n\nThe IndexConfig only exists in IndexWriterConfigs in order for vector\nindexes to handle the correct Codec. It would be nice to lift the Code\nout such that we can remove that dependency.\n\nIt is easier and nicer to pull IndexConfig things out than to force with\nin. A builder pattern helps separate and configure the IndexWriterConfig\nat different levels, before building it when needed.\n\nThis also allows for more things to be versioned and handled via\nVectorIndexVersion; such as the dimensionalilty. As well as allow for\nmore overloads that the LuceneIndexType enum tried to already reduce.\n\nInitially tried simply using a Mode enum:\n        STANDARD, POPULATION, and TRANSACTION_STATE.\n\nThough this worked, it still kept branching on LuceneIndexType enums\nwithin the implementation body, and as it scaled would hurt readability.\nTRANSACTION_STATE was also only used for fulltext indexes.\n\nSo rather than mode->type, this reverses it to type->mode.\nThis allows Text, Fulltext, and Vector indexes to have their own modes.\nIt would be nice if each mode was an enum, explicit singletons were used\nas they share settings.\n\nThis approach therefore:\n* replaces the helper functions and LuceneIndexType\n* removes the need for IndexConfig to be plumbed down\n* allows for later customisation by defering IndexWriterConfigBuilder::build\n* avoids lambda for the often required Suplier<IndexWriterConfig> as\n  IndexWriterConfigBuilder::build is already one\n\nFrom: IndexWriterConfig.population(LuceneIndexType.FULLTEXT, config, analyser, indexConfig);\nTo:   new IndexWriterConfigBuilder(FulltextModes.POPULATION, config).withAnalyser(analyser).build();"
},
{
    "commit_hash": "1c2d34552a2badee2b3b34b1e6217873846f5a7b",
    "commit_message": "Extract Procedures implementation from AllStoreHolder"
},
{
    "commit_hash": "ea4f6c66f424521f1c29deb14b854e59c00d3aa8",
    "commit_message": "Read is not Locks ()\n\nExtract Locks interface implementation from AllStoreHolder/Read"
},
{
    "commit_hash": "ebf2d127dadea61d9c7e65c8939d1b91683360e1",
    "commit_message": "Edit descriptions of the full-text configs ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "735219941719ad200d6abad9f66fe2586b240f10",
    "commit_message": "Remove unused AssertOpen and always null Read parameters"
},
{
    "commit_hash": "21410c6a277a9c4b6462d612ee5b7cff1c4accf2",
    "commit_message": "Read is not AssertOpen"
},
{
    "commit_hash": "6408b10859cfcdff762594296452b5f71ff4874d",
    "commit_message": "Inline LockingNodeUniqueIndexSeek and LockingRelationshipUniqueIndexSeek"
},
{
    "commit_hash": "61bdc348e9fd233854ce17334ac0536900619740",
    "commit_message": "Kernel API can create nodes/relationships with specific IDs\n\nThis is mainly for property sharding where the property shards\nwill need to create entities with IDs decided by the entity graph."
},
{
    "commit_hash": "60d9cc0b92da892d776c39d9f30ed974b8d7fc92",
    "commit_message": "Add runtime support for dynamic SET/REMOVE properties"
},
{
    "commit_hash": "bec8708a010379781f3873e90ab0e0c99789e39e",
    "commit_message": "AllStoreHolder is not QueryContext ()"
},
{
    "commit_hash": "17e0d5f0a65f517d71a7f96dc5979ca289c47e40",
    "commit_message": "Support for .backup files DB upload ()"
},
{
    "commit_hash": "1cfe5aec900aac0ff35cdf64020b943dd1cf8075",
    "commit_message": "Log deprecation notifications for queries that go through query router ()\n\nCo-authored-by: Georgiy Kargapolov <inmost-light@users.noreply.github.com>"
},
{
    "commit_hash": "81d0d8ce6710f5f65fa8ee15fbdcdd62f4738695",
    "commit_message": "Add further metrics for the Query API ()"
},
{
    "commit_hash": "cacefd098ba7285449dc96ce0897fa2ec67d2c4f",
    "commit_message": "User auth surface ()\n\nAdds the SET AUTH PROVIDER clause to CREATE USER\r\nAdds the SET and REMOVE AUTH PROVIDER clauses to ALTER USER\r\nAdds the SET AUTH privilege\r\nEnables the combination of REMOVE and SET clauses on ALTER USER\r\n- all removes must precede all set clauses\r\nAdd Notification for undefined providers and disabled setting\r\nChange dependency of cypher->security instead of other way around\r\nRemove old unsupported User Security versions\r\n\r\nCo-authored-by: Lasse Heemann <lasse.heemann@neo4j.com>\r\nCo-authored-by: Therese Magnusson <therese.magnusson@neo4j.com>\r\nCo-authored-by: Olivia Ytterbrink <olivia@ytterbrink.com>"
},
{
    "commit_hash": "18f5a18f6c908b3a036516f7ebe60f3bed4d8c37",
    "commit_message": "Use `SystemDatabaseProvider` in default database resolvers\n\nAlso general cleanup of default database resolvers"
},
{
    "commit_hash": "ba3a14ac75d4a7b1391cf88b88ab676489c3b258",
    "commit_message": "More sleek SystemDatabaseProvider\n\n* `execute` is `query` now\n* new void `execute`\n* only `tx` provided, if `db` is needed can get extra\n* auto commit"
},
{
    "commit_hash": "230b999e52f1f84dfebf9fd0487acd5f6c02d102",
    "commit_message": "Use SystemDatabaseProvider instead of AbstractEM.systemSupplier"
},
{
    "commit_hash": "c4f241248ea3c3f4e9c05b108b7a985b5c1155f9",
    "commit_message": "Split SystemDatabaseProvider into interface and implementation"
},
{
    "commit_hash": "77fa58bef3183cfb44d0b2214105524c96a019ac",
    "commit_message": "TransactionLogWriter position of batch after possible rotation ()"
},
{
    "commit_hash": "5a37902433f93dc81353d7a15e12bd17630284ac",
    "commit_message": "spd execution context  ()\n\nImplement execution context for spd"
},
{
    "commit_hash": "bd6333afaf6739619d4413ba39664c2849aede2e",
    "commit_message": "Fill _status_parameters in diagnostic record for notifications."
},
{
    "commit_hash": "6fa8a140be19bd2e73fefa6a3fcb175fe3704c06",
    "commit_message": "Refactor notification list parameters.\n\nPreparation to be able to send in the actual list to the status parameters map."
},
{
    "commit_hash": "6b5a34843a6f5f62c17c90bf91164481167fedd4",
    "commit_message": "add nested use clause semantic check for composite in Cypher stack ()"
},
{
    "commit_hash": "90751fb7d0b3e4060f26084f0692b7956eb1bc13",
    "commit_message": "Multi versioned log prunning ()"
},
{
    "commit_hash": "d6a61ac6f4a7e2efaae85076f87abcd9131b3fb4",
    "commit_message": "set constituentFactory on databaseTransaction after creation ()"
},
{
    "commit_hash": "6b9a57e334c3e949f09efac81b56a25b113fc9ed",
    "commit_message": "Add more constructors for QualifiedName ()\n\n.. to avoid using ProcedureSignature for this purpose.\r\n\r\nThis makes the class simpler to use, and saves us on creating a\r\nProcedureSignature.Builder for the very simple task.\r\n\r\nWe also remove the List-version of the constructor, since the\r\narray-based version is sufficient to handle the cases we need."
},
{
    "commit_hash": "e8808b19e15f58249c02ad280bf282f7639b674b",
    "commit_message": "Mark `dbms.cluster.routing.getRoutingTable()` as deprecated ()"
},
{
    "commit_hash": "8c26a538e583dce79272d8658b4534af96e2b5f6",
    "commit_message": "Block format can use the reserved ID that record format cannot\n\nThis required injecting this fact into the IndexedIdGenerator\nsince it was hard-coded to ignore that ID, and block format also\nuses this class for its ID allocation."
},
{
    "commit_hash": "f9f029034418dd05b19d8e113ee020d1542fde44",
    "commit_message": "Apply updates for eventually consistent fulltext indexes in parallel\n\nA fulltext index, if setting\ndbms.index.fulltext.eventually_consistent=true will not be updated\nby the thread applying the transaction that results in changes to it.\nRather the transaction thread will queue the updates for a dedicated\nbackground thread (per dbms) to pick up and apply.\nPreviously this was always a single background thread and so for\nhigh fulltext index update pressure (e.g. when having many/large\nfulltext indexes) this would instead become a bottleneck and\ntransactions that wanted to queue updates would wait for queue\nto become smaller.\n\nThis commit adds configurability of having multiple of these\nbackground threads taking from this queue and apply fulltext\nindex updates. To take things even further, the refreshing\nof fulltext indexes will be done with another thread pool\nwith some configurable interval as to (when eventually consistent):\n- take away refresh from main index apply execute path\n- generally reduce the number of refreshes for fulltext indexes"
},
{
    "commit_hash": "9fda544e656b1e28004d195ea4ce2ca926583bcb",
    "commit_message": "Include parser version in parser name\n\nPreparation for Cypher versions."
},
{
    "commit_hash": "2393b7ed487e169ed82d5f81d6dbc3e4a4cf41a4",
    "commit_message": "Correctly propagate info from the last checkpoint in case of partial recovery ()"
},
{
    "commit_hash": "34488ecccf9e1df4a626dd36911188aee0aea8cc",
    "commit_message": "Improve the description of infer-schema-parts ()\n\nCo-authored-by: Bastien Lou\u00ebrat <b.louerat@gmail.com>"
},
{
    "commit_hash": "9b825f8f9c96c9c7fef4e56dfddf086980c0dc8c",
    "commit_message": "Skip blocks that belong to the rolled back transactions during partial recoveries. ()"
},
{
    "commit_hash": "e480c3350c515a890373d4cb25d84bb602c52009",
    "commit_message": "Prepare the connection descriptions for testing ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "0bf677bb0f5bcb582eb81ba47f5b5f4750ef9f3b",
    "commit_message": "Propagate CypherScope to Procedures interface ()"
},
{
    "commit_hash": "af260b56f853c5c359f597dbfbdd96f2b4df4fb6",
    "commit_message": "Remove legacy internal `internal.dbms.kernel_id` setting and references ()"
},
{
    "commit_hash": "799062e4663d05e1e27b105fd81218ef4d6c61cd",
    "commit_message": "Append index based backup ()"
},
{
    "commit_hash": "8ee591a89f315fa74d96b7c7defc306b9f181ba9",
    "commit_message": "InputEntity has explicit NULL_ID constant"
},
{
    "commit_hash": "163958cc4c7579d3c836b0aa5ec4cca599d9de87",
    "commit_message": "Block incremental import test for multiple ranges"
},
{
    "commit_hash": "c0ee5ea3a51ff709fccd65793b3d5a206e39c6b4",
    "commit_message": "Block format importer splits up rel input into partitions for scalability\n\nPartitions are basically ranges of node IDs where relationships ends up\nin one out of all ranges depending on which is their lowest node.\nThen when importing, going from range to range, relationships are imported\nfor the sides that are covered in the current range so that the page cache\naccess is much more controlled."
},
{
    "commit_hash": "725c7a0ca763076d8cae2cd2864a871d806b710e",
    "commit_message": "Extract Value<-->ByteBuffer encoding from kernel module"
},
{
    "commit_hash": "fac3c4cb98edaf1778c82522c836ecf36f27cdb7",
    "commit_message": "TokenIndexUpdater yields after flushRange if parallel\n\nThis updater only actually writes updates to the backing index in batches\nand so in a parallel scenario w/o the yield() call the writer would sit\nthere and hog all its latches of its last write. Adding yield() after\nwriting a batch of updates allows other parallel writers to make updates\nunhindered."
},
{
    "commit_hash": "1d931c678d64f8cac72f23d1d8b270dd5ebd9550",
    "commit_message": "Exposes a PageCache#flush() (w/o force)"
},
{
    "commit_hash": "1e51c0b1c7e54b2d342dba22229488ee698e59fa",
    "commit_message": "should run drop/replace database on session db ()"
},
{
    "commit_hash": "6fd40d00c5b11e09e07005550244ab034e246ba4",
    "commit_message": "Use ByteUnit as part of settings parsers instead of multipliers ()"
},
{
    "commit_hash": "61499248bd76580226a8c980b2a2a3379659381b",
    "commit_message": "Rbac in txstate ()"
},
{
    "commit_hash": "0295af8a722f6fb65174068dde26882e73bcf345",
    "commit_message": "Set/Remove Dynamic Properties ()"
},
{
    "commit_hash": "5aa35545ff6970d25ff341df713e4fdcc708764a",
    "commit_message": "Allow procedures to be stored by CypherScope ()\n\n.. by adding a layer of indirection in ProcedureHolder.\r\n\r\nInstead of directly mapping to the store item, we now include a\r\nsecondary mapping from `CypherScope`->`store item`. Since \r\nthe number of CypherScope's is expected to be small, we use \r\nan int array to store the mapping.\r\n\r\nThis extension of ProcedureHolder, also fulfills an expectation of\r\nTokenIdHolder, that if the procedure at point (name, {scopes}) is \r\nadded then we expect that `ProcedureHolder.all()` enumerates this \r\nitem **only** once."
},
{
    "commit_hash": "e0c39f4ae5485b35a805b1f897437082f9a9a2f1",
    "commit_message": "improve the description of import format option ()"
},
{
    "commit_hash": "5a20e266b865068459ca4988fda4d35a5190720e",
    "commit_message": "Log deprecation notifications to the query log\n\nEnabled by `internal.dbms.logs.query.deprecation_notifications.enabled`\nflag."
},
{
    "commit_hash": "dbcad426f66b86b0c8d7b17a045a27c4066eb791",
    "commit_message": "Fix review comment"
},
{
    "commit_hash": "09332991b63959870162c677802833f67b390750",
    "commit_message": "Fix review comments"
},
{
    "commit_hash": "1eda79ce789b130f7c0b02f4c95e2d636c154a50",
    "commit_message": "Unit test CommitPhaseStatisticsListener"
},
{
    "commit_hash": "17e4ca512f14fb5861181d2447ee897667b2e7cd",
    "commit_message": "Fix page cache statistics concurrent profiling\n\nSupport profiling of multiple concurrent transaction pipes\nin the same query.\nRegister a listener for commit phase statistics to attribute\nprofile statistics to the correct transaction pipe."
},
{
    "commit_hash": "9d56ff8f7979b032fff888d0ebf5fdbdfb518887",
    "commit_message": "Support concurrent aggregation of page cache statistics\n\nPage cache statistics is aggregated in ExecutingQuery, but the\nprevious implementation was assuming serial order of execution.\nAdd a concurrent implementation that we can switch to use only\nif the query has CALL IN CONCURRENT TRANSACTIONS."
},
{
    "commit_hash": "31e11a342595784458a8938baf2930c4b290099c",
    "commit_message": "Enable running procedures in SPD ()\n\nCo-authored-by: Nadja M\u00fcller <nadja.muller@neotechnology.com>\r\nCo-authored-by: Conor Watson <conor.watson@neo4j.com>\r\nCo-authored-by: Balazs Lendvai <balazs.lendvai@neotechnology.com>"
},
{
    "commit_hash": "f77fa704f517e560122919249ae5d3a46fbd1475",
    "commit_message": "Cleanup usages of internal TransactionLogFilesHelper ()"
},
{
    "commit_hash": "4ce20c1015e51b8bb02f63ecb232634ff492741e",
    "commit_message": "Add convenience method to get GQLSTATUS as string.\n\nAlso add some missing Override annotations"
},
{
    "commit_hash": "8c9d39283b5679de976fddddbfa44c80ea83d629",
    "commit_message": "Decouple threads from connections when transactions remain active ()"
},
{
    "commit_hash": "452be74bda37219b38865adb72da52c132513953",
    "commit_message": "Minor fixes in backup-restore commands docs. ()"
},
{
    "commit_hash": "d4f33ed863dea46946ff9d368630b157bb2481f1",
    "commit_message": "Move antlr parser to package with version"
},
{
    "commit_hash": "14133c5b8fe39c790f43ba1bd438d51aaea9f583",
    "commit_message": "Align procedureGet/functionGet methods to Streams ()\n\nSince most consumers expect it anyway, there is no good reason to return the set, rather than the stream."
},
{
    "commit_hash": "e674684eb15825fcdf874c3a30298b263629a6a6",
    "commit_message": "Introduce NotificationClassification\n\nThis is currently a copy of NotificationCategory.\nThis is a preparatory step for adding GqlStatusObject API."
},
{
    "commit_hash": "ec524e10600a51b872f41111b8162a73d0c0017c",
    "commit_message": "Ability to scan log tail for the last readable appended batch  ()"
},
{
    "commit_hash": "27fe461bc49827663f18f024464e268f0eaa20dd",
    "commit_message": "Optimize relationship check loop\n\nBefore we were iterating on all `L*L` combinations of labels possible\nfor start/end for each type, but we are only interested in the `2*L+1`\ncombinations that have one (or both) ANY_LABEL.\n\nThis can become quite a hot loop when running under a long running\ndatabase that has a high mark for highLabelId and\nhighRelationshipTypeId, so with this change we can make this pass at\nleast `O(highLabelId)` times faster.\n\nBackport of c5d6b6645340aa653af3e405465079e112dd556f"
},
{
    "commit_hash": "4626db39963b9e52b9a52c4a3e3138ffc9f65662",
    "commit_message": "Remove bouncycastle references where not needed"
},
{
    "commit_hash": "1fe7cd949ae38b7746fb86e7da2297449908a516",
    "commit_message": "Don't cache available reserved store size metric"
},
{
    "commit_hash": "b44c0bc8f8381466443e0e2239b6ff981f752b10",
    "commit_message": "Referenced node schema for import is more lenient\n\nIn that there can be multiple node headers specifying the same\n\"referenced node schema\" for the same input group, as long as\nthey all specify the same schema."
},
{
    "commit_hash": "0d21dde2cbcb9bf94a1a4fbb1e2b1cfc7f8ec003",
    "commit_message": "spd detach delete implementation ()"
},
{
    "commit_hash": "36ab175c29b5882795873be24cd9f3243b658dfe",
    "commit_message": "Add tests for vector mappers and async scheduler"
},
{
    "commit_hash": "11f66a91fc74413e826476c95b982e4d469c3113",
    "commit_message": "Log deprecation notifications for queries that go through Fabric"
},
{
    "commit_hash": "f37d2acd3f2592b5b13cc3a96237f3c1826e20f0",
    "commit_message": "PR suggestion: rename setting in the code."
},
{
    "commit_hash": "462e6aff7d882f59f93816baf5a729e2cbc422bf",
    "commit_message": "Remove dependency to javacc parser in Cypher Shell\n\n- Use antlr based literal interpreted.\n- Remove unused code completion."
},
{
    "commit_hash": "0b505e5fbcbd8e07dcfd0a3feb192e48347f1fe6",
    "commit_message": "use ListValue.forEach in more PathValueBuilder cases"
},
{
    "commit_hash": "0ce5130e7a421512cdf1e801334207066accb177",
    "commit_message": "adds tests for ListValue.ternaryContains()\nPrependList/AppendList.iterationPreference is now ITERATION"
},
{
    "commit_hash": "cea08a54a1644dc6c8ac7fa0a50704d0b9fd726e",
    "commit_message": "impl ArrayListValue.ternaryContains\nimpl JavaListListValue.ternaryContains\nimpl PrependList.ternaryContains\nchange AppendList.ternaryContains to be less recursive"
},
{
    "commit_hash": "92f52c51c39711dd38b1fff3e0fc10249b90a8db",
    "commit_message": "make PrependList.forEach less recursive"
},
{
    "commit_hash": "f2e141aaef893ada700cbacda2b87ab1d91db586",
    "commit_message": "add ternaryContains() to ListValue\n* makes it possible to specialize\nspecialize ternaryContains() in AppendList"
},
{
    "commit_hash": "f01c143c2b0963de7b26f1dfcc9b66077aa6abce",
    "commit_message": "extract anonymous iterator class to clean up flamegraph"
},
{
    "commit_hash": "9906d025b67922ee784e5b7c6dda79d0d441e1a2",
    "commit_message": "precompute AppendList.size"
},
{
    "commit_hash": "24eba9f175ee8230e94b627db944a54d704b8548",
    "commit_message": "Introduce forEach method on ProcedureHolder ()\n\n..to avoid an unnecessary call to `idOf(QualifiedName)` that will cause more problems when we begin to include `CypherScope`s in the holder keys.\r\n\r\nSince we intend to add CypherScope to the keys, we will naturally extend `idOf` to `idOf(QualifiedName, CypherScope)`. However, when these methods are used (during the creation of the securityContext) we are not necessarily aware of which CypherScope to use.\r\n\r\nAll in all, this change allows us to clean up the code a little bit, leaving the overall class looking slightly tidier."
},
{
    "commit_hash": "deb94b1896d169ffa46d59fbdd96308102c1ff14",
    "commit_message": "Optimize relationship check loop\n\nBefore we were iterating on all `L*L` combinations of labels possible\nfor start/end for each type, but we are only interested in the `2*L+1`\ncombinations that have one (or both) ANY_LABEL.\n\nThis can become quite a hot loop when running under a long running\ndatabase that has a high mark for highLabelId and\nhighRelationshipTypeId, so with this change we can make this pass at\nleast `O(highLabelId)` times faster."
},
{
    "commit_hash": "d85b0141d422fe96046590949d776cf76fbabdd2",
    "commit_message": "graph type dependence types ()\n\nThis is the preliminary work for kernels side of the graph type features\r\n\r\nwe decided to infer UNDESIGNATED to the constraints that are undesignated, and if you are creating a constraint that is not UNDESIGNATED you specifiy with a boolean if it is dependent or not.\r\n\r\nCallers to dropConstraint (kernel API only) will supply if the operation is allowed to drop dependent constraints, core API callers CANNOT drop dependent constraints by design \r\n\r\nCo-authored-by: Matthew Parnell <matt@parnmatt.co.uk>"
},
{
    "commit_hash": "0bfe9ded77e895f7ffc0f4059e5078d448fbb3e6",
    "commit_message": "Move shortest settings together"
},
{
    "commit_hash": "d13b6e2e262b4487c26c8f18f5c5c2bb202c1717",
    "commit_message": "Add cachedProperties support for `properties`"
},
{
    "commit_hash": "33cb53ab86d35bb4ef50269583396f943ea8cbbf",
    "commit_message": "Fix some bugs"
},
{
    "commit_hash": "587c311c8e86fc2607b6a7ed78656eea7e5cc594",
    "commit_message": "Support cached properties in produce-result in slotted"
},
{
    "commit_hash": "d847cce2cb49891010a101dddd94f59c4c15796e",
    "commit_message": "Make offset table simpler"
},
{
    "commit_hash": "74204ee2324a00d701ebeb64bddd7507eab24f5c",
    "commit_message": "Refactor parser\n\nPrepare for Cypher versions.\nMove code that can be shared between parsers to common module."
},
{
    "commit_hash": "b42fc284d6740bce3c9dd65c8f5beae5f4992385",
    "commit_message": "Remove FailedIndexProxyFactory\n\nThe purpose of this is to clean-up the Index API a little bit, by\nremoving old and unecessary stuff. The FailedIndexProxyFactory interface\nand its single implementation are not needed at all, because when\nFlippableIndexProxy.flip() fails, the thrown\nExceptionDuringFlipKernelException will be caught by the try-catch on\nMultipleIndexPopulator.flipAfterStoreScan(), which is the only caller\nfor this code path.\n\nWhen that happens, it will call MultipleIndexPopulator.cancel(), which\nalready calls population.cancel(indexPopulationFailure) that create a\nFailedIndexProxy and flip the index to it.\n\nIn effect we were setting the delegate of FlippableIndexProxy twice to\ntwo new instances of FailedIndexProxy on the failure path!"
},
{
    "commit_hash": "738718d9119ae7234446d6b70f2f5e3b35fc3beb",
    "commit_message": "Remove IndexActivationFailedKernelException\n\nThis exception type was only caught and wrapped in a plain Java\nIllegalStateException, so it does not needs to be a KernelException.\n\nWe use the InternalKernelRuntimeException just so we can keep handling\nit on the same places we did before."
},
{
    "commit_hash": "b9934c57304f8ce2a1a766e651a6821b1086e0fc",
    "commit_message": "Fix regression and speedup avg\n\n- avoid checking for NaN and infinities in the happy path\n- Use Double instead of NumberValue"
},
{
    "commit_hash": "862b3c0db8e4e72c0b3443b729954b48feaf6ef3",
    "commit_message": "Remove FlipFailedKernelException\n\nThis was a super-class for ExceptionDuringFlipKernelException and\nIndexProxyAlreadyClosedKernelException and was used only to be able to\nsimplify throws statement in the method signature.\n\nWe simply remove this in order to simplify the whole kernel exception\nhierarchy."
},
{
    "commit_hash": "f2d4dc7483b645ddcd7a29347cec6ee63c2e3614",
    "commit_message": "Remove return value from nodeSetProperty and relationshipSetProperty in kernel API ()"
},
{
    "commit_hash": "7dec419ab088ee55a50bde52ed9f7e8c8cf024c9",
    "commit_message": "Differentiate transient exceptions coming from tokenWrite ()"
},
{
    "commit_hash": "05e03a844be85003b58690a3fc78b9c1d355cd41",
    "commit_message": "Add description framework for adding argument descriptions ()"
},
{
    "commit_hash": "352dfc5b52d59db614b28ca456325d4b4fad6d22",
    "commit_message": "Limit Packstream message complexitiy ()\n\nCo-authored-by: Grant Lodge <6323995+thelonelyvulpes@users.noreply.github.com>"
},
{
    "commit_hash": "4513b0960c809983fd00412e2457cd3447d0b6b4",
    "commit_message": "Rotate transaction log when a tx of new kernel version is seen\n\nThis means that only transactions of one single kernel version will be in each transaction log file. The rotation is done when encountering the first transaction on a new version. This will be the first transaction after the upgrade transaction.\r\n\r\nNote that this behaviour is not true for checkpoint log files yet.\r\n\r\nKnown edge cases:\r\n\r\n1. If writing the upgrade transaction triggers log rotation (because it reaches threshold) a new log file will be created on the previous version (because upgrade is not applied yet and hasn't updated the version provider). Then on the following transaction a new log file will be created on the new version. This empty log file in the middle should not be a problem and is not mitigated, just tested.\r\n\r\n2. If upgrade is the last transaction to happen before shutting down the db (or the last transaction possible to recover) a rotation to new log file must still happen before the next transaction even-though the kernel version repository is already updated. A step in TransactionLogFIle start is added that does rotation to a new log file if the header of the last transaction file doesn't match the kernel version provider.\r\n\r\n3. When using the append of byte arrays directly to the transaction log rotation won't happen on upgrade. This will be fixed separately, probably by fixing rawTxPull to respect rotation of files on the sender.\r\n\r\nThis commit also fixes recovery to handle reading over files with different log formats"
},
{
    "commit_hash": "414f7940c88526adcc9a73dad191c4c558ee3ebb",
    "commit_message": "Log deprecation notifications to the query log\n\nEnabled by `internal.dbms.logs.query.deprecation_notifications` flag."
},
{
    "commit_hash": "07cba0756b0a008d00e92801a2fc47866c86b32a",
    "commit_message": "Introduce CypherVersionScope annotation for procedures ()"
},
{
    "commit_hash": "d466be4055b0e893baca34f0c6d1bdb64611f887",
    "commit_message": "Changing the place where Kernel transactions are wrapped into SPD Kernel transactions\n\n\r\nIt is currenlty done in Query router. That does not work for at least the following cases:\r\n  - CALL IN TRANSACTIONS\r\n  - Core API\r\n  - procedures starting new transactions\r\n\r\nAlso having unmodified kernel transactions around in SPD databases poses a danger that they will resurface in unexpected places.\r\nThis change wraps SPD transactions immediately as they are created, so no code in the DBMS has a reference to an unmodified transaction."
},
{
    "commit_hash": "044d659e0ad77cbd096a36d6b0815c9a81b775d7",
    "commit_message": "Use WyHash for IdMapper"
},
{
    "commit_hash": "0509839fe83f6b5e4c208061d9c8a9a4dec7266a",
    "commit_message": "Refactored the connection shutdown behavior ()"
},
{
    "commit_hash": "6afff1953ebe29864a770ce208135459b43a2599",
    "commit_message": "Small update to help text for 'database import full' ()"
},
{
    "commit_hash": "595f8a15787d53ec46d4de900a7d0e33c84a039b",
    "commit_message": "Code review: fix typo"
},
{
    "commit_hash": "23c215fe9722d810e0d8883a89a7b7c1e84b9afe",
    "commit_message": "Code review: Add check in entity-wrapping implementations"
},
{
    "commit_hash": "b47ebdb375953c214ffd74dc84e24485a811d1b7",
    "commit_message": "Remove pointless abstraction CompositeCursor and make composite cursors composeable"
},
{
    "commit_hash": "7edf05fdff47dccc650ee52adfefd28a59cda33c",
    "commit_message": "Fix bug in query obfuscation\n\nThis commit fix a bug in query obfuscation when pre-parser options are present.\nThe pre-parser part of a query is not included in query cache keys.\nFor this reason all input positions that comes from a cache needs to be offset.\nThe query obfuscator relies on input postions to function.\nThis commit makes sure obfuscation is offset correctly with regards\nto pre-parser options.\n\nFix issues like this for example:\n\n```\n> return 1;\nQuery log: return ******\n\n> cypher replan=skip return 1;\nQuery log: cypher ******eplan=skip return 1\n```"
},
{
    "commit_hash": "d7bce3daa569a59186176581bd4fd6c682c2aa5a",
    "commit_message": "Add Bearer token support to HTTP server ()"
},
{
    "commit_hash": "84fad1532d6032d11978573e995076ae849d4fd4",
    "commit_message": "Remove unused recordSize/requiredAlignment from StoreFileMetadata\n\nRemoves an unused fields from StoreFileMetadata, and all the places sending it around unnecessarily."
},
{
    "commit_hash": "63a00b10849275b9cf9e91f7ba8c932148837fd9",
    "commit_message": "Fix null-checks in add, subtract et al\n\nBecause of how nullchecking works this caused RuntimeConstant to malfunction"
},
{
    "commit_hash": "0352d21cbd3bf505974c6000eeb63c6f2a0ade61",
    "commit_message": "5.x Fail transaction if kernel commit fails ()\n\nTrack exceptions from InternalTransactionCommitProcess and use them in onClose callback so that we can notify the user about what went wrong. Previously we'd panic, and depending on where the transaction failed in the commit process, the tx would either hang until interrupted (?) or come back as successful. Now we'll still panic, but the transaction will always come back with the exception that caused the panic.\r\n\r\nA local commit failure will have \"UnknownError\" status with a message saying that the tx failed locally but may have succeeded somewhere else."
},
{
    "commit_hash": "10ddc486904b8aadacbff1ef026cc68f468acc7d",
    "commit_message": "Less reserve/unreserve dance when looking for IDs to cache\n\nBasically maintenance() will interfere less with normal operations.\nThis is only a problem for multi-ID generators where maintenance will\nsee that the cache isn't full and there are IDs to likely find in the\ntree, but it may turn out that all or most IDs found are of sizes that\nthe cache is already full of and so there would be the pessimistic\nreservation of all those IDs, only to discover right afterwards that\nthey wouldn't fit and they would be marked as uncached."
},
{
    "commit_hash": "6d1ba4787ebebd92b08988daf2644a618402d517",
    "commit_message": "Extract shared functionality for cloud storage providers ()"
},
{
    "commit_hash": "6b72f8c3bdaeea758fd18bb8866c2e890413a921",
    "commit_message": "Fix error message creation to not format if not required ()"
},
{
    "commit_hash": "1f7d7f3de308d52e9776a7fc26548017e947b6a5",
    "commit_message": "Move parser projects around\n\nPreparation for Cypher versions."
},
{
    "commit_hash": "db25d6aa3400819e8bade8511be47edf4d46004b",
    "commit_message": "Let storage engine give idfiles list to StoreFileListing\n\nInstead of weird handling by buffered id generators\r\n\r\nAlso removes the temporary fix to clear IdGeneratorFactory on start\r\nIt is now cleared on stop for the affected Factory.\r\nTechnically not needed anymore since we don't use it in the\r\nStoreFileListing, but we shouldn't keep around a list of\r\nthe wrong idGenerators."
},
{
    "commit_hash": "03b5aded019c5c3b621cb5d9aeeac973f4dc7c37",
    "commit_message": "Fix various windows related URI test failures ()"
},
{
    "commit_hash": "1cce336a8a3b3f9ff54ca3c148f8e402c12f6b94",
    "commit_message": "Fix typed query param bug with nested maps ()"
},
{
    "commit_hash": "54b8a3f14fa9ac18c2d895b71aca65f21f5cd634",
    "commit_message": "Moves test-only constants into tests"
},
{
    "commit_hash": "b7104ac3bd4dc3190d8f6075d12ddcdccb961e1e",
    "commit_message": "Instant live-updated numUnusedIds in IndexedIdGenerator\n\nThis results in a format change of the header that IndexedIdGenerator\nwrites to the GBPTree (which is forwards and backwards-compatible tho).\n\nAn IndexedIdGenerator starting up w/o this count in the header will need\nto calculate it in start()."
},
{
    "commit_hash": "c23489c572fc47a47457c6ce38bf0b52a4fb0896",
    "commit_message": "Use bright red instead of red in Cypher Shell"
},
{
    "commit_hash": "5be8d09706fa1517a2b6d2dcd47b2164dc25140f",
    "commit_message": "snapshot backup command ()\n\nneo4j-admin cli command to create backup artifact from the offline database snapshot"
},
{
    "commit_hash": "6f59003f8022ee4b210530cddaf7871084311249",
    "commit_message": "URLAccessChecker should not assume all URLs are web\n\nCheck the URL scheme and only apply the pinning and CIDR validation for web URIs"
},
{
    "commit_hash": "c1fb1205b82c9da0f14e3f200a65034ed9ffe2b1",
    "commit_message": "Lookup transaction log file entries by append index instead of transaction id ()"
},
{
    "commit_hash": "db0cc9bc771bb3e298f14c20c01f43e5ab6c0f7c",
    "commit_message": "Remove unused functions from ProcedureView ()\n\n..to reduce the public surface of the interface."
},
{
    "commit_hash": "fe9560bce2ff054d7234f183dc8ace0613623878",
    "commit_message": "Remove references to httpv2 and replace with queryapi ()"
},
{
    "commit_hash": "3b4acdb5ac763ab7bee7083e66feea1e4d62c032",
    "commit_message": "Adds some changes to the parser to align with the language support ()"
},
{
    "commit_hash": "781fbf0a5c04f7623f824ad8cd6ace4a0a948ddd",
    "commit_message": "Changes to some cases of truncating logs in recovery\n\nThere was a case in recovery where corrupted transaction logs could be truncated even without the fail_on_corrupted_logs\r\nsetting disabled. If the first byte on a transaction boundary was 0 the reader would just think it reached the end of the file\r\nand say there were no more transactions and then truncate would happen. Now it detects this corruption and reports it\r\nso the user can decide on how to proceed.\r\n\r\nBefore a corrupt checkpoint file would only be truncated if there were transactions to recover as well.\r\nNow corruption in the checkpoint log file will be truncated away.\r\nThis replaces the functionality that was rotating to the next file if\r\nthere was unreadable bytes in the checkpoint log. It should always be\r\ntruncated instead."
},
{
    "commit_hash": "5b0091bdb1787a16babcd5ac43ae5183daf1faab",
    "commit_message": "Check max length bound ()"
},
{
    "commit_hash": "f8a0cd5cb33043679fc3a3a1ea69ffab4967a763",
    "commit_message": "Updated notification code for GQL compliance ()\n\nCo-authored-by: Louise S\u00f6derstr\u00f6m <louise.matematik@gmail.com>\r\nCo-authored-by: LinneaAndersson <linneamaria10@gmail.com>"
},
{
    "commit_hash": "0259dc18fc491e80ccef28b44ffa9ce08207c4d3",
    "commit_message": "Antlr grammar refactor enclosed property list ()"
},
{
    "commit_hash": "e2d6956bf88e68dc7a5abae5059dff50cd4669c2",
    "commit_message": "Bidirectional shortest k ()"
},
{
    "commit_hash": "9d910e3d2e633779862c682fff7c108fba3c2071",
    "commit_message": "Rename ServerDetails.Health to RunningState"
},
{
    "commit_hash": "1d80620c6bba008ab96d9aba1b524a8bf6a5534a",
    "commit_message": "5_20 checkpoints correctly says that they know about consensus index\n\nThe new 5_20 version of checkpoint record was claiming to not know about\nconsensus index. This meant that every time the logTail was read we looked\nup the consensus index by reading through all the log entries of the\nfile the checkpoint tx was in."
},
{
    "commit_hash": "ebd314603c96a18f3ef63752d29893c0ed790f9c",
    "commit_message": "5_20 checkpoints correctly says that they know about consensus index ()"
},
{
    "commit_hash": "ef947c41ae70588a78fe95ed9947de883979105b",
    "commit_message": "Rewrite duplicate use clauses ()"
},
{
    "commit_hash": "718ae87f9a0cd5c3558073215cb6597eeaa96b40",
    "commit_message": "Escape symbolic names in privilege command generation ()"
},
{
    "commit_hash": "7082d789cca332acee40f56bab18dc952c9ea4a7",
    "commit_message": "Import auto-skip header probe catches more exceptions"
},
{
    "commit_hash": "62f4b7f99e843b512c58a7282344295f708e8613",
    "commit_message": "Fix CLI handling of end-of-options delimiter in admin commands ()"
},
{
    "commit_hash": "51d395ace5059f7f31160eca461381d01b0c92f0",
    "commit_message": "replace targetComposite with databasemode in TransactionalContext ()"
},
{
    "commit_hash": "54d611bfce4ba8639be555fcced278612478908c",
    "commit_message": "Revert \"Parallel GBPTreeWriter has even less overhead on un-contended writes\"\n\nThis reverts commit 62d2d6e3a6fcd1d6aa5a6869338d1b5313c063bc."
},
{
    "commit_hash": "206508a327ee7b1075358b5de6c6a83066192a8a",
    "commit_message": "Add START_OFFSET support for EnvelopeWriteChannel ()"
},
{
    "commit_hash": "68ae200fb46d3e5a32747cf216bd8f081340aae5",
    "commit_message": "Improve messages on IndexEntryConflictException ()\n\nThis is a comprehensive clean-up around `IndexEntryConflictException` and includes:\r\n* Implementing the `getUserMessage()` API from `KernelException` and make users rely on it instead of the custom method that was there before.\r\n* Making the default exception message match closer the one generated by `getUserMessage()`. Now it is the same, only without the token resolution.\r\n* Cover missing case on the error message building, where existingId = -1 and addedId != -1.\r\n* General clean-up of the `IndexEntryConflictException` class, such as tidying-up the equals/hashCode implementation and marking test methods as such."
},
{
    "commit_hash": "ebe5c378a1cf39f21a6d2d42f7e58004f5fd6e74",
    "commit_message": "Adds minor changes to anltr parser to better work with language server ()"
},
{
    "commit_hash": "a94273bc0e6f11db18436895b101ee5bd683fddd",
    "commit_message": "Correct rollback entry information propagation to the checkpoint and metadata store on recovery ()"
},
{
    "commit_hash": "174195304f95efcdf96c0583aa90f2a97820a8dd",
    "commit_message": "TGModelSnaphost returns SPD DBReference\n\nSnapshot also lacked tests on the DatabaseReference methods"
},
{
    "commit_hash": "7dfb18a2c42c565e430e98fe38671994f098954f",
    "commit_message": "TGModel returns SPD DBReference"
},
{
    "commit_hash": "7a88673628a9e21a8e6e8a4db63e19c1d3d3eabf",
    "commit_message": "Update public/community/kernel-test/src/test/java/org/neo4j/kernel/impl/store/DefaultStoreSnapshotFactoryTest.java\r\n\r\nCorrect a test method typo\n\nCo-authored-by: Matthew Parnell <matt@parnmatt.co.uk>"
},
{
    "commit_hash": "438a2cb0d49679a91a65b3118f1d30f3163409a6",
    "commit_message": "Failed file snapshot releases its acquired store copy mutex\n\nA failure during making the snapshot would otherwise cause the\nstore copy mutex to never be released, preventing checkpoint to be run."
},
{
    "commit_hash": "b9eb2f6251b8ea88cce15c2824f68008ea5dbd49",
    "commit_message": "Seek functionality for NodeLabelIndexCursors ()\n\nto enable faster skipping of uninteresting nodes in a label index\r\n\r\nCall skipUntil together with next to skip faster than a standard while loop"
},
{
    "commit_hash": "6046737c8e004004959f242d94f9334723dbbd50",
    "commit_message": "Create kernel transaction when starting query router transaction"
},
{
    "commit_hash": "44dd8db811c9a68585271175cfdb388c6a2ef0aa",
    "commit_message": "Keep logs when an IT fails during a setup method and uses ClusterFactoryExtension ()"
},
{
    "commit_hash": "2a5e095a4e7f0e02d7e8e2fe96daef28ded9d40a",
    "commit_message": "Update driver to 5.20.0 ()"
},
{
    "commit_hash": "e7ad49eeed626ef8e5a23fb22cc8d29da47ad785",
    "commit_message": "Add option to set access mode in cypher shell"
},
{
    "commit_hash": "a1dee7c47e6340ce567cd469bdf07104b3832174",
    "commit_message": "Reintroduce separate transaction timeout for the HTTP API ()\n\nReintroduce HTTP's transaction timeout with `server.http.transaction_idle_timeout`. This replaces the removed `dbms.rest.transaction.idle_timeout` in 5.0"
},
{
    "commit_hash": "5c6dc6d0ad5ac3588e28b8273b6878dead1ecb81",
    "commit_message": "Smoother progress in ParallelSort"
},
{
    "commit_hash": "bf0d2ecdffce3ffc3afc698dfb8e72bb60041f57",
    "commit_message": "More parallel EncodingIdMapper#prepare\n\nThis involves updating all encoded ID's radixes in parallel\nand building information about collisions in parallel"
},
{
    "commit_hash": "9ac8d38149a5807056ab06a8bb2bf7b5511c16a3",
    "commit_message": "EncodingIdMapper chunk size follows estimated number of nodes a bit more\n\nSo that for larger imports fewer and larger chunks will be created\nto hold the ID mapping data. This reduces contention on creating the chunks."
},
{
    "commit_hash": "07ba47f3ba030d78cbf4f7e63789ecbc78f328ca",
    "commit_message": "Shortest refactoring ()"
},
{
    "commit_hash": "0d8ecf8321ed9eca624733e79ca56e81ce8077f9",
    "commit_message": "Test remote projection with TLS"
},
{
    "commit_hash": "cdd9f5f52ed8706b55a864fb60a7dde235b22b2f",
    "commit_message": "Refactored property in grammar ()"
},
{
    "commit_hash": "d15d3b80dadd88f61e06f35f8ee3c83c3b33a054",
    "commit_message": "Fix deadlock in parallel checking of MultiRoot tree, and enable it again\n\nThe problem:\r\nData trees are checked in batches and they all submit a job to the executor. For each batch\r\neach of the data trees do consistency check - that check is parallelized as well and each\r\nchild of that root becomes a separate task. Those tasks are submitted to the same executor.\r\nAfter having registered all its children tasks the thread used to await those futures.\r\nThe inner tasks just do their job and finish, but if there are enough of the \u2018outer\u2019\r\njobs waiting for all futures of their children to finish every thread could end up hanging\r\nin awaitAllFutures.\r\n\r\nThe fix tries to solve this by not having any of the data trees wait for\r\nthe child tasks, those tasks are instead waited on at the end of the\r\noutmost check."
},
{
    "commit_hash": "4aaee11c8aa418c4c45b11c4df4a68502b99dbce",
    "commit_message": "Parallel GBPTreeWriter has even less overhead on un-contended writes\n\nA GBPTree keeps navigated tree path and latches between operations.\nPreviously a parallel GBPTreeWriter would, with a certain frequency,\nforce a reset of this state just to be nice and not starve other potential\nconcurrent writers that need to write-latch the root\n(mostly pessimistic operations). This pre-emptive reset of state is unnecessary\nif there are no concurrent writers doing these types of operations, or indeed\nif there are no concurrent writers at all.\n\nTherefor this commit adds a check to see if there are other concurrent\nwriters that want to write-latch the root and only then do the\nreset of the state between operations. The check is made with a certain\nmodulo the number of operations, and this modulo is also reduced because it's\ngenerally way cheaper and so that writers can play even nicer with each other."
},
{
    "commit_hash": "a8c52dc14d56a27ea8dcf54f7e512ac62ce6c8d8",
    "commit_message": "Antlr grammar and ast builder clean up ()\n\nCo-authored-by: Love Kristofer Leifland <love.leifland@neotechnology.com>"
},
{
    "commit_hash": "d8ed380a0074af2d57acede0103c65ba05ec482e",
    "commit_message": "Fix bug when reading deleted relationships\n\nBackport of most of https://github.com/neo-technology/neo4j/pull/20652.\nMain issue is that we did `cursor.next` without inspecting the result leading\nto occasionally weird errors when `next` evaluated to `false`."
},
{
    "commit_hash": "495a7a4061abc8d5c42a78c5034934c369ec4ef7",
    "commit_message": "Introduce append index as a unique way of addressing appended data in transaction logs ()"
},
{
    "commit_hash": "98cbccfd874c3e3da45f74c02fc2b66bb287aaf9",
    "commit_message": "Specific empty progress printer for empty loggers.(Loaders part) ()"
},
{
    "commit_hash": "76a1c428d4b762437d8337be60942c5671cd1de6",
    "commit_message": "Add spd information to kernel transaction\n\nCo-authored-by: linneaandersson <Linneamaria10@gmail.com>"
},
{
    "commit_hash": "d035edb9428b553ccfceb7a5b8e2cd82a0e97df1",
    "commit_message": "Fix spelling in load command"
},
{
    "commit_hash": "88c1bec0ac43949b5a8f3a358669f4454db2f26c",
    "commit_message": "Move node predicate into product graph states ()"
},
{
    "commit_hash": "f49be5ffe7ee506ff47b1864ade12c330d29d791",
    "commit_message": "Specific empty progress printer for empty loggers. ()"
},
{
    "commit_hash": "82d7930e0944f09d8d235195c366e63e346fbd8d",
    "commit_message": "Antlr parser tests, privileges ()"
},
{
    "commit_hash": "45c97ade0e4f3845e4975e6f452530c2f2c0b4e6",
    "commit_message": "Added partial match option for shouldIncludeProperty\n\nTesting for --copy-schema option"
},
{
    "commit_hash": "e4c465698c9970ad2a54e500f6f22fe62168f505",
    "commit_message": "Change user facing message for copy command option --copy-schema\n\nalso added error handling to failed writes in SchemaMigrator"
},
{
    "commit_hash": "26db0bb79bbd7bd4dc483d75b1709796fed5b909",
    "commit_message": "Make `neo4j-admin database load` support full backups ()\n\nThis PR makes it possible to load full neo4j backups using neo4j-admin database load tool, including in community edition.\r\nChanges to load command behaviour:\r\n\r\ncommand looks for dump and all matching backups in the source path\r\nwhen there is only one candidate for database name is found - it loaded\r\nwhen there are multiple candidates found - error displayed with the list of candidates\r\nno candidates - error that there are no candidates\r\nload in load mode skips differential backups\r\nload in info mode can read and display metadata of differential backups\r\nbackups can be piped to stdin in the same way as dumps"
},
{
    "commit_hash": "efac5619f0edab63e34c630bf10efd0078b730a8",
    "commit_message": "Refactor cypher lexer\n\n- Introduce \"full token\" support. Preparation for better code completion in error messages.\n- Move all input position logic to CypherAstLexer."
},
{
    "commit_hash": "8bae6bb1bf080e2df284b120ab2fb172df51f283",
    "commit_message": "Editorial review of checkpoint config descriptions ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "19abbf10220b7a781c4081687a08156783c8112c",
    "commit_message": "clamp vector similairy score between 0 and 1\n\nThere are some situations that can occur, moreso as dimensionality\nincreases, that floating point errors aggregate into unphysical values.\n\nFor example comparing a vector with itself should give 1.0; however, it\nhas been observed with unit vectors as dimensionality increases, that\nthe l2-norm cosine implementation can produce a value ever so slightly\nover 1.0.\n\nThough this is understandable as floating point uncertaintly; it makes\nsome sense to clamp it. Lucene even clamp the lower end of one of their\nfunctions. It's interesting they don't do both.\n\nThis is causing some issues with external benchmark/testing with\nLangChain. One could debate the validity of equality of floating point\nnumbers, but semantically it makes sense to clamp."
},
{
    "commit_hash": "80714bccb6090d3811ca22d7848931fff1f07831",
    "commit_message": "Fix unwinding result from db.schema.visualization() ()"
},
{
    "commit_hash": "bc7638d5e2328faaac07c5671e3b16e20b61cde8",
    "commit_message": "Fix --force-system-database description"
},
{
    "commit_hash": "de038de837122aff93622311f4ba8c09fd7e16f3",
    "commit_message": "Clean out topology graph on 4->5 system db migration"
},
{
    "commit_hash": "b31af4ef530d7cb70d1739d322e3b0ab405b2653",
    "commit_message": "Minor refactoring of union cursors"
},
{
    "commit_hash": "8d2f60de3935275d7a0e33981dbe2d1dc0b11968",
    "commit_message": "Refactor intersection cursor to be able to take advantage of future improvement"
},
{
    "commit_hash": "886af42c15bc5b88c0d8f1e92388eda825ca6cb2",
    "commit_message": "Refactor/cleanup cursor implementation"
},
{
    "commit_hash": "1612f40e8f3cc7197530fe464ce691e38888bb64",
    "commit_message": "Propagate deprecatedBy to built in functions and clean up deprecation checks ()"
},
{
    "commit_hash": "d519a2030ff6673bbfa5a70255539f0aaf262b2a",
    "commit_message": "DumpLogicalLog can dump info about envelopes\n\nWhen using --envelopes DumpLogicalLog dumps information\nabout each envelope it encounters."
},
{
    "commit_hash": "2b71a0025dcdaa0647ca2a008bb42b34b9dc3f4f",
    "commit_message": "Add Anonymous Usage Data Reporting\n\nCollect anonymous usage data and report back, the collected data includes:\n* A pseudorandom unique identifier\n* Edition\n* Distribution method\n* Neo4j version\n* Size of cluster\n* Memory configuration\n* Number of processors\n* Number of nodes, relationships, and labels.\n* Store size\n* User License Status\n\nThis feature can be disabled with `dbms.usage_report.enabled=false`."
},
{
    "commit_hash": "923dc447b8f60d0172fc6b9410d918b6a8c5c7d4",
    "commit_message": "Antlr handling of unclosed quotes and comments ()"
},
{
    "commit_hash": "667fcddc50cf3b2d37ffc5b720f247a9dd7186c4",
    "commit_message": "Add GQLSTATUS information to notifications. ()\n\nInternal preparations for the new GqlStatusObject API.\r\n\r\nAdds a new GqlStatusInfo enum with GQLSTATUS and message for each notification. Later this will also be extended with the new subcondition to be able to create status descriptions for 6.x as well as any GQLSTATUS specific information needed for the diagnostic record.\r\n\r\nCo-authored-by: Linnea De Llano Balcells <linnea.andersson@neo4j.com>"
},
{
    "commit_hash": "f96e97236bffea319eabcf0206d02dc642c4a1d2",
    "commit_message": "TokenRegistry state isn't copied on add token during recovery\n\nThis heavily speeds up recovering token creations on db that has\nlots of tokens. Recovery doesn't need to look up token state\nand should therefor be safe to do."
},
{
    "commit_hash": "4c73b090f512c5659b24004dd055f77510c1d432",
    "commit_message": "ASTs for Privileges in Antlr ()"
},
{
    "commit_hash": "f8cde8d729c52d46f3705dee7c86af7567ba218d",
    "commit_message": "Fix return item text in antlr"
},
{
    "commit_hash": "9ae32932fcfd31f13e31be0259cb774a2fa5c0d1",
    "commit_message": "Preparation for envelopes\n\nChanges done in preparation for the new enveloped log format.\r\nIncludes:\r\n- Remove appendToLogFile event in favor of appendedBytes\r\n- Move creators of log readers to ReadAheadUtils\r\n- EnvelopeWriteChannel trigger rotation on rotationSize even when capacity and rotation are not multiples of eachother\r\n- setLogPosition doesn\u2019t throw when set to the end of file\r\n- Fixes one log pruning strategy to handle start of tx not being first in file\r\n- Previous checksum is read from channel on start up to not break chains on restart\r\n- Rollback entries works with envelope channel\r\n- Fixes to get getPosition correct when reading to end of file\r\n\r\nCo-authored-by: Emma Holmberg Ohlsson <emma.holmberg.ohlsson@neo4j.com>"
},
{
    "commit_hash": "036da54698ef3d8db95df1ed473873de018e5b12",
    "commit_message": "GBPTree gets properly read-only for \"permanently\" read-only mode\n\nThis has already been fixed in 5.x. The problem is that it wasn't possible\nto open a GBPTree in read-only mode, i.e. preventing it from doing any type\nof change to the file. Now this will happen if passing in a DatabaseReadOnlyChecker\nwhich is \"permanently\" read-only, like DatabaseReadOnlyChecker#readOnly()."
},
{
    "commit_hash": "36f6e7995c0d50b95f13feab380817c78f9b1962",
    "commit_message": "Removes unused import ability to set lookup index name"
},
{
    "commit_hash": "26687b3c9db120ee5640c03e12fab6f8cd59893f",
    "commit_message": "Testing for bad nodes & relationships"
},
{
    "commit_hash": "dea368a559cce80d30b48c7ad81f6a1bd8afe30a",
    "commit_message": "IndexIdMapper has less contention on checking duplicateNodeIds"
},
{
    "commit_hash": "9880082dbdab2d3cb6b7cea02e6a644a805f46cc",
    "commit_message": "TransactionalMarker#deleteAndFree also move IDs direct to cache"
},
{
    "commit_hash": "4c6eb16fa9c787e5755fa9445f8cb7274935b464",
    "commit_message": "Clean up merge/build methods a bit"
},
{
    "commit_hash": "41f6bf9de915740154c53e4e78a86b6d0fb9302b",
    "commit_message": "Adding schema monitors for nodes and relationships"
},
{
    "commit_hash": "c2f3a02e7f939d454089bba8b90ea8d2156a7c62",
    "commit_message": "RandomEntityDataGenerator generates :ID property based on id-type"
},
{
    "commit_hash": "abc6b2dcc62f655f2c2388a6020b13ee27e1e245",
    "commit_message": "Extracts common incremental import utils from record storage impl"
},
{
    "commit_hash": "280c7311041cfc108745d5e9f049e6f7ca553ae2",
    "commit_message": "Add new TRIM functionality ()"
},
{
    "commit_hash": "21d2eb9b7490f765e473850883d895154307adf5",
    "commit_message": "Mvcc execution engine retries queries ()"
},
{
    "commit_hash": "a91bd9db59b2964127f555f2361161c8d11f8508",
    "commit_message": "Enhance TransactionId to also provide kernel version ()"
},
{
    "commit_hash": "75f51c054b0c11f7ede18329af501a0af69e4987",
    "commit_message": "Fix the description of two arguments of the dump command"
},
{
    "commit_hash": "0be6fe787ece3c7fbe320c0758c3bae302acc35e",
    "commit_message": "Unflake IndexRecoveryIT ()"
},
{
    "commit_hash": "f351916f5ed07b9db969f67bbfb16d4414268f4b",
    "commit_message": "Refactor lexer\n\nPreviously we used a custom mode for string literals.\nThis caused the EOF to not match correctly\nfor queries ending in an open string."
},
{
    "commit_hash": "6dbd424e3876be708ccb113486a5290c4354976a",
    "commit_message": "Add initial QueryAPI metrics ()"
},
{
    "commit_hash": "1cdc6ebd622574c13b318ca14dc067824838a96c",
    "commit_message": "Rename query router boostrap -> bootstrap"
},
{
    "commit_hash": "0da282562c0dc91a42a5f62a833f07b482c2ec7d",
    "commit_message": "add status classification (under hidden setting) to log ()\n\nCo-authored-by: Louise S\u00f6derstr\u00f6m <louise.soderstrom@neo4j.com>"
},
{
    "commit_hash": "adc54e9fbad6bd7bd73a1e1502f19b6ab5d6be2e",
    "commit_message": "Add feature flag for ANTLR and refactor Cypher configuration ()"
},
{
    "commit_hash": "39569258615cc5a82e879a574a675c0c51bde08d",
    "commit_message": "Remove pair usage in backup utils ()"
},
{
    "commit_hash": "33e64d6265511a3fedc4f00bbfd9c505f0e66bda",
    "commit_message": "Check no exception on index/constraint creation in SchemaAcceptanceTest\n\nand some test refactor"
},
{
    "commit_hash": "6e8e81d158f142635bb95b0de84279dd54311013",
    "commit_message": "Use AssertJ instead of junit and consistent var names"
},
{
    "commit_hash": "a215755d7af1b115f1fe917d1e97a0f32601fa7c",
    "commit_message": "Use getElementId instead of deprecated getId"
},
{
    "commit_hash": "5730b468e76db208022474e715fca5f82270deb5",
    "commit_message": "Disallow lists with leading commas in Cypher parsing.\n\nExpressions like [, 'value'] were allowed by mistake in newer parsers."
},
{
    "commit_hash": "9308cec84935d25439b121ce9db601e9146a84da",
    "commit_message": "Disallow procedure parameters with leading commas in Cypher parsing.\n\nProcedure calls like `CALL foo(, 'input')` were allowed by mistake in newer parsers."
},
{
    "commit_hash": "e11c5c15bc8cb5548de72317d1b06f1ee006cfbc",
    "commit_message": "Disallow function parameters with leading commas in Cypher parsing.\n\nFunction invocations like `abs(, -42)` were allowed by mistake in newer parsers."
},
{
    "commit_hash": "edea69907b4a9733734f09dee79523491688bd37",
    "commit_message": "Disallow map projection with leading commas in Cypher parsing.\n\nExpressions like `abc {,key: 'value'}` were allowed by mistake in newer parsers."
},
{
    "commit_hash": "9bfa5d2d327bf2eafdb36f258238253199f9cbf6",
    "commit_message": "Disallow maps with leading commas in Cypher parsing.\n\nExpressions like `{,key: 'value'}` were allowed by mistake in newer parsers."
},
{
    "commit_hash": "3f3f2133a60aff7985144b9b57d80433f0a23c8b",
    "commit_message": "Allow \"asynchronous\" TransactionWriteEvents to be created without a TransactionEvent ()"
},
{
    "commit_hash": "5d624726bc85cb7ffccfa83eb1aa87f38c2446ad",
    "commit_message": "Include files for failed indexes in snapshots used by cluster store copy ()"
},
{
    "commit_hash": "0562055320f8b71bacb695951d353441dafc6523",
    "commit_message": "Cleanup metadata store related test implementations ()"
},
{
    "commit_hash": "d9277793ecfcae49ec808770359d94cd48077e69",
    "commit_message": "ASTs for Show Commands and Terminate Transaction in Antlr ()"
},
{
    "commit_hash": "0918a8de80a93db572ae30ebaee0193f2534b481",
    "commit_message": "Code review: fix typo"
},
{
    "commit_hash": "13b1df9d6bd825963c6c6fc240a59c36809eec77",
    "commit_message": "Code review: Add check in entity-wrapping implementations"
},
{
    "commit_hash": "eefddb25c2646ef7c89370e063b3cf811d4f4381",
    "commit_message": "UNION DISTINCT ()"
},
{
    "commit_hash": "27c92379d1a2d2dd325f1750ecbec8c280ae911f",
    "commit_message": "Allow large aws p2c uploads ()"
},
{
    "commit_hash": "b412df39c27751385940c1537a54fe747fdacfbc",
    "commit_message": "Make single server echo of routing context address configurable ()"
},
{
    "commit_hash": "bca7fb44c0f84cc8dd0100bb1d7585f2034f40f4",
    "commit_message": "changing to use a record for Neo4jPrincipal ()"
},
{
    "commit_hash": "44c62ff7ba1f9a6c176b527812fd56ab1d0a1c62",
    "commit_message": "Reinventing degree store again ()\n\nThis PR replaces \"old\" VersionedDegreesStore with new VersionedCachedDegreeStore \ud83c\udf89\r\n\r\nNew store uses cache, and all read or modified degrees are kept in cache for fast operations. Periodically (during checkpoint atm) the cache is flushed to the store which is multi root tree.\r\n\r\nMulti root tree is used here to provide an easy way to cleanup/overwrite all deltas related to the single degree when flushing the cache."
},
{
    "commit_hash": "0c219e87d255cf7e3a401e6db7d9d4b3b446d048",
    "commit_message": "Update backup/restore/aggregate/check commands to be able to handle cloud storage paths ()"
},
{
    "commit_hash": "443628d2d350ffe709f94931657905c0690561e1",
    "commit_message": "Add new InternalSyntaxUsageStats for GPM and legacy SHORTEST"
},
{
    "commit_hash": "1f5dfbd7178499bda33a3acf1aa9df31b6c0cb55",
    "commit_message": "Add error handling to antlr"
},
{
    "commit_hash": "a8a6ef2f10d6d6aaedafb6496b1e7b3b836bd05c",
    "commit_message": "Make the local driver creation for the HTTP Query API thread safe. ()"
},
{
    "commit_hash": "9e148404f5ae7003b38c3d6bc928f0fab86a3a80",
    "commit_message": "Add GQL Concatenation (||) ()"
},
{
    "commit_hash": "4c4ba81cc611455b54df08a0a010fd7f2e112c48",
    "commit_message": "Make SHORTEST cancellable while the algorithm runs ()"
},
{
    "commit_hash": "6473f5c71b4e37043553253388f9b81767a9a3b0",
    "commit_message": "Various improvements on the new Query API. ()\n\nThis includes\r\n* Support for impersonation via `impersonatedUser` attribute of the request body (tests have been added now to the enterprise addition, with various combinations of denial and allowance)\r\n* The stats attribute in the result has been renamed to `counters`, and the corresponding attribute in the request to `includeCounders`, aligning the API with the terminology in most of the client side drivers\r\n* Wording on the configuration description of the available server modules has been improved\r\n* Last but not least, the tests have been polished up according to JUnit 5 best practices."
},
{
    "commit_hash": "33f38b5f38cf4032ad1f996b4d6b8fd551899013",
    "commit_message": "Antlr Drop, Alter, Rename and more Commands ()"
},
{
    "commit_hash": "8f26b2dc1f0400a8a22925fbb824fe1e808efdca",
    "commit_message": "Best-effort profiling for slotted operator ()"
},
{
    "commit_hash": "79ed76c8fc297814601115d491e6b0ee6198cd9d",
    "commit_message": "Remove LongBitsManipulator\n\nIt was visible in the flame graph because it's on the hot path of sorting.\nInstead of doing a single OR instruction it did something along the lines of:\n\n1. static load\n2. vtable method call\n3. bounds check of array\n4. array load from index\n5. vtable method call\n6. calculate negative mask\n7. do the OR operation"
},
{
    "commit_hash": "bf9890d2b42c17ae97be87a0441146055f69033f",
    "commit_message": "Introduce new HTTP Query API ()\n\nThis change adds a revamped version of the HTTP Query API, formerly known as HTTP Transaction API. It is implemented as an additional Jakarta RS Resource under the name `QueryResource`. The new resource is mapped as `/{databaseName}/query/v2`.\r\n\r\nIn this first instalment, the new resource must be explicitly enabled via `server.http_enabled_modules=QUERY_API_ENDPOINTS`. As that property is not additive, any other options such as delivery of Browser files, or the old API must be enumerated as well if the should be available, too (i.e. like this `server.http_enabled_modules=BROWSER,QUERY_API_ENDPOINTS,TRANSACTIONAL_ENDPOINTS`).\r\n\r\nThe new resources uses a special instance of the Neo4j Java Driver connected via a local, in-process Netty channel, thus all interaction goes through the driver. The API is therefor able to run in a cluster in case server side routing is enabled via `dbms.routing.enabled=true` and the necessary listening and advertised addresses.\r\n\r\nThe format is heavily inspired by the original PoC of a standalone HTTP module for Neo4j, but still subject to change. The type representation is different from JOLT and much more aligned with other APIs aiming at transporting types over JSON. This should make parsing the format for downstream clients a lot easier.\r\n\r\nCo-authored-by: Michael Simons <michael.simons@neo4j.com>"
},
{
    "commit_hash": "94d51a445e298a92869b81fc29728e6fb013563a",
    "commit_message": "Use a non-locking version of indexGetOwningUniquenessConstraintId\n\n`indexGetOwningUniquenessConstraintId` acquires a shared lock, which\nprevents us from modifying constraints from a nested transaction (it\nrequires an exclusive lock)."
},
{
    "commit_hash": "c1f64a80feb0f69ce50efd6ac2c58b9159bedbaf",
    "commit_message": "Add commit timestamp to CDC ChangeIdentifiers  ()"
},
{
    "commit_hash": "449d84fd10dcc45d2eaaf31b9f868a5812e1a0b3",
    "commit_message": "Faster searching free IDs in a range in most cases"
},
{
    "commit_hash": "61b5d3b3a7d9f900edf557a8cc9a1496eebe9330",
    "commit_message": "Better notification of when free IDs are available\n\nPrevious boolean approach was prone to race between update and scanner,\nwhich became obvious, or was exposed, with the recent changes\nto batch updates."
},
{
    "commit_hash": "f381216a0baacb7e0a890118249e78e38209a0c3",
    "commit_message": "Multiple ID generator marks within same range are written w/ one call\n\nThe best case of marking fully consecutive IDs seems to be ~10x-15x faster."
},
{
    "commit_hash": "c223f2fc77e5dbc56852524bc1b2cc3259db0926",
    "commit_message": "Much faster IdRange#setBits for numberOfIds > 1"
},
{
    "commit_hash": "ecf6cdd13df474a741e4dcad263efd8fca5609a1",
    "commit_message": "Add --mask to DumpLogicalLog tool ()"
},
{
    "commit_hash": "edfb564dee0dedb9e4d05bdc832d969ff1627cbf",
    "commit_message": "Remove HeapTrackingConcurrentLongObjectHashMap.compute\n\n- Rework concurrent test case"
},
{
    "commit_hash": "1741d651d66e6267cbccbeef733ef967d13ccb0d",
    "commit_message": "Extract common static method"
},
{
    "commit_hash": "1e79cdcff32f9bbf8837636a0d3856231fa05636",
    "commit_message": "Fix HeapTrackingConcurrentHashMap.computeIfAbsent bug\n\nFix race conditions with the RESERVE state used for\ncomputeIfAbsent that could lead to an inconsistent state in\nHeapTrackingConcurrentHashMap and\nHeapTrackingConcurrentLongObjectHashMap."
},
{
    "commit_hash": "53e7bd476cf81223f625301d882ddc242dcfeb15",
    "commit_message": "Finish statement ()\n\nCo-authored-by: Gem Lamont <106068376+gem-neo4j@users.noreply.github.com>"
},
{
    "commit_hash": "945652babd142bc775f0669239624365f083bf17",
    "commit_message": "Use a non-locking version of indexGetOwningUniquenessConstraintId\n\n`indexGetOwningUniquenessConstraintId` acquires a shared lock, which\nprevents us from modifying constraints from a nested transaction (it\nrequires an exclusive lock)."
},
{
    "commit_hash": "09d8a9755dbd5f39ad54ec42cd8d8fc5f17feb25",
    "commit_message": "Lucene 9.10 upgrade ()"
},
{
    "commit_hash": "b7709e2412b303416d577e26623a9e2282824ef1",
    "commit_message": "Committed new antlr files ()"
},
{
    "commit_hash": "f636425a38222fc825f170ad0069978c7ec82e55",
    "commit_message": "RandomEntityDataGenerator handles NO_VALUE properties"
},
{
    "commit_hash": "dbb4ff6e67903b0dbb83acb797c09bd8d033b803",
    "commit_message": "New test in BlockImporterIT"
},
{
    "commit_hash": "ffad1a822bb9b72bec97231d6762710713b363fa",
    "commit_message": "SimpleRandomizedInput is more customizable"
},
{
    "commit_hash": "44b2de87c1b2dcc8df37c98ee5e79bac5f33cacd",
    "commit_message": "Update LOAD CSV to use URIs rather than URLs to enable access to cloud storage content ()"
},
{
    "commit_hash": "b18afa02ca237c4a5174ddfe13d3225d91d21031",
    "commit_message": "Extract modification to relationships mutator with multi version flag propagation ()"
},
{
    "commit_hash": "f17dc12caf0dcd498989d71ed6756c17df22f9ab",
    "commit_message": "Cluster-like low-level IndexedIdGenerator stress test"
},
{
    "commit_hash": "8a27e520c388f251828678b73c29447c28e6c552",
    "commit_message": "Properly update number of buffered IDs in FreeIdScanner"
},
{
    "commit_hash": "161bc1206506551f0fc8d71250616ebd7bcaef41",
    "commit_message": "IndexedIdGenerator#clearCache handles queued IDs too\n\nAn example of how this could be a problem (in a clustered environment):\n- On the leader there's an ID X of size e.g. 4 in cache\n- X is allocated, but only for size 3, which means the last ID (X + 3)\n  isn't allocated. When this happens the id generator tries to put\n  that \"waste\" back into the cache. However if the cache is full then\n  the waste is instead added to a queue to be handled later in maintenance\n  or when scanner loads more free IDs from the tree.\n- Very shortly after this (before the next call to maintenance,\n  which happens every second) there's a leader switch, but this member\n  is still up and running. clearCache() is called, but previously\n  these queued IDs weren't handled/cleared as part of this operation.\n- The new leader allocates the X+3 ID and uses it for some data.\n- There's another leader switch back to former member and since this\n  ID was still in the waste queue it could be put back into the cache\n  and allocated by this member, resulting in overwriting the previous record.\n\nThe fix is to handle queued IDs as part of clearCache(), i.e.\nwhen switching from being leader write those as uncached to the tree,\nand when switching to being leader just clearing them\n(although there should be none at that point)."
},
{
    "commit_hash": "0d8b463813f7d7790bb086e4a5839b10aa682933",
    "commit_message": "Proper monitoring of ID generator \"skipped\" IDs"
},
{
    "commit_hash": "a9f0b78b3c09949a1607f7e9d7289eb95f3ec678",
    "commit_message": "Fix CsvImporterTest for windows ()"
},
{
    "commit_hash": "84cafe4bf9942bdce049799108c712a4d4c89ac0",
    "commit_message": "Remove usage of deprecated RandomUtils ()"
},
{
    "commit_hash": "34484ba4df61274b2aba489722af068e055dacb9",
    "commit_message": "ElementIdMapper should be versioned ()"
},
{
    "commit_hash": "e1d6688ac3035b319202b685698d46873a7a86b3",
    "commit_message": "Dont call systemdb from dbms runtime repository in enterprise ()\n\nThe reason for this is to avoid calling txs to the system database. When running in an enterprise mode it is allowed for the system database to sometimes not be available. For example this is true when there is a store copy on the member (the database is still available from the cluster point of view). We therefore already have a model where updates to system database are cached and notified through events, which is a safe model for system.\r\n\r\nThis PR removes any transactions against system db and instead relies on the snapshots."
},
{
    "commit_hash": "c841e81041f5607bc4ae72eea9764ea0c369d331",
    "commit_message": "Fix dump commands to not log into output when --to-stdout and --verbose is specified ()"
},
{
    "commit_hash": "6c0e2650573e21c66622f7642c43a6b8ffdbe59e",
    "commit_message": "Remove indexed value from `throwSizeViolationException`"
},
{
    "commit_hash": "fac72da6f9b04e91cc8a7fdb97f0357dec363ce4",
    "commit_message": "Option in consistency-check to check tx log versions+rotations\n\nIntroduces a hidden option to consistency check to trigger\nchecking of transaction logs in regard to version upgrades.\nTo be used by upgrade-tests.\nIntroduced as a no-op check, functionality will come when the\nnew log upgrade behavior is introduced."
},
{
    "commit_hash": "05c3d3ec2cf48992f356bf79693c0b68fc49aeeb",
    "commit_message": "Wires the procedures / functions semantic analysis into the transpiled semantic analysis ()"
},
{
    "commit_hash": "19a4759b7d392d84d3236f40003249d21317c19a",
    "commit_message": "Update cypher jj files (again) ()"
},
{
    "commit_hash": "b7a59042aef657175c3109da567293a55ee2aa7e",
    "commit_message": "remove isLatestValid from VectorIndexCreationTest"
},
{
    "commit_hash": "aa8c245ae37af5eec9b0e6d7459461d5230f45dd",
    "commit_message": "DOC: Replace String in Either[String, Parameter] with AST ()\n\nEither[String, Parameter] is not easy to deal with in rewriters, neither do they have positions. Replace with AST objects extending Expression\r\nIt should be possible to build autoparameterisation for role and user names on top of this."
},
{
    "commit_hash": "e464367926bc5d2e5db10b9ee6f743f128b14a93",
    "commit_message": "Synchronizes new LoggingIndexedIdGeneratorMonitor methods"
},
{
    "commit_hash": "9b581acfcfa847b004bb282802b12807c1f76f94",
    "commit_message": "Do not take the same exclusive lock two times on the fast path in TransactionCommandValidator ()"
},
{
    "commit_hash": "5049b88b001e2b232bd17bbdf9d6f2ce53c4aebd",
    "commit_message": "Add memory tracking to NodeData and TwoWaySignpost ()"
},
{
    "commit_hash": "9b6548d8ffe3faf2368f9c28eb481419b7c860e6",
    "commit_message": "Update dump/load commands to be able to handle cloud storage paths ()"
},
{
    "commit_hash": "ba908a570837ae2b0f960bb9183e65413a94a5a8",
    "commit_message": "Remove unused pagecache from LogFilesBuilder\n\nSeems to have been leftover from when the metadatastore was pagecache based"
},
{
    "commit_hash": "7449af1a00d037429932f5ff4abd7aea4012e609",
    "commit_message": "IndexedIdGenerator#clearCache handles queued IDs too\n\nAn example of how this could be a problem (in a clustered environment):\n- On the leader there's an ID X of size e.g. 4 in cache\n- X is allocated, but only for size 3, which means the last ID (X + 3)\n  isn't allocated. When this happens the id generator tries to put\n  that \"waste\" back into the cache. However if the cache is full then\n  the waste is instead added to a queue to be handled later in maintenance\n  or when scanner loads more free IDs from the tree.\n- Very shortly after this (before the next call to maintenance,\n  which happens every second) there's a leader switch, but this member\n  is still up and running. clearCache() is called, but previously\n  these queued IDs weren't handled/cleared as part of this operation.\n- The new leader allocates the X+3 ID and uses it for some data.\n- There's another leader switch back to former member and since this\n  ID was still in the waste queue it could be put back into the cache\n  and allocated by this member, resulting in overwriting the previous record.\n\nThe fix is to handle queued IDs as part of clearCache(), i.e.\nwhen switching from being leader write those as uncached to the tree,\nand when switching to being leader just clearing them\n(although there should be none at that point)."
},
{
    "commit_hash": "951de7958e163e5657564b766479933ce0d7e45f",
    "commit_message": "Proper monitoring of ID generator \"skipped\" IDs"
},
{
    "commit_hash": "9e6145f860e068ab19da25cf905799a94dc7dc40",
    "commit_message": "remove explicit secondary sorting on neighbor entity id\n\nNeighbors who's score to a query is the same sometimes were not sorted\nas expected. ScoredEntityResultCollector::subordinate only was comparing\nscores; however, Neighbor::compareTo was secondarily comparing entity\nids.\n\nIt is possible for multiple different vectors to have the same score to\na query, but its easiest to emulate this senario with duplicate vectors.\n\nThe comparison for Neighbor is really only used in tests, and does not\nreflect what a user would actually see, unless we specifically returned\nStream::sorted.\n\nInitially I tried to make ScoredEntityResultCollector::subordinate care\nabout entity id; however, initial testing still showed the same problem.\nFurther investigation into how the priority queue is transforming would\nbe needed. As it is only documented to return sorted on score, the\nquickest solution for now is to drop the secondary sorting.\n\nIn the future it would be nice to revisit this to stablise the sort for\nthe user."
},
{
    "commit_hash": "f60501b2648bd0acda40bd6fae7b80a46da45bdd",
    "commit_message": "Add scheme-based path resolution to import command ()"
},
{
    "commit_hash": "c65ca26a9dcc9e382bf986ce74844266fe7f24eb",
    "commit_message": "Code review: use LongAccumulator instead of AtomicLong"
},
{
    "commit_hash": "2b88bc16d6ad631a0c53da49543d587a0a456627",
    "commit_message": "Track max reserved memory in parallel runtime\n\nIn the parallel runtime we don't track the total heap usage of the query since\nallocation tacking happens in multiple threads and keeping the exact value can be costly.\nHowever, we can fairly cheaply keep track of the maximum number of bytes we have reserved from\nthe transaction memory pool (we may not use all of it but it is still reserved). This PR adds\na mechanism for keeping track of this and also updates query.log to use this instead of -1 as before."
},
{
    "commit_hash": "a5254f8074c93fd0d2576b135193e5ec1a79b2f0",
    "commit_message": "Release previous node data graph on new source node ()"
},
{
    "commit_hash": "5fa4c994b1cda596650cc9c642e9c1c6c1009b04",
    "commit_message": "Remove usages and mentions of `UnlockCommercialFeatures` VM option. ()"
},
{
    "commit_hash": "6281d102b2f5ca29be8fed9166124177a0804f9f",
    "commit_message": "Replace MemoryAllocationLimitException w/ MemoryLimitExceededException"
},
{
    "commit_hash": "e22ac9d12d3e14e682a5d139b874e5a09efc3bf1",
    "commit_message": "set default to enable_vector_2 setting to true"
},
{
    "commit_hash": "13417cc1e6e7ead3493e5da2785278de4a9dfa86",
    "commit_message": "add kernel version 5.18"
},
{
    "commit_hash": "74304f207a32d4fd394dcd57269a35299d4ec6c0",
    "commit_message": "change vector field key in vector-2.0\n\nThis will allow us to potentially drop the requirement of declaring the\nvector dimensions when creating a vector-2.0 index. Done in a way that\ncould remove the restriction in a future version, if not ready for the\nsame release as vector-2.0. (additive change)\n\nvector-1.0 will simply keep using:\n    \"vector\"\n\nvector-2.0 will use the dimensionality, thus for a float[1536]:\n    \"1526d-vector\"\n\nWe need some form of VectorIndexVersion -> VectorDocumentStructure\nmapping. This could be done as an override on VectorIndexVersion, or\nsome field accessor; however, it is lucene specific. I would like to try\nand keep VectorIndexVersion agnostic to the implementation of the index.\n\nUsing switch expression as this is effectively the same as implementing\nthe effective vtable lookup manually thus shouldn't have much additional\ncost if any.\n\nUsing the exhaustive switch expression will enforce a compile-time\ncheck. It will also generate a TABLESWITCH instruction rather than a\nLOOKUPSWITCH instruction; the former being the same or faster than the\nlatter ...but such micro optimisations don't really matter, just a nice\nperk in addition to the compile-time check."
},
{
    "commit_hash": "1618109e3baea870bd30519dd263bec208a9936e",
    "commit_message": "Simplify MultiversionResourceLocker ()"
},
{
    "commit_hash": "4701d8707dc198badeab7f8c5c22947854208b77",
    "commit_message": "Small optimisations in TransactionCommandValidator ()"
},
{
    "commit_hash": "d275fad8e347a2b355250dcc014fd31cce7965fe",
    "commit_message": "CWE-835: Loop with Unreachable Exit Condition. Bump commons compress and commons codec ()"
},
{
    "commit_hash": "4ce504966664436c13f52c0b8638c6bc5e62c876",
    "commit_message": "Add START_OFFSET envelope type w/ reader support ()\n\nThis adds a new envelope type, START_OFFSET, to represent an offset that should be skipped at the beginning of the first segment of a log file so we can make sure that replicated entries in a clustered environment are correct aligned.\r\n\r\nThe ReadChannel will enforce that the payload content of the START_OFFSET envelope should be zeros. This is to fail fast on a possible corrupted log file."
},
{
    "commit_hash": "8b6690cc12123e27cae492b98ee9ce6034826880",
    "commit_message": "Adds a low-granularity means of externally tracking overall progress of import"
},
{
    "commit_hash": "5bed7a2866c5a4cef392a2a95b8433a517a0fdca",
    "commit_message": "Adds a way to listen to progress listener updates"
},
{
    "commit_hash": "3940b15eb43bea19e973e506334048532fefa34a",
    "commit_message": "Make flattenBooleanOperators and CreateIrExpressions cancellable"
},
{
    "commit_hash": "d06c12d1606fa7434dfc2b1f816cc590a06200f9",
    "commit_message": "Includes ID property name in duplicate check of CSV import header"
},
{
    "commit_hash": "869cb81c6268e94d14f204d6971e05dc1fac3190",
    "commit_message": "enhance exception messages for relationship vector index protections\n\nSeparate exception message creation from the throwing.\nfrom: void assertSupportedInVersion(String, KernelVersion)\nto:   void assertSupportedInVersion(KernelVersion, String, Object...)\n      void checkSupportedInVersion(StringBuilder, KernelVersion)\n\nThis allows to keep current functionality, but where customisation is\nneeded, we have it available through a `StringBuilder`.\n\nWhen checking a relationship vector index:\n\n* creating vector-1.0\n  * kernel version < relationship vector index introduced\n        FAIL: Failed. vector-1.0 relationship indexes unsupported, use newer. Upgrade.\n  * kernel version >= relationship vector index introduced\n        FAIL: Failed. vector-1.0 relationship indexes unsupported.\n\n* creating vector-1.0\n  * kernel version < relationship vector index introduced\n        FAIL: Failed. Upgrade.\n  * kernel version >= relationship vector index introduced\n        PASS"
},
{
    "commit_hash": "9f45ea95dde7358493147b75f18c58e8c9ce83ba",
    "commit_message": "add relationship vector indexes"
},
{
    "commit_hash": "d664f431d0e2142c142786c45273d12ecdcd9af4",
    "commit_message": "configure and extend tests to use vector-2.0"
},
{
    "commit_hash": "3002bd759047441067a0514b18cb0c0d045b0376",
    "commit_message": "add vector-2.0 index provider and descriptor"
},
{
    "commit_hash": "2ab7f36a2baab9f2d0db0297da64bfbe31593947",
    "commit_message": "avoid hardcoding vector versions and prepare tests for vector-2.0\n\nIn a few placesuse LATEST_KERNEL_VERSION for simplicity"
},
{
    "commit_hash": "c498fec4eec4335aaa45af376000d5dc66b0e3e6",
    "commit_message": "remove explicit FloatingPointArray for vector values\n\nUse VectorCandidates rather than FloatingPointArrays when writing a\nlucene document.\n\nDue to the similar code, and VectorCandidate being a fairly cheap\nwrapper; change the index capability to take the ignore strategy as a\nreference."
},
{
    "commit_hash": "f9bc79fa92a56bd649891c17223808511e1dc90f",
    "commit_message": "Clean-up Query/Transaction Id classes ()\n\nQueryId was only used to parse a String into an id and TransactionId was only used\r\n(outside tests) to format a transaction id for `db.listLocks` procedure.\r\n\r\nSo this refactor both classes into static classes with a static method to perform the\r\nsame logic, so we can drop all the boilerplate (hashcode, equals, string, ...)."
},
{
    "commit_hash": "483f11b3d2756bba307a6289a2184976b2d6089d",
    "commit_message": "Remove replicated tokens ()\n\nRemoved the ability to create ReplicatedTokens. Instead use the same token creation strategy as Kernel. This means that tokens are now just replicated transactions.\r\n\r\nThe state machines are kept but renamed to Legacy. This is because in a roll any instance may still receive ReplicatedTokens during a roll and we must therefor be able to process those as before."
},
{
    "commit_hash": "2f39d5520c624c692d5703d2684d7bf187f1a1fa",
    "commit_message": "Add IN ... CONCURRENT to CIT ()\n\nAdds support for the CONCURRENT modifier, Approved in CIP-34. The intention here is to provide support for the value in the logical plan, but not use it (yet).\r\n\r\n```\r\nCALL {\r\n  subQuery\r\n} IN [[x] CONCURRENT] TRANSACTIONS\r\n```\r\n\r\nx should evaluate to an integer, but can be omitted. If omitted the runtime should choose a sensible default."
},
{
    "commit_hash": "c073ff48b4de5c6e2be375de59de4b3140566c32",
    "commit_message": "Fix hasLabel(...) not calling tracer before returning ()\n\nWhen the hasLabel()/hasLabel(int) was returning before accessing the store cursor, because\r\nit used the transaction state to compute the return value, it was not registering that on the\r\ntracer (if set)."
},
{
    "commit_hash": "2ec33229da048a54e81616e2bcb20e96ff7cf9b9",
    "commit_message": "SHORTEST optimisations ()"
},
{
    "commit_hash": "fbef29fab3eb51e0b2509b0cd1b673c331d11a7e",
    "commit_message": "Serial execution of MVCC transaction ()"
},
{
    "commit_hash": "35d2e2a6292f934e9ea5bbd77177f5876d196961",
    "commit_message": "Allow \"asynchronous\" CommitEvents to be created without a TransactionEvent ()\n\nWith CommitEvent.NULL we lose e.g. metrics that are incremented by the child LogAppendEvent and StoreApplyEvents . On the other hand, we don't always have access to the TransactionEvent for the transaction that was committed. To work around this, this allows creating a CommitEvent without an outer TransactionEvent in such asynchronous (\"detached\"?) contexts."
},
{
    "commit_hash": "446ae21fb5124491662149ad9044636169753ec8",
    "commit_message": "Revert \"Much faster IdRange#setBits for numberOfIds > 1\"\n\nThis reverts commit e2619a9c26834f4471321da499d4b131aec94481."
},
{
    "commit_hash": "cc7d725dd159d6c8db660fa1a72b25140bc04a53",
    "commit_message": "Revert \"Multiple ID generator marks within same range are written w/ one call\"\n\nThis reverts commit a59512e5a65e8fd007b604728b8a9d00d735dd6d."
},
{
    "commit_hash": "36abcbee295779906cd345afb7ca0b3fdb7dc7cf",
    "commit_message": "Revert \"Better notification of when free IDs are available\"\n\nThis reverts commit bbba15a962977afcf45e7fa2dbe65c489b3a6084."
},
{
    "commit_hash": "f285e5a682d5c025350a86c2cc23286a9ffbf4aa",
    "commit_message": "Revert \"Faster searching free IDs in a range in most cases\"\n\nThis reverts commit 1867e6900f42775eea4dc72951c48c40a131694e."
},
{
    "commit_hash": "916ba76d0c5bf0c1b734a90298b0ec4927171e27",
    "commit_message": "Multi version page lock handling refactor ()"
},
{
    "commit_hash": "f338900c74f202b51f29f3d91fd1369994f2b6d1",
    "commit_message": "Per database tracing capabilities for MVCC tests ()\n\nCo-authored-by: Rory Steele <rory.steele@neo4j.com>"
},
{
    "commit_hash": "b8abab30fd9780d5376c06d595eb73d75d349a85",
    "commit_message": "Add applyAccessModeToTxState flag to cursors"
},
{
    "commit_hash": "975bdd43f2536cbc3b281d07a8a799666a74a99b",
    "commit_message": "ArrayQueueOutOfOrderSequence refactoring  ()"
},
{
    "commit_hash": "573395d2445ff3d106f519a661e999ae4ddb8afd",
    "commit_message": "Continue traversal of rel chain if observed head page is invisible ()"
},
{
    "commit_hash": "502aecd4c96b9c6ec7340f984d61b7b735adf211",
    "commit_message": "Shortest: Refactor PathTracer & other cleanups ()"
},
{
    "commit_hash": "071e396996ae448f815b53928b07cbb88ea83169",
    "commit_message": "Add Index to envelope header ()\n\nCo-authored-by: Bruno Caricchio Buss <bruno.buss@gmail.com>\r\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "58804c462f9dd139e999a31b159dbd20ff059337",
    "commit_message": "Force recovery after full backup instead of only asking for it ()"
},
{
    "commit_hash": "3cb54e667dd2fe48da2fafff47fca7483fe137c3",
    "commit_message": "expand tests to include all new valid/invalid vector inputs\n\n* L2_COSINE accepts wider range of inputs than SIMPLE_COSINE\n\n* IntegralArrays are supported for conversion\n\n* VirtualValues are supported so long as they are a effectively a\n  sequence of numeric types. Emulate different sources by alternating\n  implementations of ListValue.\n\n* test every combination of valid vectors to ensure the scores are valid\n\n* reorder to have null last, such that NO_VALUE and null are next to\n  eachother\n\n* remove unnecessary asLazy() calls when already lazy\n\n* remove boxed primitives; they were there in case we ever changed the\n  handling between them; however, we won't as null --> NO_VALUE covers\n  the only other case.\n\n* add some non-extreme values as well."
},
{
    "commit_hash": "7ce7159f60de01e6f53d7c0f2ba8fe2a8e8be6d5",
    "commit_message": "refactor vector index procedures to be more generic"
},
{
    "commit_hash": "2547a2630010713a1f3cf4f6588c5c612cd0dc41",
    "commit_message": "use AnyValue in VectorCandidate\n\nExpanding support for more numerical array types.\n\nBe it manually typed vectors, or constructed, cypher can provide a\nListValue (a VirtualValue), which is some wrapper around a container of\nAnyValue.\n\nAlso manually typed vectors, or ones passed through some\nstringifications could be considered a \"mixed\" list. LIST<INTEGER|FLOAT>\nbut would coerse to a LIST<FLOAT> on storage.\n\nThe usual case is a FloatingPointArray.\n\nThe less likely case of IntegralArrays would go via a NumberArray which\nwill incur a copy via a NumberValue first. This could be slightly\navoided with additions to certain base classes.\n\nThe rest would go via a SequenceValue which is an Iterable<AnyValue>.\nThis will cover all the VirtualValues, and a general catchall for\nanything that is vector-like."
},
{
    "commit_hash": "07826ae7022a22f5d62da90a1a1bc0db5e78b33a",
    "commit_message": "Report command handles several dbs and allows for specifying which\n\nBy default report will collect for all databases"
},
{
    "commit_hash": "a0f1bdc4a4786373120387b8b822ab50a61dbf2f",
    "commit_message": "Report command uses tx location from config"
},
{
    "commit_hash": "5c49f48e2c4ad3fadcec8e7f951bd0a0a86e771f",
    "commit_message": "Optimize path creation"
},
{
    "commit_hash": "c69e3fe9bfdb67afd779392f810a1312a070273a",
    "commit_message": "Faster searching free IDs in a range in most cases"
},
{
    "commit_hash": "066e0c54552b8c6fce6263f705e44ef57daa3e3e",
    "commit_message": "Better notification of when free IDs are available\n\nPrevious boolean approach was prone to race between update and scanner,\nwhich became obvious, or was exposed, with the recent changes\nto batch updates."
},
{
    "commit_hash": "bf7de8d844293228ad8d1ee13949fd4df26b5062",
    "commit_message": "Multiple ID generator marks within same range are written w/ one call\n\nThe best case of marking fully consecutive IDs seems to be ~10x-15x faster."
},
{
    "commit_hash": "a2907122963fd09b71c6ab137ff2dc301677722b",
    "commit_message": "Much faster IdRange#setBits for numberOfIds > 1"
},
{
    "commit_hash": "a13ffb114b14cc02822dcf99faf25b99bf90fc56",
    "commit_message": "Fix unicode escapes and input position in antlr\n\nAdds a preparser step that replace unicode escape sequences before parsing in antlr.\nAnd make sure to keep track of input positions in the original input while doing that."
},
{
    "commit_hash": "cdc0b48a5192373183a60ee5099bbd1ff4a33713",
    "commit_message": "Disable ServerCommandIT.startShouldFailWithNiceOutputOnPortConflict on Windows\n\nThe windows service does not propagate exit codes to the bootloader so it's not possible to test"
},
{
    "commit_hash": "fa6bfd2c7bf6214055287722754679d0cdbbbff8",
    "commit_message": "Adding tests for a upgrading reading transaction to a writing one\n\nCo-authored-by: htsedebenham"
},
{
    "commit_hash": "1e56b63ec95401f88570723c59986f36a014f512",
    "commit_message": "split index file filters\n\n* couple of implementations of the same thing\n* using native index filter when really asking if a lucene index\n        !native may not be lucene (multiple false paths)\n* some lucene descriptors missing\n* improve the tests"
},
{
    "commit_hash": "8b9b02af3d20ff82ae386ac54dae394b8e9fc887",
    "commit_message": "fix tests to reflect new messages and order"
},
{
    "commit_hash": "7aad4396c737938aeae0273975252be3a514ac8f",
    "commit_message": "reorder rolling upgrade protections\n\nSlightly reorder the checks for the index prototype to ensure the right\ninformation of issues is presented.\n\n1. is the index provider usable\n2. is the schema valid\n   a. can the entity type be used\n   b. are there repeated tokens\n   c. is composite valid\n3. has name\n4. no schema already exists\n\nEnsuring the name, asserting it's present, then getting it seemed odd.\nIf for whatever reason we enter this state: asserts can be enabled or\ndisabled. If enabled, the assert throws. If disabled the Optional::get\nthrows, but not warned as an Optional::isPresent check has been made (as\nthey're indended to be used together).\n\nThus changing this to the more explicit Optional::orElseThrow by itself.\n\nThough this seems like a change in behaviour in the case of enabled\nassertions, it is not. Ensuring the name will never set a null name as\nSchemaNameUtil::generateName cannot not return null."
},
{
    "commit_hash": "94605c70fbadcd34eaa5c17397e48889a3d28eb7",
    "commit_message": "add minimum required kernel version to IndexProviders\n\nThe availability of an index provider is determined by the kernel\nversion. Since all other indexes were part of 5.0, they were assumed to\nbe supported.\n\nVector indexes are the only new index since 5.0, and some basic\nprotections were put in place; however, with the coming addition of\nvector-2.0, it would be nice for the index provider itself to be able to\nprovide whether or not it is supported.\n\nThis adds the minimum required kernel verison to support a particular\nindex provider. This information can be used for rolling upgrade\nprotections, rather than having many nested conditionals."
},
{
    "commit_hash": "03e35fe05a3fe17427f31bce1e636e49a9b9a9b4",
    "commit_message": "Bootloader waits for webserver to start before detaching\n\nThis will allow more startup error to reach the user"
},
{
    "commit_hash": "6a6e32b9a6152993650b7bf9372a871e5891f402",
    "commit_message": "Add semantic check & proper function signature for graph.byName & graph.byId ()"
},
{
    "commit_hash": "bdd6d3d8c91e7765adf4f752f0b1fdfa9b894fd1",
    "commit_message": "Accumulate benign Bolt errors ()"
},
{
    "commit_hash": "71681613c633d9ff8ac90105fce50eaf0f0e11fe",
    "commit_message": "NMT profiler as part of hidden \"report profile\" command ()"
},
{
    "commit_hash": "2cb273b249909653c587cc1266e6cca69f1afa49",
    "commit_message": "rename COSINE singleton to SIMPLE_COSINE\n\nThis implementation of COSINE is backed by lucene's COSINE.\nIt will effectively renomalise the vector each time it is seen.\n\nShould be able to differentiate between COSINE implementations; thus\nchanging their Object::toString for clarity in tests."
},
{
    "commit_hash": "301cbaad21b98b7cbb72048b20f7d5a97a7c9c79",
    "commit_message": "refactor and update vector index creation tests\n\nMake tests more generic in preparation for the introductions of\nrelationship vector indexes with vector-2.0\n\nPrimarily moving the tests into a base class that will be implemented\nfor both node index and relationship index.\n\nShould test for every supported entity type, version, and similarity\nfunction.\n\nCoreAPI cannot specify index provider, thus must only be run on latest\nsupported by the db."
},
{
    "commit_hash": "724ef284253941060318f5bddb0132c99cfbe760",
    "commit_message": "reorder IndexProviderMap\n\n token, range(default), point, text, trigram, fulltext, vector\n\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          property indexes\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           native                           lucene\n\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      text based\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               query performance               semantic search\n\n* keep consistency\n* keep like-indexes together\n* add overload to loop\n* have null-check on all\n* add missing test cases"
},
{
    "commit_hash": "663678f6e42602f9c43fe4bc5078cdc2dbead646",
    "commit_message": "propagate VectorIndexVersion usage in tests"
},
{
    "commit_hash": "3b7fb36c9ca56d32152cca1524f85f92b360b5d2",
    "commit_message": "propagate VectorIndexVersion as source of truth"
},
{
    "commit_hash": "2e2b2591b3381b80318cbe5d6bd2f66a9947b66b",
    "commit_message": "OutOfOrderSequence optimizations for mvcc ()\n\nMVCC database needs to know ids of uncompleted transactions at the start of every transaction. It does it by asking reverse snapshot from OutOfOrderSequence.\r\nThis PR adds some improvements to that functionality, without affecting performance of non-mvcc stores (hopefully).\r\n\r\nSimple cache for ReverseSnapshot. It cached in the volatile filed and every time new number is offered the field is reset to null. Readers if observe null create and cache snapshot for use by other lucky readers. This reduces number of actual scans for missing items in the sequence.\r\nSequenceArray now keeps track of number of missing numbers, so it can create array of exact size when missing items/or snapshot is requested, thus avoiding extra allocations.\r\nNot entirely related, snapshot of sequence is simplified and doesn't contain meta anymore, because it is never used, also avoiding some extra allocations."
},
{
    "commit_hash": "600069633267868ba3c581f73c1428168185ab0a",
    "commit_message": "Adjust Bolt to new error"
},
{
    "commit_hash": "e1c81199b17ac914ec5dad8ec40367bb6237b5a6",
    "commit_message": "Code review: Improve readability"
},
{
    "commit_hash": "b3f88cdb16bbcf1ea1ef83fb42cc97d01922d02d",
    "commit_message": "Consolidate normalizing errors"
},
{
    "commit_hash": "01b97fced861abf460fcc1e69749d6ee85cfe3bd",
    "commit_message": "refactor vector similarity functions from enum into singltons\n\nVectorSimilarityFunction::name should be used for refering to the\nfunction name, rather than toString()"
},
{
    "commit_hash": "08a047b84f552d2c17b221a903b5af888e6bc2d3",
    "commit_message": "move all vector validation into interface"
},
{
    "commit_hash": "045c37454959ca9526ab378dc5b73dc8f5827bf8",
    "commit_message": "add vector similarity function interface"
},
{
    "commit_hash": "161b58339165bea52159144b357a2092ce6263aa",
    "commit_message": "rename test utility class and api\n\nYears ago I wrote the utility to help generate tokens for the partition\nscanned tests, but it was useful to extract and use elsewhere.\n\nI was mistaken at the time thinking that \"token\" only referred to\n\"label\" and \"relationship type\", and \"property key\" was not a token;\nthus I called it \"Tags\".\n\nProperty keys are tokens, thus renaming this class.\n\nAlso returning a `List<Integer>` is awkward, almost everything using\nthis would prefer to take an `int[]`, thus changing the API to return an\n`int[]`.\n\nBoth have been pet peeves since."
},
{
    "commit_hash": "27bd9cfcefda7b210f8e5688fc33690741698201",
    "commit_message": "use VectorCandidate in similarity function signature"
},
{
    "commit_hash": "db2cf5921e41a3b1ca4a802e3f4cb85dc4dd9bba",
    "commit_message": "Do not version context activities by default  ()"
},
{
    "commit_hash": "c8b5321baa3f23f76df6c5bc2d908b3ffc17e332",
    "commit_message": "override numerical scalar and array value type returns\n\n* Reorder to keep methods returning small to large in that order.\n\n* Specialise such that the value type of a returned element is the\n  smallest scope it knows of, rather than AnyValue.\n\n           ArrayValue --> AnyValue\n          NumberArray --> NumberValue\n           FloatArray --> FloatValue\n\n* Allow NumberValue to convert to a float via floatValue.\n\n  Though it would be possible to go via asObjectCopy().floatValue()\n  this makes a copy via boxing to a Number, before calling a conversion\n  method. This is quite wasteful.\n\n* Use java 14's enhanced instanceof to cast at the same time as the\n  instance check."
},
{
    "commit_hash": "7c09b21746750a7b82feca7c66b964d339c5c7e8",
    "commit_message": "One less copy when merging sets in PrimitiveLongCollections ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "82c5ecb194d2f1c6c6a2e4b013f0d2530a03d789",
    "commit_message": "FileSizeThreshold is lenient regarding non-existent files\n\nJust as the other thresholds are."
},
{
    "commit_hash": "7810ccd896c76506d8c1a30524437c2c9bfccf09",
    "commit_message": "RandomExtension prints seed outside of test ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "0aa5ff36fa2309167f62c5731b08cb97d590e861",
    "commit_message": "Do not preallocate regional page files in multi-versioned stores ()"
},
{
    "commit_hash": "cecff53256ff5e51bf5e1e76e3c9fa3f9183a92d",
    "commit_message": "Verify that the selected database is a constituent of the session database in the transaction ()"
},
{
    "commit_hash": "ef605eece6c247d68f5d70c560ddac53048b2ab9",
    "commit_message": "Add additional end-position to StringLiteral and SensitiveStringLiteral"
},
{
    "commit_hash": "9b4d8b02db40833990b23cd5a2370f7bff4fa5c3",
    "commit_message": "Add a config value `internal.initial.dbms.default_database.enable` which\ncontrols whether or not to create a default database on first start of\nthe dbms.\n\nThis flag is read by the SystemGraphInitializer and dictates whether to\ncreate a Database node and allocations in the Topology model.\n\nSeveral changes to test extensions were also required to support the\nlack of a default database."
},
{
    "commit_hash": "034ccf32440696dab090e334d0b691bdb128912d",
    "commit_message": "Backport restoring tentative index state on startup to 4.4 ()"
},
{
    "commit_hash": "9ae47b541a0546ea07a23650fead188fcf9923fe",
    "commit_message": "Make `DatabaseAdministrationException` abstract ()"
},
{
    "commit_hash": "edad82aff58cbbeca8f2605170f1a2a68bc98ca5",
    "commit_message": "Use lazy cursors created only when necessary in storage engine Loaders ()"
},
{
    "commit_hash": "b667aa275503b10dd25e4503d0708a42c6bd6f8d",
    "commit_message": "Enforce zeroes in the end padding section of segment ()"
},
{
    "commit_hash": "77c1c55492affbe9989edae3d2261df89dcd5e97",
    "commit_message": "Revert ceec643de1310fcdfe289fd8fc08c52061bf3b7e"
},
{
    "commit_hash": "c51be7c68162ae752cfc948bc9a98cd39c597eed",
    "commit_message": "Update --from-path flag description ()\n\nTo make it clear that if the directory contains multiple backup chains (either single full backups or full w/ sequence of diff backups), it will pickup the \"latest\" backup chain (so it is not by timestamp or anything similar either)."
},
{
    "commit_hash": "d70bd9eafa9ad59984d72aa67880702b2f256b79",
    "commit_message": "Better handling of added columns in CALL In TX on composite\n\nWe add 1-2 artificial columns during planning of CALL In TX on composite The runtime stripped away only 1."
},
{
    "commit_hash": "29f372230db4b99284b6fbadfa4c824afe032d1a",
    "commit_message": "Use LongAdder and LongAccumulator in DefaultIndexUsageTracking ()"
},
{
    "commit_hash": "8df8d76afcdf684e611a1c242b55e6c6c5dfd748",
    "commit_message": "Clear validator checked map instead of creating new ()"
},
{
    "commit_hash": "fb137ece4a02435d54486eae7f844fdfc1225450",
    "commit_message": "Guard against NPE on overridableSecurityContext ()\n\nIf the transaction has been closed (or not initialized yet), then overridableSecurityContext will be null and trigger a NPE if we try to use it.\r\n\r\nThis change does not remove the failure - since that is correct - but make the throw exception to be more descriptive, indicating the root cause (trying to use it when it is closed) more clearly for users."
},
{
    "commit_hash": "c024255bd8546b9b1ee6c4819af7fccedebfc68e",
    "commit_message": "PROFILE is not supported on composite databases ()"
},
{
    "commit_hash": "be79c938bd49225536d58beda4e8f8c2c80ca267",
    "commit_message": "Property Rules: Add support for the `IN` and `NOT IN` operator to Kernel for Property Rules ()"
},
{
    "commit_hash": "8527700892a6a6af93600df36939f2f37bc8fe12",
    "commit_message": "Make checkpoint logs to be at least 2 segments size ()\n\nPart of introducing the new Envelope model for our logs, we need our logs to be able to fit at least 2 envelop segments."
},
{
    "commit_hash": "a1fc14ec9562b71a0d7e4c4e2735e91a1b12e455",
    "commit_message": "Add support for HTTP/2 in server ()"
},
{
    "commit_hash": "61946aa0a8d77a65223d28018622a1f8e2fd9435",
    "commit_message": "Fix read snapshot loading for multiversioned page ()"
},
{
    "commit_hash": "b0d6b302c284cfcdac74c1748788db5da59d797f",
    "commit_message": "MVCC transaction event listeners ()"
},
{
    "commit_hash": "880ac0be79c145e0923866735470dd7c241fc201",
    "commit_message": "Optimise java value mapper"
},
{
    "commit_hash": "45ec1604691e7bb65f8665faf38706b19b326721",
    "commit_message": "Show notifications in Cypher Shell\n\nAdds argument `--notifications` to Cypher Shell to enable\nnotifications in interactive mode."
},
{
    "commit_hash": "edba0783cdc815e8f98f4286ead80954cc03b5c4",
    "commit_message": "Fast transactional IDs rollback ()"
},
{
    "commit_hash": "79819a39d1768d1536fc7918b423ebd9f44a73bc",
    "commit_message": "ProcessedQueryInfoCache should care about all things the AST cache ca\u2026 ()"
},
{
    "commit_hash": "e3ed8c11f3b4a2488a79b073ef4c305cbb28522e",
    "commit_message": "Periodic cleanup errorprone cleanup ()"
},
{
    "commit_hash": "45cc170d60ece104128f5be9d5ce286b1926adc9",
    "commit_message": "handle SequenceValue to FloatingPointArray conversion for vectors"
},
{
    "commit_hash": "ed7db41cac7cab7e493bbd8ed13b90568e61d8a7",
    "commit_message": "Correct synchronization in multi-threaded progress printing"
},
{
    "commit_hash": "90b65513c93763bc3587a6e3457efe62ffbb67a8",
    "commit_message": "Make PGPathPropagatingBFS testable ()"
},
{
    "commit_hash": "aebe6c4148ff2ba5c6e0128a01dadc44887413f1",
    "commit_message": "Handling of ON ERROR BREAK in composite"
},
{
    "commit_hash": "d74b674246cec30732b730a01b05853e380fdcc9",
    "commit_message": "Introduce the possibility of performing batch unallocation for the sequence of IDs ()"
},
{
    "commit_hash": "308328db3aa7362a1416069eac03f52a708d41db",
    "commit_message": "BlockBatchInputTest correctly counts number of read entities"
},
{
    "commit_hash": "db90108089847774dd27cd5cc104454249a0939f",
    "commit_message": "Make Lighthouse log levels use the main logging config ()\n\nLighthouse logging was using the 'discovery log levels' setting that was used for Akka, but that was introduced because we didn't control the logging, and it was too noisy. Now if the log levels are wrong, we can fix it in a normal way."
},
{
    "commit_hash": "310db0dacb702b708d88448bff88879b03dcb346",
    "commit_message": "forward session database reference to constituent transaction factory ()\n\nCo-authored-by: linneaandersson <Linneamaria10@gmail.com>"
},
{
    "commit_hash": "c1ee65f162e4b75f0651d11353b274290e04a4b0",
    "commit_message": "Require thar all notifications have a  message."
},
{
    "commit_hash": "05daab591efe9ff44f38a52f3725c4158c01fb0e",
    "commit_message": "Rename method to follow standard format."
},
{
    "commit_hash": "712f3105d9545b98dcc9218b42204d3852a0b0fa",
    "commit_message": "Simplify NotificationAcceptanceTest assertions."
},
{
    "commit_hash": "727b1a14fb66b187b2ee897d54a38e612b1d86e8",
    "commit_message": "speedup foreach"
},
{
    "commit_hash": "de928b69af6571f79333f663eff7eea1dc39c8a2",
    "commit_message": "Preserve relationships (and nodes) when constructing paths"
},
{
    "commit_hash": "4c8deaf288feca21f3a90b58d2fbc98eaf7aac34",
    "commit_message": "refactor: moving cypher-related `toString` stuff to a more suitable home ()"
},
{
    "commit_hash": "69efa5e0a51acaf8dcccb7fd1e748f9f19403378",
    "commit_message": "Mvcc counts entities by scan ()\n\nBenchmarking of versioned count store demonstrated that it is very expensive to keep versioned counters. Updating all counters produced by transaction typically takes few times more CPU then updating indexes and removing them gets about 20% performance on LDBC SF100.\r\n\r\nWith this in mind we decided to change approach for countstore. MVCC will be using unversioned count store, but counters from it will be used only as statistics for cardinality estimation and in other appropriate areas.\r\nWhenever counters are required for user visible input they will be calculated by store or token index scan. Such scan already adheres visibility rules, so produces correct results."
},
{
    "commit_hash": "20a506a532e024992d9cb920fe44ffcdd112116c",
    "commit_message": "Put a thread dump to test dir on failure in cluster extension ()"
},
{
    "commit_hash": "6e1ae5b9d476067546aa971cce6a6cf4a018fbe4",
    "commit_message": "Dynamic selection of reused IDs range based on available IDs ()"
},
{
    "commit_hash": "71f37974feced9417759f86a7652bcbd4f6dc001",
    "commit_message": "ProcessedQueryInfoCache bug ()"
},
{
    "commit_hash": "0e8ac5577a65a34fb20d161363b934d4cf9ca4b7",
    "commit_message": "Disable Windows test for local scheme resolution in cloud storage layer ()"
},
{
    "commit_hash": "6fdd5b27756c8313e07bbf43f2d10261eb8b32db",
    "commit_message": "Introduce separate metric to count page flushed by background evictor ()"
},
{
    "commit_hash": "a3283d4d7ed0a3d030d92b5a8cae1377755a93c5",
    "commit_message": "4.4 Flush and use lastCommittedTxId for determining required transactions range ()"
},
{
    "commit_hash": "d245cb9062be7575cec322975c554029e76e7fa8",
    "commit_message": "Do IDs unallocation once per transaction ()"
},
{
    "commit_hash": "7afc64b16f0aee86fbd31f89156a60b75fe62749",
    "commit_message": "when pretty printing text arrays, make string or char in the array prettified as well. ()"
},
{
    "commit_hash": "f2d55b958844cfc95cdc4897d75f40397ee5d971",
    "commit_message": "Adds GBPTree Writer#yield\n\nFor the ability to reduce contention with other parallel writers.\nIt's particularly beneficial after write calls knowing that the next\nwrite call to it may not be immediate, perhaps a costly or I/O dependent\ncalculation of the next data to write.\n\nAlso adds this call up the stack for IndexUpdater/IndexImporter."
},
{
    "commit_hash": "cda18c00f0696cec8390a6c71003a479763ea17c",
    "commit_message": "CALL IN TX on Composite \n\n- Modifications to Fabric Stitcher\r\n- CALL IN TX executor\r\n- The first batch of ITs\r\n\r\nCo-authored-by: Hannah Tse Debenham <31847376+htsedebenham@users.noreply.github.com>"
},
{
    "commit_hash": "bfcd0305f47892b4a4b959247d99b302a5a0c82d",
    "commit_message": "Creation of scheme-based file system abstraction ()"
},
{
    "commit_hash": "18c9e5aacc79bfa27cbeb54f8a32309e22394779",
    "commit_message": "More accurate index population progress\n\nPreviously the progress of an index population was based on how many\nentities had been fed into the pipeline, where generating index updates\nand finally writing them to the index that it's building, is further\ndown the line in the pipeline.\n\nThis would often have the reported progress be ahead of the actual progress,\nthe amount would depend on number of populating threads and batch size.\nFor example it would almost always be the case that it would seem that\nan index population would start at 10-20% or so (depending on threads\nand also dataset size), getting to 100% and then staying there for a\nlong time before actually completing."
},
{
    "commit_hash": "a91f934670d409585dcd9db435bb4f0ba9343ea7",
    "commit_message": "Cheaper RelationshipSelection creation (for multiple types)"
},
{
    "commit_hash": "04b7bcae194433292438256df652312daf3577a7",
    "commit_message": "Cypher Shell custom history file"
},
{
    "commit_hash": "4aec590d32f212eb9c02dd71b06a0d8ac375c898",
    "commit_message": "Update DocumentedDefaultValue of dbms.record_format to avoid ambiguity"
},
{
    "commit_hash": "fc5ee3ce50df1a5fedb10b5766b04d1d4dbae5fe",
    "commit_message": "Register created tokens directly when they are applied to store\n\nThis is done for two reasons:\n1. There was a small window, where the token is committed, before it is added to the TokenRegistry, where we could see a\npossible corruption if that thread crashed, while another concurrently accesses that token.\n2. In preparation for removing special handling for ReplicatedTokens. There was special application for those in\ntransaction application already, now all token transactions do the same and there would be double adding for the\nreplicated token case in the future.\n\nAs a side effect any token creation not going through the regular\ntransaction flow now needs to add these tokens to the registry\nthemselves."
},
{
    "commit_hash": "7824386bca5078aba6f4c2ea977e2b167c273c44",
    "commit_message": "Update descriptions of db.index.fulltext.queryNodes,<...>.queryRelationships,db.index.vector.createNodeIndex ()\n\nCo-authored-by: Matthew Parnell <matthew.parnell@neo4j.com>"
},
{
    "commit_hash": "13812a735aae0fa110f3366b2700c7eeb71eaffa",
    "commit_message": "Introduce logic for linked users in ldap\n\nBackend work for enabling linked users.\r\n\r\nImpersonation of users with external auth will not be allowed in this first part.\r\nBlock native auth when user has only external auth"
},
{
    "commit_hash": "ae7fd96aa1f27c116789556e30593f267558c523",
    "commit_message": "Don't treat LinkageError as fatal when loading plugin classes. ()"
},
{
    "commit_hash": "8884f5e52869e1975d980d8545e24366bb9823ed",
    "commit_message": "adds ExpandInto support to StatefulShortestPathPipes"
},
{
    "commit_hash": "285ca41392c2f0413f2c5d8f2926c0004ff12645",
    "commit_message": "introduce graph reference ast ()\n\nCo-authored-by: linneaandersson <Linneamaria10@gmail.com>"
},
{
    "commit_hash": "d774ab1bab704be5c4fc2359aac9d4c29593249b",
    "commit_message": "Block importer gives proper memory space for the id mapper"
},
{
    "commit_hash": "6a64acb4eabd3de5c1a5c283b02395bbdd4adbf5",
    "commit_message": "Better progress reporting code for preparing IdMapper"
},
{
    "commit_hash": "0a19f2504cadf5ffc9b6ca18eab8714bafdd8cbc",
    "commit_message": "Block importer uses EncodingIdMapper\n\nInstead of its own home-grown Id mapping, which turned out to be\na bit too inefficient for string IDs."
},
{
    "commit_hash": "3c67b231ef5140aac9ddcccbf7ad489398b67516",
    "commit_message": "Restore tentative state of uniqueness constraint indexes without owning constraint on restart ()\n\nPopulated uniqueness indexes whose constraint hasn't (yet) been created shouldn't be treated as online. They are left in a tentative state until they are activated by the creation of the constraint, but currently they will be online after a restart. This change maintains the tentative state for indexes that still don't have an owning constraint upon restart.\r\n\r\n* Always flip populating uniqueness constraint index proxies to tentative if they have no owning constraint\r\n* Online uniqueness indexes without an owning constraint must also pass through tentative state on startup"
},
{
    "commit_hash": "9f1caef3a5e7825d22510f2d68423a28dc2b95e9",
    "commit_message": "Fabric executor refactoring\n\nThe purpose of the refactoring is preparation\r\nfor execution of CALL IN TRANSACTIONS.\r\nCALL IN TXs  execution logic will share a lot\r\nof code with standard query execution.\r\nAlso, Fabric Executor has grown to a really bloated\r\nclass, so we don't want to add more complex logic\r\nthere.\r\nSo this commit reafactors the standard query execution\r\nlogic out of Fabric Executor, so it is more easily\r\nshared between standard query execution\r\nand soon-to-be-added CALL IN TRANSACTIONS."
},
{
    "commit_hash": "b62f6e7f15760d2b6a752a85cafe8407aef08324",
    "commit_message": "TransactionIdStore should be updated whenever the system database is started ()"
},
{
    "commit_hash": "5e1cfbab245bebf45b2d8c1c19e2fd8833b92cf4",
    "commit_message": "Errorprone cleanup round ()"
},
{
    "commit_hash": "87cb1ceb9175ea1a09ae2f9932b54cf87ea074ca",
    "commit_message": "Remove DocumentedDefaultValue annotations and implementation. ()"
},
{
    "commit_hash": "9a4af561576925ed90f3e7d72e8712a01372a8cd",
    "commit_message": "Replace old test only equals/hashCode implementations with specialized query based ()"
},
{
    "commit_hash": "edfa0231edb2863a35dbcbcb7db529ed9cada421",
    "commit_message": "Splits notifications for semantic analysis ()"
},
{
    "commit_hash": "aac28c67a20aa73343500ef899f30918cc6481c0",
    "commit_message": "Plumb the JobScheduler into the VectorIndexProvider"
},
{
    "commit_hash": "9a29d59074609743ba2c653dfd5a970be09f3c24",
    "commit_message": "Change vector index merge factor on index state\n\nPopulation should have higher merge factor to reduce the population\ntime.\n\nStandard should have lower merge factor to reduce number of segment\nfiles and increase query time."
},
{
    "commit_hash": "dab31d22da3e987dcefcb4ca481b10cccbe70b4f",
    "commit_message": "Report command handles several dbs and allows for specifying which\n\nBy default report will collect for all databases"
},
{
    "commit_hash": "a0d197e8e8f85a2ff6728887eed4a01e1899d967",
    "commit_message": "Handle edge cases in AggregateDeltas when aggregation starts at the end of the leaf."
},
{
    "commit_hash": "b183c4ebf7b8089c6b50a597f35040c302cbe13e",
    "commit_message": "Allow migration from 4.4 store to block format"
},
{
    "commit_hash": "0ea0ec027a72886402d3e91820d17c9b83d21ebb",
    "commit_message": "Forward preparser options to constituents ()\n\nCo-authored-by: Hannah Tse Debenham <31847376+htsedebenham@users.noreply.github.com>"
},
{
    "commit_hash": "1dbd6e4b20cbffbc91eb39aa4d2a7761b1bd7dae",
    "commit_message": "Add --mask switch to DumpLogicalLog tool ()"
},
{
    "commit_hash": "3fa362ca39120e04d2a7a44a6018197e34f7959e",
    "commit_message": "Add fallback that reads consensus index from transaction when reading pre 5.7 checkpoint ()"
},
{
    "commit_hash": "c8161b4c77accf01c5aa9c90d3065ceabff526cb",
    "commit_message": "Passing a logprovider to CheckBackup ()"
},
{
    "commit_hash": "a03e76c4c7c66026ee8e38cb622eaa615ee06d37",
    "commit_message": "Add preparser flag for label inference"
},
{
    "commit_hash": "eda0ae166f5315f17fd1ac4e8783d7e2e77b5b28",
    "commit_message": "5.x constituent query executor ()"
},
{
    "commit_hash": "cbfd28e61218e4c42e31cd1fc0cd6fc977801f4f",
    "commit_message": "MVCC dense node relationships update schema ()"
},
{
    "commit_hash": "a3ef4144d14a626ac85d2a1be43fcceadadedc3f",
    "commit_message": "Rewrite MuninnPageCacheMultiVersionIT::flushOnEvictionStillVisibleVersionedPages ()"
},
{
    "commit_hash": "861b8d8f4ad89def09dfe4c7e491078d687288e4",
    "commit_message": "Replace o.n.c.ProgressReporter with ProgressMonitorFactory/ProgressListener ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "22735354eb7b2970a47a79a118cbbc2ca3c75752",
    "commit_message": "Allow parameters for index and constraint name in create and drop commands ()"
},
{
    "commit_hash": "2a0e82b2f3427c86e87ab492651a138dce89950f",
    "commit_message": "Code review:\n- make class into record\n- add type test\n- fix name"
},
{
    "commit_hash": "9e20b4fcd9e539d098e1b74d5e9ae71aa7945479",
    "commit_message": "Clean up of BFS:\n\n- Break out to different files.\n- Separate concerns\n- Don't use \"raw generics\"\n- Use Direction instead of SemanticDirection"
},
{
    "commit_hash": "13fc9efc644f68c1fa9df8be5d937446beda5343",
    "commit_message": "Make more use of StorageNodeCursor#labelsAndProperties\n\nWhich gets an atomic view of labels and properties, as well as\nrequiring only a single page access instead of two, in block format."
},
{
    "commit_hash": "a0dbaf7fe7998cdc200a4c4053e7f5e1fbef766d",
    "commit_message": "CIDR range support for GRANT/DENY LOAD ()\n\nThis PR introduced CIDR range support for GRANT and DENY for LOAD: e.g:\r\n\r\n`GRANT LOAD ON CIDR \"127.0.0.1/32\" TO role`\r\n\r\nThere is also some significant refactoring here:\r\n\r\n* URL validation is already insufficient for validating security, since DNS can change between checking and opening. The  existing implementation was therefore checking URLS _twice_. This PR removes that and passes the stream through the QueryContext instead of the URL so we only have to check it once. This restructures the LOAD CSV implementation a quite a bit. \r\n\r\n* We can no longer take the short cut where there are no blocklist rules present though as the GRANT / DENY logic is more complex. \r\n\r\nCo-authored-by: Therese Magnusson <therese.magnusson@neo4j.com>"
},
{
    "commit_hash": "3770629ef967902637649aa6e7eb56ffa86a259a",
    "commit_message": "Use BFSPruningVarExpandCursor for variable ExpandInto ()"
},
{
    "commit_hash": "1222f92b826a984940e7e888809d4801834a6ef7",
    "commit_message": "Update description string generation for config settings for editorial reasons ()\n\n* Improve the description strings produced for settings\r\n* Update some constraints' descriptions to work grammatically in the produced strings\r\n* Remove constraints that disallow naming the system database in some settings (already checked by the DATABASENAME parser but collection parsers didn't invoke the underlying parser's validation)\r\n\r\nThis introduces some discrepancies with the docs pages, which should be addressed when they are highlighted by docs testing\r\n\r\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>\r\nCo-authored-by: Nils Ceberg <nils.ceberg@neo4j.com>"
},
{
    "commit_hash": "de0ba6226e2e294695eccd9f9555c33350d10d81",
    "commit_message": "forward dynamic graph functions to planner ()"
},
{
    "commit_hash": "79c3e7477feeba2757dc3ce8e6ead85149fa08f0",
    "commit_message": "Fix regression in equals"
},
{
    "commit_hash": "f0be2bf0b6f0de41ac8bffd8fee4abba569cc6b5",
    "commit_message": "Add notification message for RepeatedVarLengthRelationshipReference\n\nBy oversight, this was only done for RepeatedRelationshipReference before."
},
{
    "commit_hash": "744603e76773143ae549ee0d07a351b14605a917",
    "commit_message": "Periodic dependencies bump ()"
},
{
    "commit_hash": "5a2e1690f348db47428c67ecffb4997f67e89e86",
    "commit_message": "Update Import --normalize-types flag help\n\nTo make it clear that it does not affect array-types."
},
{
    "commit_hash": "33f2fcdab8d73175f20b9855720df068264d2b45",
    "commit_message": "Update Import --normalize-types flag help\n\nTo make it clear that it does not affect array-types."
},
{
    "commit_hash": "27154d32e2aebfb28f408edef2490ac975cba599",
    "commit_message": "Squash reservePartition implementations\n\nBefore when we had two versions of the reservedPartition API, most\nimplementations were using a private method that would be used by both.\nNow that the deprecated API is gone, we can squash the remaining one\nwith this internal private method to simplify/clean-up the code."
},
{
    "commit_hash": "c3f0b948535b8c7f96fd1c761575e6586e6d123f",
    "commit_message": "Remove fallbackRead from Partitioned*IndexCursor[Scan|Seek]\n\nNow that the old API relying on CursorContext only is gone from\nPartitionedScan, those fallback read fields are not used anymore and we\ncan remove them.\n\nSince the read passed to the constructor is only used at construction\ntime to check if the current tx has changes, we can also move it outside\nand clean-up the constructor too."
},
{
    "commit_hash": "f7c021b15ea3dce06f4f9fe3dfa5c01eeb427cfc",
    "commit_message": "Extract IndexProvider.Adaptor as BaseTestingIndexProvider\n\nAdaptor implementation is currently only being used as the base\nimplementation for two testing providers in kernel-it, so this moves it\ninside the kernel-it package closer to where it is being used in order\nto clean-up IndexProvider a bit."
},
{
    "commit_hash": "133ef848a9ccefa83b2b8429deaf03cd6b6377c1",
    "commit_message": "Add support for AllowSameNode in pipelined runtime"
},
{
    "commit_hash": "2a59424272487eb7c578b934e25bd6574719f97b",
    "commit_message": "AllowSameNode sort of support in Interpreted\n\nAlso make Skip and Allow actually do\nwhat they claim to do."
},
{
    "commit_hash": "b0bffd0b8a63a276fa96e0edbbd75fb3e9646c0c",
    "commit_message": "Revert array nomalization back to 5.13 behaviour\n\nArray extractors will not normalize their types anymore and will keep\nthe types were originally set to."
},
{
    "commit_hash": "b235c5d83b955227dd590da6d428f1eb0a9ab278",
    "commit_message": "Implement CancellationChecker for query router"
},
{
    "commit_hash": "8957b100bc08d52323daa59fcf5c7c78ff930bed",
    "commit_message": "Code review: Additional test"
},
{
    "commit_hash": "adc96469c800cd04ba30c041c6ecb1b981adc5b7",
    "commit_message": "Fix nested exception handling in generated byte code\n\nWhen generating byte code for nested exceptions, e.g.\n\n```java\ntry {\n try {\n   //run safe code\n } catch (Exception e) {\n   //handle inner error\n }\n} catch (Exception e) {\n  //handle outer error\n}\n```\n\nwe were generating the exception table in the wrong order. In the spec it says:\n> _The order in which the exception handlers of a method are searched for a match is important. Within a class file, the exception handlers for each method are stored in a table (\u00a74.7.3). At run time, when an exception is thrown, the Java Virtual Machine searches the exception handlers of the current method in the order that they appear in the corresponding exception handler table in the class file, starting from the beginning of that table._\n\nThis lead to some cases where the wrong error handler was invoked for nested errors."
},
{
    "commit_hash": "1f01400ba7216858d91f0f80ebcf2591b4530814",
    "commit_message": "Serve up to last committed transaction in tx pull requests ()"
},
{
    "commit_hash": "cc77f2cf92889d7c959aee96a4d19d678aa80ee6",
    "commit_message": "improve the descriptions of --skip-bad-entries-logging and --bad-tolerance ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "03a451c7eea44ba1cc68c4c8dc2150c404b2fd77",
    "commit_message": "Special handling of routing errors in Query Processor"
},
{
    "commit_hash": "46d8689dfa7a4ee9926568514d076ef9c7fd6cc1",
    "commit_message": "SHORTEST K ExpansionMode (plus refactor) ()"
},
{
    "commit_hash": "3005fc47f87a88dc7465f4f74f582e1c38c48746",
    "commit_message": "Interrupt background index sampling when stopping IndexingService"
},
{
    "commit_hash": "07f8b9c2361f669fbb3961a946ee60365ab2397d",
    "commit_message": "Add back text removed by preparsing in QueryProcessor"
},
{
    "commit_hash": "d1e47846758002d7783a8cd3de16d175be70926f",
    "commit_message": "Property rules inequality administrative runtime ()\n\nCo-authored-by: Hannes Sandberg <hannes.sandberg@neotechnology.com>"
},
{
    "commit_hash": "42b4d4fb91eb86e06a29536f6c378002fed2ab3f",
    "commit_message": "Make TransactionConflictException and rethrow in on transaction commit failure instead of wrapping it ()"
},
{
    "commit_hash": "1c19affb3608ace94f6b70d60fd7efba11aa7758",
    "commit_message": "Do not create separate rollback context for storage engine rollback ()"
},
{
    "commit_hash": "0d7b862aaa41293883273c109c7abe6d4867f8ee",
    "commit_message": "Add space between sentences in setting description. ()"
},
{
    "commit_hash": "5f8b4f2c83c9cfc7381a24a85722fdfd5ea63436",
    "commit_message": "Pass in config when unpacking artifacts\n\nThis is used by the TransactionLogFiles to determine the highest transaction id after recovery."
},
{
    "commit_hash": "da56669f922bc541a0613a04fd5d3918251269d2",
    "commit_message": "Ensure we recover legacy backups"
},
{
    "commit_hash": "4d402fa2abc0af2a3d641f8361bfbc22b90a0464",
    "commit_message": "Introduce URLAccessChecker, remove URLAccessRule(s) from ExternalDependencies ()"
},
{
    "commit_hash": "60943208e0f6a1b1613809becc235e36638900dc",
    "commit_message": "Make Sensitve part of the public API"
},
{
    "commit_hash": "ce4da996058a846ee95e81419a5472a4010a1356",
    "commit_message": "Rewrite query for remote aliases to remove USE clause"
},
{
    "commit_hash": "7997fc1c0ecb454f4ceec0e8b18bd2ef68d05edf",
    "commit_message": "Refactor CustomExtensionUtils create unloaded classes ()\n\n.. in order to avoid issues with the classloader in the tests.\r\n\r\nThere were several hacks with the previous solution, e.g. CustomExtensionFactory had to be package protected, in order to avoid having the service loader discovering it in tests.\r\n\r\nThis solution applies ByteBuddy to create the necessary byte code, making sure that they are not loaded implicitly by the tests."
},
{
    "commit_hash": "64778394739ada8c841c7ca59c334e5a7ac9ed09",
    "commit_message": "RBAC Property Rules: Preparation for supporting inequalities ()\n\nCo-authored-by: Hannes Sandberg <hannes.sandberg@neotechnology.com>"
},
{
    "commit_hash": "c211e65ebd53c2f87905a57b6bc41c30cdbe4f19",
    "commit_message": "Check AccessMode when generating NodeID in constraing error ()\n\nSigned-off-by: Ionut Anghelcovici <148086607+neo-ionut@users.noreply.github.com>\r\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "93719dabe21f10dbfbc7e0c2137d6d3d7e0c710d",
    "commit_message": "Update neo4j-admin strings ()\n\nSigned-off-by: Ionut Anghelcovici <148086607+neo-ionut@users.noreply.github.com>\r\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "47625940426fae60cc5b1e7705eb2f8b971710aa",
    "commit_message": "Adds project to compile semantic analysis for the language support ()\n\nCo-authored-by: Love Leifland <love.leifland@neotechnology.com>"
},
{
    "commit_hash": "ce08bb31c1e44fb4a76e3b6fa8d003df3bca50bb",
    "commit_message": "Fix incorrect logic in ProcedureClassLoaderTest.whenJarIsOnClasspath ()\n\n.. to only expect usage of the ProcedureClassLoader when we have the feature enabled and the JAR does not contain extensions. In all other instances, we expect that the application classloader to take over."
},
{
    "commit_hash": "f89d954b1110cc40eec3f1ea1c4709c167ad4736",
    "commit_message": "Refactor ChangeDataCapture enrichment and streaming of logical keys ()"
},
{
    "commit_hash": "e82ceef9747109a13afc1fa6d04ebc9709446b88",
    "commit_message": "Update descriptions of the procedures ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>\r\nCo-authored-by: Therese Magnusson <therese.magnusson@neotechnology.com>"
},
{
    "commit_hash": "0010e5169fda49f3ddea0a27b1d5fd8ec44db5c2",
    "commit_message": "Remove AcquireLockTimeoutException in favor for LockAcquisitionTimeoutException\n\nIts old and seems to no longer be needed. Keep status and error message for compatibility but this should not really be user facing"
},
{
    "commit_hash": "30864e9eb618eb4ea2776d39a0c52d5f18888bca",
    "commit_message": "Interrupt background index sampling when stopping IndexingService ()"
},
{
    "commit_hash": "91683fc975bfdf01187276b8657f66434c65549f",
    "commit_message": "Slight rework of PropertySelection\n\nDriver was adding a way to exclude keys from a selection.\nThe case is an existing entity that has changed properties in tx-state\nwhere the selection given to the store cursor should exclude the overridden\ntx-state properties, to not read them unnecessarily from store.\n\nBlockPropertyCursor reads the requested properties eagerly.\nThis doesn't work well with DefaultPropertyCursor when the entity\nexists since before but has changed properties in the tx state,\nwhere the changed properties are read from store and then later ignored\nin favor of the tx state. This doesn't happen in record storage since\nproperty values are read more lazily and gets filtered out in\nDefaultPropertyCursor#next. Now that filtering moved to PropertySelection\nso that both approaches can enjoy it."
},
{
    "commit_hash": "40869cb69cb26cc8e4403b05eeab6d325a256ba8",
    "commit_message": "Allow non-admin show commands to be combined in a single query ()\n\nAllow combining `SHOW PROCEDURES`, `SHOW FUNCTIONS`, `SHOW SETTINGS`, `SHOW INDEXES`, `SHOW CONSTRAINTS`, `SHOW TRANSACTIONS` and `TERMINATE TRANSACTIONS` in a single command, following the rules of how `SHOW TRANSACTIONS` and `TERMINATE TRANSACTIONS` already can be combined. This is currently hidden behind feature flag for further experimentation and discussions around command composability in general (without committing to this behaviour to users)."
},
{
    "commit_hash": "777e087e4aaaf69f8fc37cad03458ff6a729fd16",
    "commit_message": "Introduce page cursor option to avoid page content loading ()"
},
{
    "commit_hash": "e253c6ff0402320951eee4c668031977e7467437",
    "commit_message": "Null safe log header access ()"
},
{
    "commit_hash": "7bf5e5ec36f192df2100e762c768618366b953d1",
    "commit_message": "Stop passing AccessMode directly to cursors\n\nCurrently there is a mix of ways on how different cursors gets the\nintended AccessMode to be used for security/access checks: some cursors\nrequires an AccessMode parameter; some of them accepts for some\ninitializations but not for others, e.g. requires for scanBatch() but\nnot for scan() or single(); some read eagerly from Read's\ngetAccessMode(); other read lazily from Read's getAccessMode().\n\nThis also make it more confusing to track what each check is using and\ncould cause mismatches between what one cursor uses and what one of its\ninternal cursor might want to use.\n\nThis patch removes completely the AccessMode passing for/from cursors\nand always relies on the Read's accessMode() method. All the\nimplementations of this method are thread-safe if correctly usage of\ntransactions are happening (for example, properly creating independent\nExecutionContext objects for each worker thread).\n\nBy cleaning-up all the passing of AccessModes from this part of the API,\nit'll also make it more clear that the proper way to make change to\naccess modes it through the KernelTransaction's overrideWith(context)\ncall (which is also exposed by the InternalTransaction interface).\n\nMost of the patch is just removing the arguments from the methods and\ncalls, with some small code reorganization inside cursors to use the\ngetAccessMode()."
},
{
    "commit_hash": "e3ab2a110b53202c4706e27a3d50de5d8ae01e7e",
    "commit_message": "Moving neo4j-random-values to testing directory ()"
},
{
    "commit_hash": "fe021e2e019617d1e322ff09bdd6456de02aa925",
    "commit_message": "Reordering checks in Query Router\n\nThe old stack first checks the access mode and then routability.\r\nThis change does the checks in Query Router in the same order\r\nin order to get the same errors in various situations."
},
{
    "commit_hash": "55bf2116e50c4c40d36042dd984a5928a1a5d004",
    "commit_message": "Multiversion countstore redesigned  ()\n\nThis PR adds makes multiversion countstore use multiversion tree. So, we have multiple levels (two) of multiversion here.\r\nUsing of multiversion tree is required in order to make readers concurrent with writers to produce consistent results when count store compaction is enabled.\r\nWhen using non versioned tree concurrent seekers will observe partial results of delta aggregation, thus producing corrupted counts.\r\nThere are comments on AggregateDeltas and VersionedCountsStore with some details on design and implementation."
},
{
    "commit_hash": "ff43dffd3eea99d22fcf724fce3c6f45f398787a",
    "commit_message": "Query router inserts processed query into a pre-parser and AST cache of the target database.\n\n\r\nQuery router needs to parse query strings into ASTs, so it makes sense to populate the caches\r\nof the target database with the result, so it does not have to be done again.\r\n\r\nThis is not done for System DB queries as it it more trouble than worth. System DB queries might\r\ncontain sensitive information and AST cache is not used for them anyway."
},
{
    "commit_hash": "b9bbef59315ff0bcc3fd3398fd6af3cb82b9f79c",
    "commit_message": "Ability to disable ID generator direct-to-cache for mvcc\n\nMoving IDs directly to cache when freed had problems with ID buffering\nin clustering, which is why it was effectively removed on followers.\nMVCC must use ID buffering all the time, at least as long as ID generator\nimplementation works like it does and because of that direct-to-cache\ndoesn't quite work with MVCC."
},
{
    "commit_hash": "d261c7dc1abf6be85ce27ffb25eb455ba20c7a7d",
    "commit_message": "\"Allocation enabled\" state owned by FreeIdScanner"
},
{
    "commit_hash": "0cada4e4e3e6fbcf5ec60646e6677c0d183e308f",
    "commit_message": "BufferingIdGenerator(s) respect \"allocation enabled\" state too"
},
{
    "commit_hash": "f70c09f7a8586a3578fe8e6664e6458a1aad443d",
    "commit_message": "Correct allocationEnabled in EnterpriseEditionModule"
},
{
    "commit_hash": "90b684e3161c3050d0eaf7c757a6479f8c55a89b",
    "commit_message": "IndexedIdGenerator#clearCache can affect follower/leader mode"
},
{
    "commit_hash": "0b7697d805675806fb1cc50349557d9a1e9d0b8f",
    "commit_message": "BufferingIdGeneratorFactory clears buffered IDs in clearCache\n\nNow when buffered IDs could go directly into the cache, they too\nmust be cleared when clearing the cache. They should've been previously\nas well, but it just happened to work w/o."
},
{
    "commit_hash": "89c7c0e5ddc5a44cdb03f0429b990397a2eb73c4",
    "commit_message": "Specific exception for ID transition failures"
},
{
    "commit_hash": "e737fabcc54a03dbeece8c7571f650c6b2b182a7",
    "commit_message": "Update LoggingIndexedIdGeneratorMonitor"
},
{
    "commit_hash": "0d6f8d5f4a8b68d8a12937acbef8b10a21aa1d78",
    "commit_message": "Freed IDs enter the cache directly w/o marking or scanning\n\nThis heavily reduces I/O, both from avoiding marking and from\navoiding scanning to let those IDs enter the cache.\nSo now the states for an ID becomes:\n\n- used\n- deleted\n- either of:\n  - *cached* (w/o getting marked as either free or reserved)\n  - free (and later reserved as it's found in a scan)\n\nSo an ID in the cache is either marked as deleted+free+reserved,\nor more typically _only_ deleted. IDs that gets e.g. rolled back\nor on cleared cache gets the free mark added and its reserved mark\nremoved to restore the correct state for a scan later."
},
{
    "commit_hash": "9d255569f95e5fa054a7c6e39e5cd262080048b4",
    "commit_message": "ID generator cache supports parallel offer"
},
{
    "commit_hash": "2c12c5ac0ffd7f8e7257c3a158eada927f2fcd6a",
    "commit_message": "Add `block` as a valid value (description only) for admin command arguments and db.format setting ()"
},
{
    "commit_hash": "c09a0e923214189615b28bd31fa23a5a225617b5",
    "commit_message": "Fix high limit format bug: do not infer requiresSecondaryUnit from secondaryUnitId in setSecondaryUnitIdOnLoad\n\n(requiresSecondaryUnit = false && secondaryUnitId != NO_ID) is a valid\nstate that arises when a double record shrinks to be a single record,\nbut when reading from the transaction log, requiresSecondaryUnit\nwas previously overridden to true if secondaryUnitId != NO_ID.\n\nThis meant that on recovery / tx replication in clusters,\nshrunk records would be written to the store either\n\n* in fixed reference format but with a header claiming two records are used, or\n* in non-fixed reference format with a secondaryUnitId that may now have been reused by another record."
},
{
    "commit_hash": "4c9e28f31ac3c9dc7dd94ce043d7c6b3b431241c",
    "commit_message": "Better coverage of secondary unit state in command serialization tests\n\nAlso asserts on all the fields of all records. This exposed some issues\nthat were also fixed, e.g. writing relationship group records with type -1\nwould read it back as the max type - fixed by converting <max> to -1 if unused.\n\nTesting around \"created\" field and all secondaryUnit fields is tricky\nbecause of how the command serialization doesn't actually store all fields,\nrather lets deserialization infer some of them. So therefor commands,\nand their records within them cannot be 100% randomized, rather some aspects\nof them carefully crafted to mimic real-world usage so that a round trip\nthrough the tx log can still produce the same result as was expected."
},
{
    "commit_hash": "ac09a0c317ab6405e6e5c832f6b549d9cf2f8f2f",
    "commit_message": "Adds --to-file option to DumpIdGenerator tool"
},
{
    "commit_hash": "058e748142ea653305e38a7315c441cd199d06ab",
    "commit_message": "Fix bug in query obfuscation of sensitive parameters in query router"
},
{
    "commit_hash": "ae01fcc16df3f054d114036c2561408b07be34d0",
    "commit_message": "route composite queries through new stack"
},
{
    "commit_hash": "d33a78be12a5258fca88f7fda245d7ca4794e4f9",
    "commit_message": "Make the upgrade tx a regular transaction\n\nNOTE! this changes the version of the upgrade transaction. It will now be written\r\nin the 'from' version instead of the 'to' version.\r\n\r\nCo-authored-by: Valdemar Roxling <valdemar.roxling@neo4j.com>"
},
{
    "commit_hash": "75cc38a12b3ee49d3798e8a7f374a1a1c2bbe840",
    "commit_message": "Code review: expose error instead"
},
{
    "commit_hash": "00e1363c4e085fea38a0a4dced70b2fef0a2b9de",
    "commit_message": "Don't call monitor.endSuccess after failure"
},
{
    "commit_hash": "176ea6e75367cb9869439c30a7467cc2032c1daf",
    "commit_message": "BlockConsistencyChecker now catches and reports all unexpected exceptions instead of crashing ()"
},
{
    "commit_hash": "cb9f66291d72a70320951267b996fd7f6da29778",
    "commit_message": "Allow recovery to survive last log entry corruption ()"
},
{
    "commit_hash": "8f2f0df01daec0c8a7eb4e5c8e4289d65656ae02",
    "commit_message": "Escape toString for constrains  ()\n\nSigned-off-by: Ionut Anghelcovici <148086607+neo-ionut@users.noreply.github.com>\r\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "2c5aad0d6a4f08b431c80bb8ec8c26905d6df249",
    "commit_message": "Add IS to SET and REMOVE"
},
{
    "commit_hash": "42798b895c0bc99a89151097aa78b1a0ade8548b",
    "commit_message": "Change minimal transaction rotation size to two segments\n\nDoesn't touch the actual setting as that would be breaking, instead just 'rounds up a little bit more' in the unlikely case that someone set a really low threshold.\r\nNeeded for when the switch to envelope channels happen. One segment is reserved for the log header, which means we\r\nneed at least one more to actually have space for data"
},
{
    "commit_hash": "6fcaed4201d3f7009bd5179c47e0f3afbaa7cb77",
    "commit_message": "Support SHORTEST in the interpreted/slotted runtime ()\n\nCo-authored-by: erik-leffler <erik.leffler@neotechnology.com>"
},
{
    "commit_hash": "90a71e673144a790fe5b022141b16aaa1f7ca1ed",
    "commit_message": "More correct max label id\n\nChange max number of label id bits to 31 since that is the maximum the\nints used for the ids can handle. This gives the nicer\nTokenCapacityExceededKernelException instead of an ArithmeticException: integer overflow\n\nTechnically it should actually be limited to Integer.MAX_VALUE - 8 since that is what\nis guaranteed to fit in the TokenRegistry's hashmap on all VMs, but that could be\na problem for anyone that managed to create such a token already."
},
{
    "commit_hash": "2752c3b17e1b52e009b3675e2e0c2bad13034bce",
    "commit_message": "Adding support for external references to Query Router - Part 1 \n\nQuery router needs to rewrite queries using external database references.\r\nThe reason is that external references don't exist on the external reference\r\ntarget DBMS, so they need to be replaced in a query with the target database\r\nname, before the query is sent to the target DBMS.\r\n\r\nThis work just refactors Query Router services to make such rewriting possible.\r\n\r\nCo-authored-by: LinneaAndersson <linneamaria10@gmail.com>"
},
{
    "commit_hash": "71ea32dc8c1534b5c3a5c8ddf1d48a33e8793c1e",
    "commit_message": "Switch ExecutingQuery transactionBindings to be CHM instead of queue ()"
},
{
    "commit_hash": "3d933dddbfc8058bf6ec0883091673407c890142",
    "commit_message": "Improved the way we set heap and pagecache for macro benchmarks ()\n\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>\r\nCo-authored-by: Michal Jonko <michal.jonko@neotechnology.com>"
},
{
    "commit_hash": "e1ea38362df0bed332422a9a4de3bd975718c172",
    "commit_message": "Improved the way we set heap and pagecache for macro benchmarks ()\n\nCo-authored-by: Michal Jonko <michal.jonko@neotechnology.com>"
},
{
    "commit_hash": "e37b55cf113489a4c8b585d254fd3204966fbbc7",
    "commit_message": "Metrics changes for Block format ()\n\nSigned-off-by: Ionut Anghelcovici <148086607+neo-ionut@users.noreply.github.com>\r\nCo-authored-by: Agent Smith <103414894+neo4jagentsmith[bot]@users.noreply.github.com>"
},
{
    "commit_hash": "f39a32a9924b58f6cf958f884d30a077eebb7250",
    "commit_message": "Make TransactionInfo mutable and update txMetadata when setting metadata"
},
{
    "commit_hash": "7eb52a4f91e3a613b82e7892fa9ab74ea8463ed4",
    "commit_message": "remove json data from verbose output ()"
},
{
    "commit_hash": "a4ec42ccc748380144985fa178c369991a7aeead",
    "commit_message": "remove json data from debugging ()"
},
{
    "commit_hash": "89fe2fffbcc677fa1c02de22dae8140af36b0610",
    "commit_message": "Adding authorization to Query Router transaction start\n\nCo-authored-by: htsedebenham <31847376+htsedebenham@users.noreply.github.com>"
},
{
    "commit_hash": "7d1ce91928f69fa71924ba8291d79798d4f1cc08",
    "commit_message": "Fixes import command \"auto header skipping\" when using multiline fields"
},
{
    "commit_hash": "c667d441e817148b01f56de3ff79322166bdd9a0",
    "commit_message": "Rework ArrayExtractor to avoid array copy and boxing\n\nBy changing ArrayExtractor to use a parse and store pattern, we can\navoid unnecessary boxing/unboxing and array copying for primitive types\nwhile still keeping the type system happy and generic enough to support\nneo4j Values."
},
{
    "commit_hash": "6b362f9b550972b0826bd80b993dd1f1f4cb52df",
    "commit_message": "Use name() instead of producesType() to check Extractor type\n\nAlso clean-up the naming of the extractors and add tests to cover the\ngenerated names."
},
{
    "commit_hash": "0bcb43bc62bea282b317aec46497b5d69a4b3bb5",
    "commit_message": "Tidy-up extractors implementation visibility\n\nSince Extractor<T> is the public interface for extractors, we should\nclose the visibility of all particular implementations so there is no\ndirect dependency on them.\n\nIn order to achieve this we change the methods on Extractors to return\nExtractor<Type> instead of the TypeExtractor and provide a new method in\nthe Extractor interface to allow users of that interface to do dynamic\ntype checking during runtime, even when we're mostly passing\nExtractor<?> around."
},
{
    "commit_hash": "0d56a57cadc85c6f69cbaa4a7942e50a8fe5988a",
    "commit_message": "Refactor ArrayExtractors to remove code duplication\n\nEvery implementation of ArrayExtractor contained a duplicate\nimplementation of the logic for traversing the input string and\nextracting each element. This logic is not very trivial nor easy to\nfollow, so might be very easy to get something wrong or to fix a bug in\none and not in others.\n\nThis refactor is based on the observation that all array extractors\nactually use an intermediary element type, before converting to the\nfinal output type of the extractor. Based on this, we can use that\nintermediary type as a generic argument, provide new abstract methods so\neach type implement only its own convertion and extract the duplicated\ncode and bring it up to the base ArrayExtractor class."
},
{
    "commit_hash": "7f285301e409d3612d994b6b8680ab4c60e43474",
    "commit_message": "Drop unused extractor discovery logic\n\nExtractors() constructor used to fill the extractors instances map based\non the static fields of the Extractors class. This is currently unused\nand we set-up all the instances through the add() pattern, which is more\nclear anyway."
},
{
    "commit_hash": "6e58ebff1a8ce7af7212e1182e9e661c71e76ef7",
    "commit_message": "Making tx.setMetaData function work with the new Query Router stack"
},
{
    "commit_hash": "5ea9c3d4f493f769444a53dabc56b713cf2e48f1",
    "commit_message": "query router should validate statement type after access mode"
},
{
    "commit_hash": "832a67067fe84f29263afd625ae636398ea061b8",
    "commit_message": "Use common conditions from utility class instead of creating test local equivalents ()"
},
{
    "commit_hash": "145712aa59b00737596d91c76deaf6a2c43d4f8a",
    "commit_message": "Block IdGeneratorConsistencyChecker now works correctly after uncommitted txs ()\n\nBy only hashing and checking up to highest written for all id checks\r\ninstead of high ID (which does not represent the committed state).\r\n\r\nCo-authored-by: Mattias Finn\u00e9 <mattias@neotechnology.com>"
},
{
    "commit_hash": "5d73902abdc045d86c35f6d5bcf8083afc3c42cd",
    "commit_message": "Expose metric for the number of MVCC transaction validation failures ()"
},
{
    "commit_hash": "40e3cf72d6f8fed7e450a7baea791c86425341e0",
    "commit_message": "Explicitly use Query Router old stack for tests testing it"
},
{
    "commit_hash": "84a705a734368b80ccc65f143468a28210d319e8",
    "commit_message": "db.clearQueryCaches now clears Query Router cache, too \n\nCurrently, we just clear everything from the Query Router cache, regardless of the target database. The reason is that this cache contains entries for databases that might not exist locally, so there is no way how to target such entries with the current API.\r\n\r\nThis should not be a big issue as this cache saves us only query parsing.\r\n\r\nWe might do something more fine-grained later as a performance optimisation."
},
{
    "commit_hash": "3ad7b97d32dadf417f176f159f6e25fd5f6a3fcb",
    "commit_message": "Rework file size and log position assertions\n\nWe check log positions API to make sure we wrote to the file as\nexpected, even if the writting is not visible in the file size because\nit was pre-allocated.\n\nWe then check the size after truncate based on some expected size we\nhave before hand."
},
{
    "commit_hash": "d3e22270d8bf7d8c4ed1e755b2aeb0f3310b41b0",
    "commit_message": "Use RandomExtension instead of ThreadLocalRandom\n\nUsing RandomExtension for generating random pads of zeroes, allow us to\nregister the random seed that was used in the test in order to debug\nfailures in the future.\n\nThis also unifies the logic on those two tests, so both are writting a\nrandom quantity of bytes instead one using bytes and other longs (which\nproduces 4x more bytes written)."
},
{
    "commit_hash": "b5309248c8aebe5bf7a16718ade9f985ee204f2f",
    "commit_message": "Use LogPosition from channel instead of Files.size()\n\nFiles.size() will return the size on disk, which includes the\npre-allocated bytes. When we build a LogPosition manually with it to\npass to .truncate() method, it might deviate from how the code is used\nin production.\n\nOne of this case is for example the\ntruncateLogWithCorruptionThatLooksLikePreAllocatedZeros() test, where we\nwrite a random number of zeroes plus a \"corrupt byte\" to the file, but if\nthe total number of bytes written is less than what was pre-allocated, than\nthe test will fail because truncate() will only scan for corruption from\nthe position we pass to it.\n\nThis change should deflake\ntruncateLogWithCorruptionThatLooksLikePreAllocatedZeros()."
},
{
    "commit_hash": "3ff6a1c7836e50e803e158d5889f54fa907b33a7",
    "commit_message": "Adding visible field to auth config discovery response ()"
},
{
    "commit_hash": "a1b1b09831b91ffd164b2514e3f776a3619fe9ff",
    "commit_message": "Refactor Segment subclasses into records ()"
},
{
    "commit_hash": "1488a935f85130afb8b455b8c88bd421feeb4836",
    "commit_message": "Improve memory calculation for Record consistency checker ()\n\nCo-authored-by: Mattias Finn\u00e9 <mattias@neotechnology.com>"
},
{
    "commit_hash": "d7c5bc4ab81fd9a838e3df287d7b32b22f6b9ef0",
    "commit_message": "Periodic errorprone cleanup ()"
},
{
    "commit_hash": "45e7eca3f6f4e2b6cfc25222fa22f3f3936a0609",
    "commit_message": "Connectivity listener problems"
},
{
    "commit_hash": "ae44afbd1de08d4235b57c0fb15eb8398a7e2e06",
    "commit_message": "Teach QuickImport to generate all primitives + arrays\n\nThis adds support into DataGeneratorInput to generate all the primitive\ntypes plus all their respective array versions.\n\nThe size of the arrays are controlled using two new command line flags,\n--min-array-size and --max-array-size, and are piped through the code\nvia the RandomValues.Configuration to be passed to the RandomValues\ngenerator.\n\nThe previous \"int\" and \"long\" were changed to produce a int and long\nrespectively and not be arbitrarily bounded anymore."
},
{
    "commit_hash": "749688ed9c6bd6fa1fc899bc8ed8bf37d4ef8dd6",
    "commit_message": "Add total number of indexes and elapsed time when logging index state"
},
{
    "commit_hash": "b69d767150bf6da084fed756e54c12e9c7d7d625",
    "commit_message": "Add ParallelQueryStateStressTest"
},
{
    "commit_hash": "afd22ac834415cfd25d101057debd5394476e738",
    "commit_message": "Unify token ids to always be int ()"
},
{
    "commit_hash": "8f48ae1b081640cbdac78e9629529569dc4700ba",
    "commit_message": "Fixing an ssue when queries from Query Router would bypass snapshot execution engine logic"
},
{
    "commit_hash": "2eec3cfe6f3eb9d8a26f5ea080e0e1a385065f02",
    "commit_message": "Adding a check for an illegal combination of an ambient and explit graph\n\n\r\n+ Creating a Query Router version of FabricGraphSelectionIT"
},
{
    "commit_hash": "f9bfe88af12035399f8a50d6984547859ec9fa91",
    "commit_message": "Refactoring of Query Router transaction \n\nThis change splits Query Router transaction from Fabric Transaction.\r\nThe main benefit is that Query Router transaction can do with much\r\nsimpler concurrency model which greatly simplifies the code and makes\r\nthe reasoning about the correctness much simpler.\r\nAnother benefit is that Query Router does not throw Fabric Exceptions anymore.\r\n\r\nConcurrency model of Query Router transaction:\r\n\r\nTransaction termination is the only modification operation that can be invoked\r\nconcurrently with other operations. It iterates through all child transactions\r\nand marks them as terminated, which is a thread-safe operation for both kernel\r\nand driver transactions.\r\nTransactions being non-thread-safe apart from termination has been a feature of Kernel\r\ntransactions since the dawn of time and all clients of the API are build to respect it.\r\nTherefore since a Query Router transaction is just a thin layer between those clients\r\nand kernel transactions (and driver transactions), it can rely on this concurrency model."
},
{
    "commit_hash": "6eaf6d5c7ce6098a773d8fc5ac782a79b2375c6a",
    "commit_message": "Remove redundant stream transformation in DefaultNodeValueIndexCursor::canAccessAllDescribedEntities ()"
},
{
    "commit_hash": "f7abb6be9fb5c4335dcd639b3c67dd7f5091380f",
    "commit_message": "Update the description of `server.cypher.parallel.worker_limit` ()\n\nCo-authored-by: Reneta Popova <reneta.popova@neo4j.com>"
},
{
    "commit_hash": "d6b31d84e9089500b1e4c9741c73308f0d38afae",
    "commit_message": "Code review: Remove unnecessary abstraction"
},
{
    "commit_hash": "4edd874c4b902e979ca7c69a1291633c598aa5c9",
    "commit_message": "Skip usage check when trying to evict page on swapper close ()"
},
{
    "commit_hash": "320e79a239b734a76e12f6fedbe7a83b3dca7872",
    "commit_message": "Block format migration from legacy huge node store to unified"
},
