{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Consider the set of GitHub repositories in the provided link: http://aserg-ufmg.github.io/why-we-refactor/#/projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 a: Web scrapping to get 124 github links\n",
    "Instruction to use selenium: https://towardsdatascience.com/how-to-use-selenium-to-web-scrape-with-example-80f9b23a843a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12700\\1210067507.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Download and Access WebDriver\n",
    "# In my case, I download at the link: https://googlechromelabs.github.io/chrome-for-testing/#stable\n",
    "\n",
    "# Specify the path to the downloaded ChromeDriver\n",
    "driver_path = os.path.join(os.getcwd(), \"chromedriver-win64\", \"chromedriver.exe\")\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "cService = webdriver.ChromeService(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=cService)\n",
    "\n",
    "# Access Website Via Python\n",
    "driver.get(\"http://aserg-ufmg.github.io/why-we-refactor/#/projects\")\n",
    "\n",
    "driver.set_page_load_timeout(50)  # wait for the page to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621\n",
      "                                               Project Creation Date Commits  \\\n",
      "0    https://github.com/JetBrains/intellij-communit...       9/30/11  162625   \n",
      "1                 https://github.com/JetBrains/MPS.git       8/15/11   66445   \n",
      "2    https://github.com/CyanogenMod/android_framewo...       5/13/13   62208   \n",
      "3       https://github.com/liferay/liferay-plugins.git       9/25/09   33929   \n",
      "4                   https://github.com/neo4j/neo4j.git      11/12/12   29187   \n",
      "..                                                 ...           ...     ...   \n",
      "119               https://github.com/zeromq/jeromq.git        8/1/12     316   \n",
      "120          https://github.com/bitfireAT/davdroid.git       8/25/13     291   \n",
      "121           https://github.com/bennidi/mbassador.git      10/23/12     236   \n",
      "122        https://github.com/novoda/android-demos.git       7/26/09     142   \n",
      "123               https://github.com/jfinal/jfinal.git       4/25/12     102   \n",
      "\n",
      "    Java Files Contributors  \n",
      "0        47552          306  \n",
      "1        19226           60  \n",
      "2         5482         2568  \n",
      "3         2024          344  \n",
      "4         4607          150  \n",
      "..         ...          ...  \n",
      "119        215           39  \n",
      "120        438           19  \n",
      "121        115           15  \n",
      "122         69           11  \n",
      "123        175            6  \n",
      "\n",
      "[124 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"Project\", \"Creation Date\", \"Commits\", \"Java Files\", \"Contributors\"])  # creates master dataframe\n",
    "\n",
    "# Extract project rows from the table\n",
    "data = driver.find_elements(By.CLASS_NAME, \"ng-binding\")\n",
    "list_data = enumerate(data)\n",
    "\n",
    "print(len(list(list_data)))\n",
    "\n",
    "# Group data into chunks of 5 for each project\n",
    "rows = []\n",
    "for i in range(1, len(data), 5):\n",
    "    project_link = data[i].text\n",
    "    creation_date = data[i + 1].text\n",
    "    commits = data[i + 2].text\n",
    "    java_files = data[i + 3].text\n",
    "    contributors = data[i + 4].text\n",
    "\n",
    "    # Create a dictionary for each project\n",
    "    row = {\n",
    "        \"Project\": project_link,\n",
    "        \"Creation Date\": creation_date,\n",
    "        \"Commits\": commits,\n",
    "        \"Java Files\": java_files,\n",
    "        \"Contributors\": contributors,\n",
    "    }\n",
    "\n",
    "    # Append each row to the list\n",
    "    rows.append(row)\n",
    "\n",
    "# Use pd.concat to combine all rows into the DataFrame\n",
    "df = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser once the data is collected\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 b: Divide 124 projects into 5 groups (Keep the last group with 24 projects), randomly select 2 projects from each group. Because we aim for the bonus point, we will do mining for all projects. \n",
    "- Hung: 1-25\n",
    "- Mazen: 26-50\n",
    "- Juuso: 51-75\n",
    "- Nicolas: 76-100\n",
    "- Bharu: 101-124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't run this function, as it's used for randomly select 10 projects from 5 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   9  28  48  61  69  89  91 109 117]\n",
      "Selected projects to work with:\n",
      "Project_1\n",
      "Project_9\n",
      "Project_28\n",
      "Project_48\n",
      "Project_61\n",
      "Project_69\n",
      "Project_89\n",
      "Project_91\n",
      "Project_109\n",
      "Project_117\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from numpy import sort\n",
    "# List of 124 project names (example list, you can replace it with actual project names)\n",
    "projects = [f\"{i+1}\" for i in range(124)]\n",
    "\n",
    "# Divide projects into 5 groups, with first 4 groups contain 25 projects each\n",
    "group_size = 25\n",
    "groups = [projects[i : i + group_size] for i in range(0, 100, group_size)]\n",
    "# Add the remaining 24 projects as the last group\n",
    "groups.append(projects[100:])\n",
    "\n",
    "# Randomly select 2 projects from each group\n",
    "selected_projects = []\n",
    "for group in groups:\n",
    "    selected_projects.extend(random.sample(group, 2))\n",
    "\n",
    "selected_projects = [int(project) for project in selected_projects] # Convert to int for sorting\n",
    "selected_projects = sort(selected_projects)\n",
    "\n",
    "print(selected_projects)\n",
    "# Output the selected 10 projects\n",
    "print(\"Selected projects to work with:\")\n",
    "for project in selected_projects:\n",
    "    print(\"Project_\"+str(project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of project that you are assigned to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "Start = 1\n",
    "End = 25\n",
    "selected_projects = [i for i in range(Start, End+1)]\n",
    "\n",
    "print(selected_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get github links that we generate above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/JetBrains/intellij-community.git\n",
      "https://github.com/JetBrains/MPS.git\n",
      "https://github.com/CyanogenMod/android_frameworks_base.git\n",
      "https://github.com/liferay/liferay-plugins.git\n",
      "https://github.com/neo4j/neo4j.git\n",
      "https://github.com/apache/camel.git\n",
      "https://github.com/gradle/gradle.git\n",
      "https://github.com/processing/processing.git\n",
      "https://github.com/elastic/elasticsearch.git\n",
      "https://github.com/belaban/JGroups.git\n",
      "https://github.com/osmandapp/Osmand.git\n",
      "https://github.com/wildfly/wildfly.git\n",
      "https://github.com/SonarSource/sonarqube.git\n",
      "https://github.com/VoltDB/voltdb.git\n",
      "https://github.com/languagetool-org/languagetool.git\n",
      "https://github.com/grails/grails-core.git\n",
      "https://github.com/apache/hive.git\n",
      "https://github.com/fabric8io/fabric8.git\n",
      "https://github.com/hazelcast/hazelcast.git\n",
      "https://github.com/apache/cassandra.git\n",
      "https://github.com/rstudio/rstudio.git\n",
      "https://github.com/spring-projects/spring-framework.git\n",
      "https://github.com/droolsjbpm/drools.git\n",
      "https://github.com/BroadleafCommerce/BroadleafCommerce.git\n",
      "https://github.com/eclipse/jetty.project.git\n"
     ]
    }
   ],
   "source": [
    "project_links = []\n",
    "\n",
    "for project in selected_projects:\n",
    "    project_link = df.loc[project - 1, \"Project\"]\n",
    "    project_links.append(project_link)\n",
    "\n",
    "# Print the project links\n",
    "for link in project_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_links = [\n",
    "    \"https://github.com/JetBrains/intellij-community.git\",\n",
    "    \"https://github.com/JetBrains/MPS.git\",\n",
    "    \"https://github.com/CyanogenMod/android_frameworks_base.git\",\n",
    "    \"https://github.com/liferay/liferay-plugins.git\",\n",
    "    \"https://github.com/neo4j/neo4j.git\",\n",
    "    \"https://github.com/apache/camel.git\",\n",
    "    \"https://github.com/gradle/gradle.git\",\n",
    "    \"https://github.com/processing/processing.git\",\n",
    "    \"https://github.com/elastic/elasticsearch.git\",\n",
    "    \"https://github.com/belaban/JGroups.git\",\n",
    "    \"https://github.com/osmandapp/Osmand.git\",\n",
    "    \"https://github.com/wildfly/wildfly.git\",\n",
    "    \"https://github.com/SonarSource/sonarqube.git\",\n",
    "    \"https://github.com/VoltDB/voltdb.git\",\n",
    "    \"https://github.com/languagetool-org/languagetool.git\",\n",
    "    \"https://github.com/grails/grails-core.git\",\n",
    "    \"https://github.com/apache/hive.git\",\n",
    "    \"https://github.com/fabric8io/fabric8.git\",\n",
    "    \"https://github.com/hazelcast/hazelcast.git\",\n",
    "    \"https://github.com/apache/cassandra.git\",\n",
    "    \"https://github.com/rstudio/rstudio.git\",\n",
    "    \"https://github.com/spring-projects/spring-framework.git\",\n",
    "    \"https://github.com/droolsjbpm/drools.git\",\n",
    "    \"https://github.com/BroadleafCommerce/BroadleafCommerce.git\",\n",
    "    \"https://github.com/eclipse/jetty.project.git\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get repo name for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intellij-community\n",
      "MPS\n",
      "android_frameworks_base\n",
      "liferay-plugins\n",
      "neo4j\n",
      "camel\n",
      "gradle\n",
      "processing\n",
      "elasticsearch\n",
      "JGroups\n",
      "Osmand\n",
      "wildfly\n",
      "sonarqube\n",
      "voltdb\n",
      "languagetool\n",
      "grails-core\n",
      "hive\n",
      "fabric8\n",
      "hazelcast\n",
      "cassandra\n",
      "rstudio\n",
      "spring-framework\n",
      "drools\n",
      "BroadleafCommerce\n",
      "jetty.project\n",
      "['intellij-community', 'MPS', 'android_frameworks_base', 'liferay-plugins', 'neo4j', 'camel', 'gradle', 'processing', 'elasticsearch', 'JGroups', 'Osmand', 'wildfly', 'sonarqube', 'voltdb', 'languagetool', 'grails-core', 'hive', 'fabric8', 'hazelcast', 'cassandra', 'rstudio', 'spring-framework', 'drools', 'BroadleafCommerce', 'jetty.project']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pwd = os.getcwd()\n",
    "\n",
    "repo_names = []\n",
    "\n",
    "for link in project_links:\n",
    "    # Get the clean repo name by splitting the URL and removing \".git\"\n",
    "    repo_suffix = link.split(\"/\")[-1]\n",
    "    if repo_suffix.endswith(\".git\"):\n",
    "        repo_name = repo_suffix[:-4]  # Remove the last 4 characters ('.git')\n",
    "    else:\n",
    "        repo_name = repo_suffix\n",
    "    print(repo_name)\n",
    "    repo_names.append(repo_name)\n",
    "\n",
    "print(repo_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone 25 github repos to local machine.\n",
    "\n",
    "Note that you might face error when cloning that returned non-zero exit status 128. If so, fix it by run cmd as admin, run \"git config --system core.longpaths true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning project intellij-community\n",
      "Error cloning intellij-community: Command '['git', 'clone', 'https://github.com/JetBrains/intellij-community.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\intellij-community']' returned non-zero exit status 128.\n",
      "Cloning project MPS\n",
      "Error cloning MPS: Command '['git', 'clone', 'https://github.com/JetBrains/MPS.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\MPS']' returned non-zero exit status 128.\n",
      "Cloning project android_frameworks_base\n",
      "Error cloning android_frameworks_base: Command '['git', 'clone', 'https://github.com/CyanogenMod/android_frameworks_base.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\android_frameworks_base']' returned non-zero exit status 128.\n",
      "Cloning project liferay-plugins\n",
      "Error cloning liferay-plugins: Command '['git', 'clone', 'https://github.com/liferay/liferay-plugins.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\liferay-plugins']' returned non-zero exit status 128.\n",
      "Cloning project neo4j\n",
      "Error cloning neo4j: Command '['git', 'clone', 'https://github.com/neo4j/neo4j.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\neo4j']' returned non-zero exit status 128.\n",
      "Cloning project camel\n",
      "Error cloning camel: Command '['git', 'clone', 'https://github.com/apache/camel.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\camel']' returned non-zero exit status 128.\n",
      "Cloning project gradle\n",
      "Error cloning gradle: Command '['git', 'clone', 'https://github.com/gradle/gradle.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\gradle']' returned non-zero exit status 128.\n",
      "Cloning project processing\n",
      "Error cloning processing: Command '['git', 'clone', 'https://github.com/processing/processing.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\processing']' returned non-zero exit status 128.\n",
      "Cloning project elasticsearch\n",
      "Error cloning elasticsearch: Command '['git', 'clone', 'https://github.com/elastic/elasticsearch.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\elasticsearch']' returned non-zero exit status 128.\n",
      "Cloning project JGroups\n",
      "Error cloning JGroups: Command '['git', 'clone', 'https://github.com/belaban/JGroups.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\JGroups']' returned non-zero exit status 128.\n",
      "Cloning project Osmand\n",
      "Error cloning Osmand: Command '['git', 'clone', 'https://github.com/osmandapp/Osmand.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\Osmand']' returned non-zero exit status 128.\n",
      "Cloning project wildfly\n",
      "Error cloning wildfly: Command '['git', 'clone', 'https://github.com/wildfly/wildfly.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\wildfly']' returned non-zero exit status 128.\n",
      "Cloning project sonarqube\n",
      "Project sonarqube cloned\n",
      "\n",
      "Cloning project voltdb\n",
      "Project voltdb cloned\n",
      "\n",
      "Cloning project languagetool\n",
      "Project languagetool cloned\n",
      "\n",
      "Cloning project grails-core\n",
      "Project grails-core cloned\n",
      "\n",
      "Cloning project hive\n",
      "Project hive cloned\n",
      "\n",
      "Cloning project fabric8\n",
      "Project fabric8 cloned\n",
      "\n",
      "Cloning project hazelcast\n",
      "Project hazelcast cloned\n",
      "\n",
      "Cloning project cassandra\n",
      "Project cassandra cloned\n",
      "\n",
      "Cloning project rstudio\n",
      "Project rstudio cloned\n",
      "\n",
      "Cloning project spring-framework\n",
      "Project spring-framework cloned\n",
      "\n",
      "Cloning project drools\n",
      "Project drools cloned\n",
      "\n",
      "Cloning project BroadleafCommerce\n",
      "Project BroadleafCommerce cloned\n",
      "\n",
      "Cloning project jetty.project\n",
      "Error cloning jetty.project: Command '['git', 'clone', 'https://github.com/eclipse/jetty.project.git', 'd:\\\\Oulun\\\\Period 5\\\\Software development, Maintenance and Operation\\\\Projects\\\\RepoFolder\\\\jetty.project']' returned non-zero exit status 128.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "pwd = os.getcwd()\n",
    "\n",
    "for link in project_links:\n",
    "    # Get the clean repo name by splitting the URL and removing \".git\"\n",
    "    repo_suffix = link.split(\"/\")[-1]\n",
    "    if repo_suffix.endswith(\".git\"):\n",
    "        repo_name = repo_suffix[:-4]  # Remove the last 4 characters ('.git')\n",
    "    else:\n",
    "        repo_name = repo_suffix\n",
    "    # Path to the target directory where the repository will be cloned\n",
    "    repo_path = os.path.join(pwd, \"RepoFolder\", repo_name)\n",
    "\n",
    "    # Cloning the project into the current working directory\n",
    "    # Check if the directory already exists\n",
    "\n",
    "    print(f\"Cloning project {repo_name}\")\n",
    "    # Cloning the project with error handling\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", link, repo_path], check=True)\n",
    "        print(f\"Project {repo_name} cloned\\n\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cloning {repo_name}: {e}\")\n",
    "        continue  # Skip to the next repository if cloning fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run it for now, keep it last \n",
    "\n",
    "Delete projects to free memory (you can do it manually at last :D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo path:  d:\\Oulun\\Period 5\\Software development, Maintenance and Operation\\Projects\\RepoFolder\n",
      "Projects erased\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stat\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def remove_readonly(func, path, _):\n",
    "    \"Clear the readonly bit and reattempt the removal\"\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    func(path)\n",
    "\n",
    "\n",
    "repo_path = os.path.join(pwd, \"RepoFolder\")\n",
    "print(\"Repo path: \", repo_path)\n",
    "\n",
    "# Clean up: Remove the cloned repository\n",
    "# if os.path.exists(repo_path):\n",
    "#     shutil.rmtree(repo_path, onerror=remove_readonly)\n",
    "    \n",
    "print(f\"Projects erased\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 c: Mine the refactoring activity applied in the history of the cloned projects using RefactoringMiner library\n",
    "You can read more here: https://github.com/tsantalis/RefactoringMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: First thing first, download RefactoringMiner library\n",
    "- Link and instruction to download: https://github.com/tsantalis/RefactoringMiner/releases\n",
    "- I use version 3.0.8: https://github.com/tsantalis/RefactoringMiner/releases/download/3.0.8/RefactoringMiner-3.0.8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Add path of bin to system environment variables\n",
    "Search google if you don't know how to add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Make sure it run normally\n",
    "Make sure you have Java installed on your computer. If you don't, download it here: https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html\n",
    "\n",
    "Then try to run this command line:\n",
    "> git clone https://github.com/danilofes/refactoring-toy-example.git refactoring-toy-example\n",
    "\n",
    "> RefactoringMiner -c refactoring-toy-example 36287f7c3b09eff78395267a3ac0d7da067863fd -json result.json\n",
    "\n",
    "Since release 3.0.0, RefactoringMiner requires Java 17 or newer and Gradle 7.4 or newer\n",
    "\n",
    "> RefactoringMiner diff --url https://github.com/JabRef/jabref/pull/11180\n",
    "\n",
    "To run the command above, you must provide a valid OAuth token in the github-oauth.properties file stored in the bin folder. You can generate an OAuth token in GitHub Settings -> Developer settings -> Personal access tokens. Together with creating a file named .github in your home directory (C:\\Users\\ADMIN\\.github for Windows).\n",
    "\n",
    "The file should contain your GitHub credentials (personal access token or username/password). The format should look like: \n",
    "\n",
    "> login=your_github_username\n",
    "\n",
    "> oauth=your_github_personal_access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Run RefactoringMiner on the cloned repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RepoFolder\\\\intellij-community', 'RepoFolder\\\\MPS', 'RepoFolder\\\\android_frameworks_base', 'RepoFolder\\\\liferay-plugins', 'RepoFolder\\\\neo4j', 'RepoFolder\\\\camel', 'RepoFolder\\\\gradle', 'RepoFolder\\\\processing', 'RepoFolder\\\\elasticsearch', 'RepoFolder\\\\JGroups', 'RepoFolder\\\\Osmand', 'RepoFolder\\\\wildfly', 'RepoFolder\\\\sonarqube', 'RepoFolder\\\\voltdb', 'RepoFolder\\\\languagetool', 'RepoFolder\\\\grails-core', 'RepoFolder\\\\hive', 'RepoFolder\\\\fabric8', 'RepoFolder\\\\hazelcast', 'RepoFolder\\\\cassandra', 'RepoFolder\\\\rstudio', 'RepoFolder\\\\spring-framework', 'RepoFolder\\\\drools', 'RepoFolder\\\\BroadleafCommerce', 'RepoFolder\\\\jetty.project']\n"
     ]
    }
   ],
   "source": [
    "# Now we are ready to look at data dimension to be mined\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# repo_paths = [os.path.join(os.getcwd(), \"RepoFolder\", i) for i in repo_names]\n",
    "repo_paths = [os.path.join(\"RepoFolder\", i) for i in repo_names]\n",
    "\n",
    "print(repo_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_paths = [\n",
    "    \"RepoFolder\\\\intellij-community\",\n",
    "    \"RepoFolder\\\\MPS\",\n",
    "    \"RepoFolder\\\\android_frameworks_base\",\n",
    "    \"RepoFolder\\\\liferay-plugins\",\n",
    "    \"RepoFolder\\\\neo4j\",\n",
    "    \"RepoFolder\\\\camel\",\n",
    "    \"RepoFolder\\\\gradle\",\n",
    "    \"RepoFolder\\\\processing\",\n",
    "    \"RepoFolder\\\\elasticsearch\",\n",
    "    \"RepoFolder\\\\JGroups\",\n",
    "    \"RepoFolder\\\\Osmand\",\n",
    "    \"RepoFolder\\\\wildfly\",\n",
    "    \"RepoFolder\\\\sonarqube\",\n",
    "    \"RepoFolder\\\\voltdb\",\n",
    "    \"RepoFolder\\\\languagetool\",\n",
    "    \"RepoFolder\\\\grails-core\",\n",
    "    \"RepoFolder\\\\hive\",\n",
    "    \"RepoFolder\\\\fabric8\",\n",
    "    \"RepoFolder\\\\hazelcast\",\n",
    "    \"RepoFolder\\\\cassandra\",\n",
    "    \"RepoFolder\\\\rstudio\",\n",
    "    \"RepoFolder\\\\spring-framework\",\n",
    "    \"RepoFolder\\\\drools\",\n",
    "    \"RepoFolder\\\\BroadleafCommerce\",\n",
    "    \"RepoFolder\\\\jetty.project\",\n",
    "]\n",
    "repo_names = [\n",
    "    \"intellij-community\",\n",
    "    \"MPS\",\n",
    "    \"android_frameworks_base\",\n",
    "    \"liferay-plugins\",\n",
    "    \"neo4j\",\n",
    "    \"camel\",\n",
    "    \"gradle\",\n",
    "    \"processing\",\n",
    "    \"elasticsearch\",\n",
    "    \"JGroups\",\n",
    "    \"Osmand\",\n",
    "    \"wildfly\",\n",
    "    \"sonarqube\",\n",
    "    \"voltdb\",\n",
    "    \"languagetool\",\n",
    "    \"grails-core\",\n",
    "    \"hive\",\n",
    "    \"fabric8\",\n",
    "    \"hazelcast\",\n",
    "    \"cassandra\",\n",
    "    \"rstudio\",\n",
    "    \"spring-framework\",\n",
    "    \"drools\",\n",
    "    \"BroadleafCommerce\",\n",
    "    \"jetty.project\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling for repo number 4\n",
      "Handling for repo number 6\n",
      "Handling for repo number 7\n",
      "Handling for repo number 8\n",
      "Handling for repo number 9\n",
      "Handling for repo number 10\n",
      "Handling for repo number 11\n",
      "Handling for repo number 12\n",
      "Handling for repo number 13\n",
      "Handling for repo number 14\n",
      "Handling for repo number 15\n",
      "Handling for repo number 16\n",
      "Handling for repo number 17\n",
      "Handling for repo number 18\n",
      "Handling for repo number 19\n",
      "Handling for repo number 20\n",
      "Handling for repo number 21\n",
      "Handling for repo number 22\n",
      "Handling for repo number 23\n",
      "Handling for repo number 24\n",
      "Handling for repo number 25\n",
      "Error mining refactoring for RepoFolder\\liferay-plugins: Command '['RefactoringMiner', '-a', 'RepoFolder\\\\liferay-plugins', '-json', 'RefactoringMiner-Result\\\\liferay-plugins.json']' returned non-zero exit status 1.\n",
      "Error mining refactoring for RepoFolder\\elasticsearch: Command '['RefactoringMiner', '-a', 'RepoFolder\\\\elasticsearch', '-json', 'RefactoringMiner-Result\\\\elasticsearch.json']' returned non-zero exit status 1.\n",
      "Refactoring mined for RepoFolder\\JGroups saved in RefactoringMiner-Result\\JGroups.json\n",
      "Refactoring mined for RepoFolder\\processing saved in RefactoringMiner-Result\\processing.json\n",
      "Refactoring mined for RepoFolder\\Osmand saved in RefactoringMiner-Result\\Osmand.json\n",
      "Refactoring mined for RepoFolder\\gradle saved in RefactoringMiner-Result\\gradle.json\n",
      "Refactoring mined for RepoFolder\\wildfly saved in RefactoringMiner-Result\\wildfly.json\n",
      "Refactoring mined for RepoFolder\\grails-core saved in RefactoringMiner-Result\\grails-core.json\n",
      "Refactoring mined for RepoFolder\\voltdb saved in RefactoringMiner-Result\\voltdb.json\n",
      "Refactoring mined for RepoFolder\\languagetool saved in RefactoringMiner-Result\\languagetool.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Create the directory for saving results if it doesn't already exist\n",
    "os.makedirs(\"RefactoringMiner-Result\", exist_ok=True)\n",
    "\n",
    "\n",
    "def run_command(command, repo_path, save_dir):\n",
    "    print(f\"Starting mining for {repo_path}...\")\n",
    "    try:\n",
    "        subprocess.run(command, check=True, shell=True)\n",
    "        print(f\"Refactoring mined for {repo_path} saved in {save_dir}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error mining refactoring for {repo_path}: {e}\")\n",
    "\n",
    "\n",
    "# Run analysis in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(\n",
    "    max_workers=5\n",
    ") as executor:  # Adjust the number of workers as needed\n",
    "    for idx, repo_path in enumerate(repo_paths):\n",
    "        if idx in [0, 1, 2, 4, 9]:  # Skipping some repos if needed\n",
    "            continue\n",
    "        # print(f\"Handling for repo number {idx+1}\")\n",
    "        save_dir = os.path.join(\"RefactoringMiner-Result\", f\"{repo_names[idx]}.json\")\n",
    "        command = [\"RefactoringMiner\", \"-a\", repo_path, \"-json\", save_dir]\n",
    "        executor.submit(run_command, command, repo_path, save_dir)\n",
    "\n",
    "print(\"Finished mining all data\")\n",
    "# RefactoringMiner -a RepoFolder/neo4j -json RefactoringMiner-Result/neo4j.json\n",
    "# RefactoringMiner -a RepoFolder/liferay-plugins -json RefactoringMiner-Result/liferay-plugins.json heap space\n",
    "# RefactoringMiner -a RepoFolder/elasticsearch -json RefactoringMiner-Result/elasticsearch.json heap space\n",
    "# RefactoringMiner -a RepoFolder/jetty.project -json RefactoringMiner-Result/jetty.project.json\n",
    "# RefactoringMiner -a RepoFolder/MPS -json RefactoringMiner-Result/MPS.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 d: Collect the commit message from the commits where refactoring activity has been detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing refactorings for grails-core...\n",
      "Number of commits in grails-core:  19384\n",
      "Commit messages for grails-core saved to CommitMessages\\grails-core_commit_messages.json\n",
      "Finished processing all repositories.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pydriller import Repository\n",
    "\n",
    "# Function to get commit message using pydriller\n",
    "def get_commit_message_from_url(url, repo_name):\n",
    "    repo_path = os.path.join(\"RepoFolder\", repo_name)  # Local repo path\n",
    "    commit_hash = url.split('/')[-1]  # Extract commit hash\n",
    "\n",
    "    try:\n",
    "        # Use PyDriller to get commit message\n",
    "        for commit in Repository(repo_path, single=commit_hash).traverse_commits():\n",
    "            return commit.msg\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving commit message for {commit_hash}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to process refactoring JSON file and extract commit messages\n",
    "def process_refactoring_json(file_path, output_file, repo_name):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data = data.get(\"commits\")\n",
    "    print(f\"Number of commits in {repo_name}: \", len(data))\n",
    "    # count = 0\n",
    "    for commit in data:\n",
    "        commit_id = commit.get(\"sha1\")\n",
    "        url = commit.get(\"url\")\n",
    "        refactoring = commit.get(\"refactorings\")\n",
    "\n",
    "        if not refactoring:\n",
    "            continue  # Skip if no refactoring is detected\n",
    "        # count += 1\n",
    "        # print(count)\n",
    "        commit_message = get_commit_message_from_url(url, repo_name)\n",
    "\n",
    "        if commit_message:\n",
    "            result = {\"commit_hash\": commit_id, \"commit_message\": commit_message}\n",
    "\n",
    "            # Update JSON file incrementally\n",
    "            with open(output_file, \"a\") as out_f:\n",
    "                out_f.write(\n",
    "                    json.dumps(result, indent=4) + \",\\n\"\n",
    "                )  # Append result to file\n",
    "\n",
    "\n",
    "# Main logic\n",
    "results_dir = \"RefactoringMiner-Result\"\n",
    "output_dir = \"CommitMessages\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List of repository names\n",
    "repo_names = [\n",
    "    # \"neo4j\",  # Add all repository names\n",
    "    \"grails-core\",\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# Process each repository\n",
    "for repo_name in repo_names:\n",
    "    json_file_path = os.path.join(results_dir, f\"{repo_name}_final.json\")\n",
    "    output_file = os.path.join(output_dir, f\"{repo_name}_commit_messages.json\")\n",
    "\n",
    "    # Clear output file and initialize it\n",
    "    with open(output_file, \"w\") as out_f:\n",
    "        out_f.write(\"[\\n\")  # Start JSON array\n",
    "\n",
    "    if os.path.exists(json_file_path):\n",
    "        print(f\"Processing refactorings for {repo_name}...\")\n",
    "        process_refactoring_json(json_file_path, output_file, repo_name)\n",
    "        print(f\"Commit messages for {repo_name} saved to {output_file}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping {repo_name} as refactoring results not found\")\n",
    "\n",
    "    # Finalize JSON array at the end of each repository\n",
    "    with open(output_file, \"a\") as out_f:\n",
    "        out_f.write(\"]\\n\")  # Close JSON array\n",
    "\n",
    "print(\"Finished processing all repositories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 e: Calculate and collect the diff change between the detected commit and the previous commit. (Hint: You might use pydriller library if you are using Python for this project.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing refactorings for grails-core...\n",
      "Number of commits in grails-core:  19384\n",
      "Commit messages and diffs for grails-core saved to CommitDiff\\grails-core_commit_diff.json\n",
      "Finished processing all repositories.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pydriller import Repository\n",
    "\n",
    "\n",
    "# Function to get commit message, diff, and stats using pydriller\n",
    "def get_commit_data_from_url(url, repo_name):\n",
    "    repo_path = os.path.join(\"RepoFolder\", repo_name)  # Local repo path\n",
    "    commit_hash = url.split(\"/\")[-1]  # Extract commit hash\n",
    "\n",
    "    try:\n",
    "        # Use PyDriller to get commit message, diff, and stats\n",
    "        for commit in Repository(repo_path, single=commit_hash).traverse_commits():\n",
    "            parent_hash = (\n",
    "                commit.parents[0] if commit.parents else None\n",
    "            )  # Get previous commit hash\n",
    "\n",
    "            # Get diff between the current and previous commit, and calculate stats\n",
    "            diff_data = []\n",
    "            diff_stats = {\"total_lines_added\": 0, \"total_lines_deleted\": 0}\n",
    "\n",
    "            for modified_file in commit.modified_files:\n",
    "                # Collect diff and file path info\n",
    "                diff = modified_file.diff  # Full diff content for each file\n",
    "                old_path = modified_file.old_path  # File path before modification\n",
    "                new_path = (\n",
    "                    modified_file.new_path\n",
    "                )  # File path after modification (if renamed)\n",
    "\n",
    "                # Collect numerical stats: lines added, lines deleted\n",
    "                lines_added = modified_file.added_lines\n",
    "                lines_deleted = modified_file.deleted_lines\n",
    "\n",
    "                # Update overall stats\n",
    "                diff_stats[\"total_lines_added\"] += lines_added\n",
    "                diff_stats[\"total_lines_deleted\"] += lines_deleted\n",
    "\n",
    "                # Append diff info for each modified file\n",
    "                diff_data.append(\n",
    "                    {\n",
    "                        \"file\": {\"old_path\": old_path, \"new_path\": new_path},\n",
    "                        \"diff\": diff, \n",
    "                        \"lines_added\": lines_added,\n",
    "                        \"lines_deleted\": lines_deleted,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Return commit data including the message, previous commit hash, diff, and stats\n",
    "            return {\n",
    "                \"commit_hash\": commit.hash,\n",
    "                \"previous_commit_hash\": parent_hash,\n",
    "                \"commit_message\": commit.msg,\n",
    "                \"diff_stats\": diff_stats,\n",
    "                \"diff_content\": diff_data,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving commit data for {commit_hash}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to process refactoring JSON file and extract commit messages and diffs\n",
    "def process_refactoring_json(file_path, output_file, repo_name):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data = data.get(\"commits\")\n",
    "    print(f\"Number of commits in {repo_name}: \", len(data))\n",
    "\n",
    "    for commit in data:\n",
    "        url = commit.get(\"url\")\n",
    "        refactoring = commit.get(\"refactorings\")\n",
    "\n",
    "        if not refactoring:\n",
    "            continue  # Skip if no refactoring is detected\n",
    "\n",
    "        # Get commit message, diff, and stats\n",
    "        commit_data = get_commit_data_from_url(url, repo_name)\n",
    "\n",
    "        if commit_data:\n",
    "            # Write the commit message and diff to the output JSON file incrementally\n",
    "            with open(output_file, \"a\") as out_f:\n",
    "                out_f.write(json.dumps(commit_data, indent=4) + \",\\n\")\n",
    "\n",
    "\n",
    "# Main logic\n",
    "results_dir = \"RefactoringMiner-Result\"\n",
    "output_dir = \"CommitDiff\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List of repository names\n",
    "repo_names = [\n",
    "    # Add all repository names here\n",
    "    \"grails-core\",\n",
    "]\n",
    "\n",
    "# Process each repository\n",
    "for repo_name in repo_names:\n",
    "    json_file_path = os.path.join(results_dir, f\"{repo_name}_final.json\")\n",
    "    output_file = os.path.join(output_dir, f\"{repo_name}_commit_diff.json\")\n",
    "\n",
    "    # Clear output file and initialize it\n",
    "    with open(output_file, \"w\") as out_f:\n",
    "        out_f.write(\"[\\n\")  # Start JSON array\n",
    "\n",
    "    if os.path.exists(json_file_path):\n",
    "        print(f\"Processing refactorings for {repo_name}...\")\n",
    "        process_refactoring_json(json_file_path, output_file, repo_name)\n",
    "        print(f\"Commit messages and diffs for {repo_name} saved to {output_file}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping {repo_name} as refactoring results not found\")\n",
    "\n",
    "    # Finalize JSON array at the end of each repository\n",
    "    with open(output_file, \"a\") as out_f:\n",
    "        out_f.write(\"]\\n\")  # Close JSON array\n",
    "\n",
    "print(\"Finished processing all repositories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: SOFTWARE METRICS CALCULATION: Consider the software metrics (product & process) provided in the table at the end of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install required libraries\n",
    "CK: For calculating class-level metrics like CBO, LCOM, and WMC\n",
    "> pip install pydriller pandas matplotlib seaborn ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from pydriller import Repository\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from git import Repo  # For extracting author-level metrics\n",
    "\n",
    "\n",
    "# # Commit-Level Metric Calculations\n",
    "# def calculate_commit_metrics(commit):\n",
    "#     \"\"\"\n",
    "#     Calculate commit-level metrics from the PyDriller commit object.\n",
    "#     Metrics:\n",
    "#         - COMM: The number of commits made to a file up to the considered commit.\n",
    "#         - ADD: The number of lines added in the commit.\n",
    "#         - DEL: The number of lines deleted in the commit.\n",
    "#     \"\"\"\n",
    "#     additions = sum(mod.added_lines for mod in commit.modified_files)\n",
    "#     deletions = sum(mod.deleted_lines for mod in commit.modified_files)\n",
    "#     num_modified_files = len(commit.modified_files)\n",
    "\n",
    "#     return {\n",
    "#         \"commit_hash\": commit.hash,\n",
    "#         \"author\": commit.author.name,\n",
    "#         \"date\": commit.committer_date,\n",
    "#         \"additions\": additions,  # ADD: Lines added in the commit\n",
    "#         \"deletions\": deletions,  # DEL: Lines deleted in the commit\n",
    "#         \"num_files\": num_modified_files,  # Number of files modified in the commit\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Author-Level Metrics\n",
    "# def calculate_author_metrics(repo_path):\n",
    "#     \"\"\"\n",
    "#     Calculate author-level metrics using GitPython.\n",
    "#     Metrics:\n",
    "#         - ADEV: Number of developers who contributed to a file.\n",
    "#         - NADEV: Number of distinct developers who modified specific files.\n",
    "#     \"\"\"\n",
    "#     repo = Repo(repo_path)\n",
    "#     author_stats = {}\n",
    "\n",
    "#     for commit in repo.iter_commits():\n",
    "#         author_name = commit.author.name\n",
    "#         if author_name not in author_stats:\n",
    "#             author_stats[author_name] = {\"commit_count\": 0, \"files\": set()}\n",
    "#         author_stats[author_name][\"commit_count\"] += 1\n",
    "\n",
    "#         for file in commit.stats.files:\n",
    "#             author_stats[author_name][\"files\"].add(file)\n",
    "\n",
    "#     return author_stats\n",
    "\n",
    "\n",
    "# # File-Level Metric Calculations\n",
    "# def calculate_file_metrics(modified_file):\n",
    "#     \"\"\"\n",
    "#     Calculate file-level metrics.\n",
    "#     Metrics:\n",
    "#         - OWN: Percentage of lines contributed by the highest contributor in a file.\n",
    "#         - NSCTR: Number of subsystems (directories) that a file belongs to.\n",
    "#     \"\"\"\n",
    "#     old_path = modified_file.old_path if modified_file.old_path else \"\"\n",
    "#     new_path = modified_file.new_path if modified_file.new_path else old_path\n",
    "#     added_lines = modified_file.added_lines\n",
    "#     deleted_lines = modified_file.deleted_lines\n",
    "\n",
    "#     return {\n",
    "#         \"old_path\": old_path,\n",
    "#         \"new_path\": new_path,\n",
    "#         \"added_lines\": added_lines,\n",
    "#         \"deleted_lines\": deleted_lines,\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Class-Level Metric Calculations using CK\n",
    "# def calculate_class_metrics(repo_path):\n",
    "#     \"\"\"\n",
    "#     Calculate class-level metrics for Java files using CK.jar.\n",
    "#     Metrics:\n",
    "#         - CBO: Coupling Between Object Classes.\n",
    "#         - WMC: Weighted Methods Per Class.\n",
    "#         - LCOM: Lack of Cohesion Methods.\n",
    "#     \"\"\"\n",
    "#     os.system(f\"java -jar ck.jar {repo_path} False 0 False > ck_output.txt\")\n",
    "\n",
    "#     ck_metrics = []\n",
    "#     with open(\"ck_output.txt\", \"r\") as ck_file:\n",
    "#         for line in ck_file.readlines():\n",
    "#             metric_data = line.strip().split()\n",
    "#             if len(metric_data) == 13:\n",
    "#                 file_path, cbo, wmc, lcom = (\n",
    "#                     metric_data[0],\n",
    "#                     metric_data[4],\n",
    "#                     metric_data[5],\n",
    "#                     metric_data[6],\n",
    "#                 )\n",
    "#                 ck_metrics.append(\n",
    "#                     {\"file_path\": file_path, \"CBO\": cbo, \"WMC\": wmc, \"LCOM\": lcom}\n",
    "#                 )\n",
    "\n",
    "#     return ck_metrics\n",
    "\n",
    "\n",
    "# # Metric calculation for each commit\n",
    "# def calculate_all_metrics(repo_path, output_file):\n",
    "#     # DataFrame to store all metrics\n",
    "#     metrics_df = pd.DataFrame(\n",
    "#         columns=[\n",
    "#             \"commit_hash\",\n",
    "#             \"author\",\n",
    "#             \"date\",\n",
    "#             \"additions\",\n",
    "#             \"deletions\",\n",
    "#             \"num_files\",\n",
    "#             \"class_metrics\",\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Traverse each commit in the repository\n",
    "#     for commit in Repository(repo_path).traverse_commits():\n",
    "#         commit_metrics = calculate_commit_metrics(commit)\n",
    "#         file_metrics = [\n",
    "#             calculate_file_metrics(mod_file) for mod_file in commit.modified_files\n",
    "#         ]\n",
    "#         class_metrics = calculate_class_metrics(repo_path)\n",
    "\n",
    "#         # Append the data to the DataFrame\n",
    "#         metrics_df = metrics_df.append(\n",
    "#             {\n",
    "#                 **commit_metrics,  # Unpack commit metrics\n",
    "#                 \"file_metrics\": file_metrics,\n",
    "#                 \"class_metrics\": class_metrics,\n",
    "#             },\n",
    "#             ignore_index=True,\n",
    "#         )\n",
    "\n",
    "#     # Save metrics to a CSV file\n",
    "#     metrics_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Metrics saved to {output_file}\")\n",
    "\n",
    "\n",
    "# # Visualization of metric evolution\n",
    "# def plot_metric_evolution(metrics_csv):\n",
    "#     df = pd.read_csv(metrics_csv)\n",
    "\n",
    "#     # Plot additions, deletions, and number of files modified over time\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.lineplot(x=\"date\", y=\"additions\", data=df, label=\"Additions\")\n",
    "#     sns.lineplot(x=\"date\", y=\"deletions\", data=df, label=\"Deletions\")\n",
    "#     sns.lineplot(x=\"date\", y=\"num_files\", data=df, label=\"Modified Files\")\n",
    "\n",
    "#     plt.title(\"Code Changes Over Time\")\n",
    "#     plt.xlabel(\"Date\")\n",
    "#     plt.ylabel(\"Lines of Code / Files Modified\")\n",
    "#     plt.legend()\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Main function to run metric calculations for the given repo\n",
    "# if __name__ == \"__main__\":\n",
    "#     repo_path = \"path/to/your/repo\"\n",
    "#     output_file = \"metrics_output.csv\"\n",
    "\n",
    "#     print(f\"Processing repository: {repo_path}\")\n",
    "#     calculate_all_metrics(repo_path, output_file)\n",
    "#     plot_metric_evolution(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
